# å¦‚ä½•åšå››å…­çº§è€ƒè¯•çš„æ®µè½åŒ¹é…é¢˜

> è¿™ç¯‡æ–‡ç« çœŸçš„åœ¨æ•™ä½ ä»¥ä¸€ä¸ªéå¸¸é«˜æ•ˆçš„æ–¹æ³•å®Œæˆè¿™ç±»é¢˜ç›®ã€‚

## å†™è¿™ä¹ˆé•¿è°è¯»å¾—æ‡‚å•Šï¼Ÿ

åšè¿‡è‹±è¯­å››å…­çº§è¯•å·çš„åŒå­¦éƒ½çŸ¥é“ï¼Œè¯•å·çš„ **Section Bï¼šInformation Matchingï¼ˆé•¿ç¯‡é˜…è¯»/æ®µè½åŒ¹é…ï¼‰** æ˜¯ä¸€ä¸ªä½“åŠ›æ´»ã€‚

æ–‡ç« æœ‰17~18ä¸ªæ®µè½ï¼Œç»å¸¸è¦å  3 é¡µçº¸çš„ç¯‡å¹…ï¼Œåé¢è·Ÿç€ 10 ä¸ªå¥å­ï¼Œè¦æ±‚ä½ æ‰¾å‡ºæ¯ä¸ªå¥å­å‡ºè‡ªå“ªä¸€æ®µã€‚

ç½‘ä¸Šç»™å‡ºäº†å¾ˆå¤šåšæ³•ï¼Œæ¯”å¦‚ï¼š

![alt text](image-6.png)

ä½†æ˜¯è¦æ˜¯æˆ‘ç»™æ‰€æœ‰çš„å…³é”®è¯éƒ½æ¢ä¸€ä¸ªè¯´æ³•ï¼Œè¿™ä¸å°±æ²¡æ„ä¹‰äº†å˜›ï¼å‡ºé¢˜äººæœ€å–œæ¬¢çš„å¥—è·¯å°±æ˜¯ï¼š

1.  **åŒä¹‰æ›¿æ¢**ï¼šé¢˜ç›®é‡Œçš„è¯åœ¨æ–‡ç« é‡Œæ‰¾ä¸åˆ°ï¼Œæ¢æˆäº†æ„æ€ç›¸è¿‘çš„è¯ã€‚
2.  **æ¦‚æ‹¬æ€»ç»“**ï¼šé¢˜ç›®æ˜¯å¯¹æŸä¸€æ®µè½å¤§æ„çš„æ€»ç»“ï¼Œè€Œä¸æ˜¯åŸå¥é‡ç°ã€‚
3.  **ä¿¡æ¯å¹²æ‰°**ï¼šå¥½å‡ ä¸ªæ®µè½éƒ½å‡ºç°äº†ç±»ä¼¼çš„å…³é”®è¯ï¼Œå®¹æ˜“é€‰é”™ã€‚

æ›´ä½•å†µè¿™ä¸æ˜¯ä¸€ç§â€œåšæ³•â€ï¼Œå®ƒåªå‘Šè¯‰ä½ è¦çœ‹å…³é”®è¯ï¼Œé™¤æ­¤ä¹‹å¤–æ²¡æœ‰è¯´æ˜åšçš„é¡ºåºï¼Œæ€ä¹ˆè¿›è¡ŒåŒ¹é…ï¼Œç­‰ç­‰ã€‚

ä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™ç±»å‹é¢˜ç›®çš„ä¸€ä¸ªéš¾ç‚¹å°±æ˜¯â€”â€”å†™è¿™ä¹ˆé•¿è°è¯»å¾—æ‡‚å•Šï¼Ÿ

## è®©æœºå™¨è¯»å°±è¡Œäº†å•ŠğŸ¤“

å—¯ï¼Œæˆ‘ä¸æ˜¯ç”¨ LLM æˆ–è€…ä¸“ç²¾æ¨ç†çš„ LRMã€‚å°½ç®¡è¿™ä¸ªä»»åŠ¡éå¸¸ç±»ä¼¼äºæç®€ç‰ˆçš„â€œå¤§æµ·æé’ˆâ€æµ‹è¯•ï¼Œä½†ä½¿ç”¨ LLM å°±æ˜¯é«˜å°„ç‚®æ‰“èšŠå­äº†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç›´æ¥ç”¨æ–‡æœ¬åµŒå…¥å°±è¡Œäº†ã€‚

ç®€å•æ¥è¯´ï¼ŒEmbedding å¯ä»¥å°†ä¸€æ®µæ–‡æœ¬ï¼ˆå¥å­æˆ–æ®µè½ï¼‰è½¬åŒ–æˆä¸€ä¸ªé«˜ç»´çš„å‘é‡ï¼ˆä¸€ä¸²æ•°å­—ï¼‰ã€‚**åœ¨è¿™ä¸ªå‘é‡ç©ºé—´ä¸­ï¼Œè¯­ä¹‰è¶Šç›¸ä¼¼çš„æ–‡æœ¬ï¼Œè·ç¦»è¶Šè¿‘ã€‚**

è¿™æ„å‘³ç€ï¼šå³ä½¿é¢˜ç›®æ˜¯ "The initial plan failed"ï¼Œæ–‡ç« åŸå¥æ˜¯ "Their first attempt was unsuccessful"ï¼Œè™½ç„¶æ²¡æœ‰å•è¯é‡åˆï¼Œä½†å®ƒä»¬çš„å‘é‡è·ç¦»ä¼šéå¸¸è¿‘ï¼

## ä»è¯»æ‡‚åˆ°ä¼šåšé¢˜

é‚£ä¹ˆå…·ä½“æ€ä¹ˆåˆ©ç”¨è¿™ä¸ªåµŒå…¥ä¿¡æ¯æ¥åšé¢˜å‘¢ï¼Ÿæˆ‘ä»¬è€ƒè™‘ä¸¤ç§æƒ…å†µï¼š

1. æå–å‡ºæ¥çš„å¥å­æ˜¯å¯¹åŸè¯åŸå¥çš„æ”¹å†™â€”â€”è¿™æ ·ï¼Œå¯¹åŸæ–‡æŸä¸€å¥çš„ç›¸ä¼¼åº¦ä¼šå¾ˆé«˜ã€‚
2. æå–å‡ºæ¥çš„å¥å­æ¦‚æ‹¬äº†æ®µè½å¤§æ„â€”â€”è¿™æ ·å¯¹åŸæ–‡æŸä¸€æ®µçš„ç›¸ä¼¼åº¦ä¼šå¾ˆé«˜ã€‚

ä¸‹é¢æ˜¯å…·ä½“çš„ç®—æ³•æµç¨‹ï¼š

### 1. æ•°æ®é¢„å¤„ç†
é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ–‡ç« è¯»å–è¿›æ¥ï¼Œåˆ©ç”¨ Python çš„ `nltk` åº“ï¼Œåšä¸¤å±‚åˆ‡å‰²ï¼š
*   **æ®µè½å±‚**ï¼šå°†æ–‡ç« åˆ‡åˆ†æˆ Paragraph A, B, C...
*   **å¥å­å±‚**ï¼šå°†æ¯ä¸ªæ®µè½è¿›ä¸€æ­¥åˆ‡åˆ†æˆå¥å­é›†åˆ $S$ã€‚
*   åŒæ—¶ï¼Œè¯»å–é¢˜ç›®ç»™å‡ºçš„ 10 ä¸ªå¥å­ï¼Œè®°ä¸ºé›†åˆ $T$ã€‚

### 2. å¹¶å‘å‘é‡åŒ–
åˆ©ç”¨ä»»ä½•ä¸€ä¸ª OpenAI å…¼å®¹çš„ API åšè°ƒç”¨ï¼Œæˆ‘ä½¿ç”¨äº† `Qwen3-Embedding-8B` æ¨¡å‹ï¼ˆç›®å‰ç¡…åŸºå¹³å°ä¸Šé¢å‚æ•°é‡æœ€å¤§çš„åµŒå…¥æ¨¡å‹ï¼‰ã€‚ä¸ºäº†ä¿è¯é€Ÿåº¦ï¼Œä»£ç å¼€å¯äº†å¹¶å‘ã€‚

### 3. åŒ¹é…ç®—æ³•

æ ¹æ®ä¸Šé¢çš„è®¨è®ºï¼Œç®€å•åœ°å¯»æ‰¾ç›¸ä¼¼åº¦æœ€é«˜çš„å¥å­ä¸ä¸€å®šæå¾—å®šï¼Œéœ€è¦ç»“åˆäº†ä¸¤ä¸ªç»´åº¦ï¼š

ç¬¬ä¸€æ­¥æ˜¯åšå¥å­çš„å±‚é¢åŒ¹é…ã€‚è¿™ä¸€æ­¥è®¡ç®—é¢˜ç›®å¥å­ $t$ ä¸æ–‡ç« ä¸­**æ¯ä¸€ä¸ªå¥å­**çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚ä¹Ÿå°±æ˜¯æ¨¡æ‹Ÿæˆ‘ä»¬åšé¢˜æ—¶å¯»æ‰¾â€œåŸå¥æ”¹å†™â€çš„è¿‡ç¨‹ã€‚å¦‚æœæŸä¸€æ®µé‡Œæœ‰ä¸€ä¸ªå¥å­å’Œé¢˜ç›®æåº¦ç›¸ä¼¼ï¼Œé‚£ä¹ˆè¿™ä¸€æ®µçš„å¾—åˆ†å°±è¯¥å¾ˆé«˜ã€‚å¯ä»¥æŒ‰ç…§ä¸‹é¢çš„å¼å­è¿›è¡Œè®¡ç®—ï¼š $\mathrm{Score}_{1}(\mathrm{Para}_k) = \max(\mathrm{Similarity}(t, s))$ï¼Œå…¶ä¸­ $s \in \mathrm{Para}_k$ã€‚

ç¬¬äºŒæ­¥æ˜¯åšæ®µè½å±‚é¢çš„åŒ¹é…ã€‚è¿™ä¸€æ­¥è®¡ç®—é¢˜ç›®å¥å­ $t$ ä¸**æ•´æ®µæ–‡æœ¬**çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚ä¹Ÿå°±æ˜¯æ¨¡æ‹Ÿäº†æˆ‘ä»¬åšé¢˜æ—¶å¯»æ‰¾â€œæ®µè½å¤§æ„â€çš„è¿‡ç¨‹ã€‚æœ‰æ—¶å€™é¢˜ç›®æ˜¯å¯¹å…¨æ®µçš„æ€»ç»“ï¼Œå•çœ‹æŸä¸€å¥è¯åŒ¹é…åº¦ä¸é«˜ï¼Œä½†çœ‹æ•´ä½“å°±éå¸¸å»åˆã€‚è¿™ä¸€æ­¥çš„è®¡ç®—å¼å­æ˜¯ $\mathrm{Score}_{2}(\mathrm{Para}_k) = \mathrm{Similarity}(t, \mathrm{Para}_k)$ã€‚

### 4. ç»¼åˆå†³ç­–
æœ€ç»ˆï¼Œå¯¹äºæ¯ä¸€ä¸ªé¢˜ç›®ï¼Œæˆ‘ä»¬è®¡ç®—æ¯ä¸ªæ®µè½çš„ç»¼åˆå¾—åˆ†ï¼š

$$
S(k) = \mathrm{Score}_{1}(\mathrm{Para}_k)+\mathrm{Score}_{2}(\mathrm{Para}_k)
$$

è¿™é‡Œå¯ä»¥æ ¹æ®å‡ºé¢˜é£æ ¼è°ƒä¸€ä¸‹åŠ æƒç³»æ•°ï¼Œä¸è¿‡ä¿æŒé»˜è®¤çš„è¡¨ç°å°±å·²ç»å¾ˆå¥½äº†ã€‚æœ€åå¾—åˆ†æœ€é«˜çš„é‚£ä¸ªæ®µè½ï¼Œå°±æ˜¯æˆ‘ä»¬çš„ç­”æ¡ˆã€‚

## ä»£ç å®ç°

ä¸‹é¢ç»™å‡ºäº†å®Œæ•´çš„ä»£ç ï¼š

```python
# éœ€è¦çš„åº“ï¼š pip install requests numpy scikit-learn nltk
# åœ¨ passage.txt å†…å¡«å…¥é˜…è¯»åŸæ–‡ï¼Œä»¥ \n\n åˆ†æ®µ
# åœ¨ sentences.txt å†…å¡«å…¥å¾…å¤„ç†çš„å¥å­ï¼Œç”¨ \n\n åˆ†å¼€
# ç„¶åè¿è¡Œï¼
import os
import requests
import json
import numpy as np
import nltk
from concurrent.futures import ThreadPoolExecutor
from sklearn.metrics.pairwise import cosine_similarity

# ================= é…ç½®åŒºåŸŸ =================
# è¯·åœ¨æ­¤å¤„å¡«å…¥ä½ çš„ API Key
API_KEY = "sk-*******"

# è¯·åœ¨æ­¤å¤„å¡«å…¥æ¨¡å‹åç§° (ä¾‹å¦‚: BAAI/bge-m3, BAAI/bge-large-en-v1.5 ç­‰)
MODEL_NAME = "Qwen/Qwen3-Embedding-8B"

# å¹¶å‘æ•°è®¾ç½®
CONCURRENCY = 50

# å…¼å®¹ OpenAI æ ¼å¼ API åœ°å€
API_URL = "https://api.siliconflow.cn/v1/embeddings"
# ===========================================

def download_nltk_data():
    """ä¸‹è½½å¿…è¦çš„åˆ†å¥æ•°æ®"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½ NLTK punkt åˆ†è¯æ•°æ®...")
        nltk.download('punkt')
        nltk.download('punkt_tab')

def get_embedding(text):
    """
    è°ƒç”¨ SiliconFlow API è·å–æ–‡æœ¬åµŒå…¥å‘é‡
    """
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    
    # ç§»é™¤æ¢è¡Œç¬¦ï¼Œé˜²æ­¢å½±å“ Embedding è´¨é‡
    clean_text = text.replace("\n", " ")
    
    payload = {
        "model": MODEL_NAME,
        "input": clean_text
    }
    
    try:
        response = requests.post(API_URL, json=payload, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()
        return data['data'][0]['embedding']
    except Exception as e:
        print(f"Error getting embedding for text: {clean_text[:30]}... : {e}")
        # å¦‚æœå¤±è´¥è¿”å›å…¨0å‘é‡é˜²æ­¢ç¨‹åºå´©æºƒï¼ˆå®é™…ç”Ÿäº§ä¸­åº”é‡è¯•ï¼‰
        return None

def read_files():
    """è¯»å–æ–‡ä»¶å†…å®¹"""
    try:
        with open('passage.txt', 'r', encoding='utf-8') as f:
            passage_text = f.read().strip()
        
        with open('sentences.txt', 'r', encoding='utf-8') as f:
            sentences_text = f.read().strip()
            
        return passage_text, sentences_text
    except FileNotFoundError as e:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ - {e}")
        exit(1)

def main():
    # 0. å‡†å¤‡å·¥ä½œ
    print("Preparing...")
    download_nltk_data()
    if API_KEY == "YOUR_SILICONFLOW_API_KEY_HERE":
        print("è¯·åœ¨ä»£ç ä¸­è®¾ç½®æ­£ç¡®çš„ API_KEY")
        return

    # 1. è¯»å–æ–‡ä»¶
    print("1. è¯»å–æ–‡ä»¶...")
    passage_raw, sentences_raw = read_files()

    # å¤„ç† passage.txt: æŒ‰ \n\n åˆ†å‰²æ®µè½
    paragraphs = [p.strip() for p in passage_raw.split('\n\n') if p.strip()]
    
    # å¤„ç† sentences.txt: æŒ‰ \n\n åˆ†å‰²å¥å­ (é›†åˆ T)
    t_sentences = [s.strip() for s in sentences_raw.split('\n\n') if s.strip()]

    # 2. å°†æ®µè½åˆ†å¥å¾—åˆ°é›†åˆ Sï¼Œå¹¶è®°å½•æ‰€å±æ®µè½ç´¢å¼•
    print("2. é¢„å¤„ç†æ®µè½ä¸åˆ†å¥...")
    s_data = [] # å­˜å‚¨æ ¼å¼: {'text': sentence, 'para_idx': int}
    
    for idx, para in enumerate(paragraphs):
        # ä½¿ç”¨ nltk è¿›è¡Œè‹±æ–‡åˆ†å¥
        sents = nltk.sent_tokenize(para)
        for s in sents:
            s_data.append({'text': s, 'para_idx': idx})
            
    # 3. å‡†å¤‡æ‰€æœ‰éœ€è¦ Embedding çš„æ–‡æœ¬åˆ—è¡¨
    #æˆ‘ä»¬éœ€è¦ Embed: T ä¸­çš„å¥å­, S ä¸­çš„å¥å­, ä»¥åŠåŸæ®µè½(ç”¨äºæ­¥éª¤5)
    all_texts_to_embed = []
    
    # æ˜ å°„ç´¢å¼•èŒƒå›´
    t_range = range(0, len(t_sentences))
    start_s = len(t_sentences)
    s_range = range(start_s, start_s + len(s_data))
    start_p = start_s + len(s_data)
    p_range = range(start_p, start_p + len(paragraphs))
    
    all_texts_to_embed.extend(t_sentences)
    all_texts_to_embed.extend([item['text'] for item in s_data])
    all_texts_to_embed.extend(paragraphs)
    
    print(f"3. å¼€å§‹å¹¶å‘è·å– Embeddings (æ€»æ•°: {len(all_texts_to_embed)}, å¹¶å‘æ•°: {CONCURRENCY})...")
    
    embeddings_map = {}
    
    # ä½¿ç”¨ ThreadPoolExecutor å¹¶å‘è°ƒç”¨æ¥å£
    with ThreadPoolExecutor(max_workers=CONCURRENCY) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {executor.submit(get_embedding, text): i for i, text in enumerate(all_texts_to_embed)}
        
        # æ”¶é›†ç»“æœï¼ˆä¿æŒé¡ºåºï¼‰
        results = [None] * len(all_texts_to_embed)
        for future in futures:
            idx = futures[future]
            res = future.result()
            if res is None:
                print("è·å– Embedding å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
                return
            results[idx] = res

    # åˆ†ç¦» Embedding å‘é‡
    t_embeddings = np.array(results[:len(t_sentences)])
    s_embeddings = np.array(results[start_s:start_s + len(s_data)])
    p_embeddings = np.array(results[start_p:])

    print("4 & 5. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ...")
    
    # è®¡ç®— T vs S çš„ç›¸ä¼¼åº¦çŸ©é˜µ (Shape: len(T) x len(S))
    sim_t_s = cosine_similarity(t_embeddings, s_embeddings)
    
    # è®¡ç®— T vs Paragraphs çš„ç›¸ä¼¼åº¦çŸ©é˜µ (Shape: len(T) x len(Paragraphs))
    sim_t_p = cosine_similarity(t_embeddings, p_embeddings)

    print("6. ç»¼åˆè¯„åˆ†å¹¶åŒ¹é…æ®µè½...")
    
    final_results = []
    num_paras = len(paragraphs)

    for i in range(len(t_sentences)):
        # --- æ­¥éª¤ 4 é€»è¾‘ ---
        # æ‰¾å‡ºå½“å‰ T å¥å­ä¸ S é›†åˆä¸­ç›¸ä¼¼åº¦æœ€é«˜çš„é‚£ä¸ªå¥å­ï¼Œå¹¶è·å–å…¶æ‰€å±æ®µè½
        # è¿™é‡Œæˆ‘ä»¬ä¸åªå–æœ€å¤§å€¼ï¼Œè€Œæ˜¯è®¡ç®—â€œåŸºäºå¥å­çš„æ®µè½å¾—åˆ†â€
        # ç­–ç•¥ï¼šå¯¹äºæ¯ä¸ªæ®µè½ï¼Œæ‰¾å‡ºè¯¥æ®µè½ä¸­ä¸å½“å‰ T å¥å­ç›¸ä¼¼åº¦æœ€é«˜çš„é‚£ä¸ªå¥å­çš„å¾—åˆ†
        
        # åˆå§‹åŒ–è¯¥ T å¥å­é’ˆå¯¹æ¯ä¸ªæ®µè½çš„å¥å­åŒ¹é…æœ€é«˜åˆ†
        best_sentence_score_per_para = np.zeros(num_paras)
        
        # éå†æ‰€æœ‰ S å¥å­åŠå…¶ç›¸ä¼¼åº¦
        for j, score in enumerate(sim_t_s[i]):
            p_idx = s_data[j]['para_idx']
            if score > best_sentence_score_per_para[p_idx]:
                best_sentence_score_per_para[p_idx] = score
        
        # --- æ­¥éª¤ 5 é€»è¾‘ ---
        # å½“å‰ T å¥å­ä¸æ®µè½é›†åˆçš„ç›´æ¥ç›¸ä¼¼åº¦
        direct_para_scores = sim_t_p[i]
        
        # --- æ­¥éª¤ 6 ç»¼åˆé€»è¾‘ ---
        # ç»¼åˆè¯„åˆ†ç­–ç•¥ï¼šç›´æ¥ç›¸åŠ  (æˆ–è€…åŠ æƒç›¸åŠ )
        # Final Score(Para k) = Sim(T, Para k) + Max(Sim(T, Sentence in Para k))
        # è¿™ç§æ–¹æ³•æ—¢è€ƒè™‘äº†å®è§‚è¯­ä¹‰(æ®µè½çº§)ï¼Œä¹Ÿè€ƒè™‘äº†å¾®è§‚ç»†èŠ‚(å¥å­çº§)
        
        combined_scores = direct_para_scores + best_sentence_score_per_para
        
        # æ‰¾åˆ°ç»¼åˆå¾—åˆ†æœ€é«˜çš„æ®µè½ç´¢å¼•
        best_para_idx = np.argmax(combined_scores)
        
        # è½¬æ¢ä¸ºå­—æ¯ (0->A, 1->B)
        para_letter = chr(ord('A') + best_para_idx)
        final_results.append(para_letter)

    # 7. è¾“å‡ºç»“æœ
    print("\n---------- åŒ¹é…ç»“æœ ----------")
    for res in final_results:
        print(res)
    print("-----------------------------")

if __name__ == "__main__":
    main()
```

## å®æµ‹

æˆ‘ç”¨2025å¹´6æœˆè‹±è¯­å…­çº§è€ƒè¯•çš„ä¸‰å¥—å·å­åšäº†ä¸€ä¸ªç®€å•çš„æµ‹è¯•ã€‚

![alt text](9ef70fed6d8f045d2653695d18ddd813.png)

![alt text](359824d91bf434801ec23c727974d4f1.png)

![alt text](0a58d510169cbe36939aac225e772bc9.png)

![alt text](470e98f039abe63c9a4f4aca3fb359e4.png)

![alt text](766cba64f904c5ada963654b5c97d77a.png)

![alt text](a87cffee9df7549df64bebe08f727613.png)

ç»“æœå¾ˆæ£’ï¼è¿™ä¸ªç®—æ³•è·å¾—äº†æ»¡åˆ†ã€‚

æˆ‘åŒæ—¶ä¹Ÿåœ¨ä¸€äº›å•†ä¸š LRM ä¸Šæµ‹è¯•äº†é¢˜ç›®ã€‚æˆ‘æµ‹è¯•çš„æç¤ºè¯æ˜¯

```
åšæ®µè½åŒ¹é…é¢˜ç›®ï¼Œæ–‡ç« ï¼š{passage.txt} å¥å­ï¼š{sentence.txt} ç»™å‡ºç­”æ¡ˆã€‚ä¸è¦ç”¨æ•°å­—è¿›è¡Œæ®µè½ç¼–å·ï¼Œç”¨è‹±æ–‡å­—æ¯ï¼Œå¦‚A=1
```

é™¤äº† Deepseek ç¬¬ä¸‰ç¯‡åšé”™äº†ä¸€é“ä»¥å¤–ï¼Œå…¶ä»–çš„ LRM éƒ½æ˜¯æ»¡åˆ†ã€‚æ‰€ä»¥æ³›åŒ–æ˜¯å¾ˆææ€–çš„ï¼ŒThe bitter lesson è¿˜åœ¨è¿½æˆ‘ã€‚

ç¬”è€…è€ƒå®Œå…­çº§æœ‰æ„Ÿè€Œå‘ã€‚
