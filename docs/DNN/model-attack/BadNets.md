# BadNets

[文章](https://arxiv.org/pdf/1708.06733) 的作者都来自纽约大学。

## 内容

文章本身没有什么好谈的，我觉得其结论是一个很 trivial 的东西，并没有很丰富的营养。

文章提出了三种网络范式：

- A: 干净的网络，通过干净的训练样本训练得到。
- B: 理想的后门攻击，也就是在干净网络侧面加入一个并行模块用来识别后门。一旦出现攻击的样本就被触发并自动进行干扰。
- C: 现实情况是，不可能单独设置一个独立的后门触发逻辑，而是从数据侧投毒。

文章主要论证了 C 情况也是相当可行的。我们对数据进行投毒。在图像分类领域里面，我们可以在图像的一小部分添加一个特定修改，然后让修改后的图像指向我们期望的分类。

文章除了在背景知识里面放了几个基础的公式之后，后面就没有什么式子了。文章主要的贡献是提出了这个威胁模型，并且做了很多可行性验证。

文章对提出的后门攻击进行了实验，效果很好。

在迁移学习情境下，文章也论证了后门仍然在一定程度上被保留。

## 评述

笔者认为这篇文章没什么营养（除了做的几个实验有点看头），因为一个神经网络无非就是一个拟合机器，而通过数据投毒的方式引入后门，本身也并没有离开神经网络拟合与泛化的本质。只要它能够通过训练特征提取器识别到“后门”的特征，并且在标签中给出区分，那么这就是一个有意义的优化目标，自然能够被埋入后门。

并且后门还存在反演的可能，因此如何实现更隐蔽的后门以及如何实现更高效的后门反演/检测都是问题，很可惜这篇文章没有提到。如果说后门在 $L$ 范数意义下是难以察觉的，那么一般的针对 $L$ 范数的对抗样本方法理应找到对应的后门触发器。
