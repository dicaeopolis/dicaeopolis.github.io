# HSJA 攻击

[文章](https://arxiv.org/pdf/1904.02144) 作者是来自加州大学伯克利分校。

和我们之前做笔记的 FGSM, PGD, C&W 攻击不一样，HSJA 攻击是一种**黑盒攻击**。但其实本质上和白盒攻击区别没有那么大。

回顾我们之前在 C&W 的动机角度聊过的，一次理想的攻击需要追求两个拮抗的目标：使模型出错（分为有目标和无目标攻击）和扰动尽可能小（肉眼难以发觉）。为此我们将这两个指标量化，第一个目标用 $f$ 函数指示，第二个目标使用范数约束。最后**利用模型的梯度信息**进行优化。

现在模型变成黑盒了，拿不到梯度信息了，咋办？

HSJA 提出的方案是：利用先分类结果重构梯度信息，然后再逼近原始图像。

## 二分逼近

首先需要明确的是，如果我们对图像做任何扰动之后，标签没有发生任何改变，这就意味着我们不能从中获取任何（梯度）信息。因此我们首先需要做的是把图像**移动到决策边界**。

假设我们有源图像 $x^*$，目标图像 $x$，其输出符合我们的需要。如果我们要进行有目标攻击，我们大可以把 $x$ 设置为我们需要的类别图像；如果是无目标攻击，我们可以倍增加噪直到获取到一个差异的分类。现在我们要基于这两个样本来找到决策边界。

很自然地，我们会使用二分法。我们定义一个指标函数 $\phi(x_t)$，为 $-1$ 代表分为原类别，为 $1$ 代表分为目标类别。对于 $\alpha\in (0,1)$，定义

$$
x(\alpha)=\alpha x+(1-\alpha)x^*
$$

通过对 $\alpha$ 做二分法，就可以在满足 $L$ 范数精度小于阈值 $\theta$ 内找到刚刚好使得 $\phi$ 改变的 $\hat\alpha$。

我们将这个流程抽象成一个函数，接受 $x,x^*,\theta, L$，分别代表输入图像，目标图像，估计的决策边界宽度（也就是二分阈值）以及约束的范数，最后输出的是 $\hat x=x(\hat\alpha)$，和输入目标样本的分类一致但更靠近决策边界。

## 估计边界梯度

有了一个在边界的样本 $\hat x$，我们可以在上面进行扰动，为此，我们首先生成一个在单位球上均匀分布的样本集合（可以通过正态分布投影到超球面，也可以通过基于拒绝采样的 Marsaglia 算法）得到 $u_0, \dots, u_B$，然后我们进行扰动，也就是计算

$$
S_i = \phi(x+\delta u_i)=0\mathrm{\ or\ } 1
$$

下面我们要基于此来估计梯度了。首先计算 $S_i$ 的均值

$$
\bar S=\dfrac{1}{B}\sum_{i=1}^B S_i
$$

这表征此样本的一个基线估计，然后我们计算估计的梯度：

$$
\hat g=\dfrac{1}{B-1}\sum_{i=1}^B(S_i-\bar{S})u_i
$$

这里的 $B-1$ 就是利用的无偏估计，虽然我觉得用处不大，因为我们马上就要归一化：

$$
v=\mathrm{normalized}_L(\hat g)
$$

## 迭代求解

现在，我们已经在决策边界处获得了一个候选点 $\hat x$ 以及该点的估计梯度方向 $v$，下面我们要根据这些信息进行进一步的迭代求解。

和 PGD 等其他基于梯度的方法类似，我们也进行投影梯度下降。因此选择一个参数 $\xi$ 计算

$$
\tilde{x}=\hat x+\xi v
$$

但是有可能用力过猛飞出目标样本的决策边界了……因此我们要折半 $\xi$ 使得其保留在原有的边界内。

这个 $\tilde x$ 就是相对原始的目标样本 $x$ 更优的目标了（因为我们沿着梯度的方向走了一步），因此，我们持续进行迭代，直到达到最大的次数限制或者已经收敛而不再变化。

最后，为了我们可以再进行一次二分搜索，使得输出 $\hat x$ 在决策边界上，而更靠近原始样本 $x^*$。

## 参数渐变

刚刚对算法只是模糊地提点了一下，下面我们来详细讨论一下刚刚算法里面提到的一堆参数的设置，这决定了算法的收敛性。
