# FGSM æ”»å‡»

[æ–‡ç« ](https://arxiv.org/abs/1412.6572) å‘è¡¨åœ¨ ICLR 2015ï¼Œä½œè€…æ˜¯ GAN çš„æå‡ºè€… Ian Goodfellowã€‚

æœ¬æ–‡çš„è¡Œæ–‡æ€è·¯è‚¯å®šå’ŒåŸè®ºæ–‡ä¸ä¸€æ ·ï¼Œè¦ä¸ç„¶ç™½å†™äº†â€¦â€¦è€Œä¸”æˆ‘è§‰å¾—åŸè®ºæ–‡è›®åƒé‚£ç§çŸ¥ä¹ä¸“æ æ–‡ç« ï¼ŒæŒºå¥½è¯»çš„ï¼Œä»¥å‰çš„ ICLR è¿™ä¹ˆå¥½å‘å—ğŸ˜±ğŸ˜±ğŸ˜±

## æ”»å‡»æ–¹å¼

FGSM æ”»å‡»æ˜¯ä¸€ç§**ç™½ç›’æ”»å‡»**ï¼Œå¿…é¡»æ‹¿åˆ°æ¨¡å‹å¯¹è¾“å…¥çš„æ¢¯åº¦ï¼Œä¹Ÿå°±æ˜¯

$$
g_x=\nabla_x \mathcal L(\theta;x)
$$

è¿™é‡Œå¾ˆå¾®å¦™çš„ä¸€ç‚¹æ˜¯ï¼Œåœ¨ä¼˜åŒ–å™¨é¢†åŸŸï¼Œæˆ‘ä»¬æ˜¯åŸºäºè®¡ç®—

$$
g_\theta=\nabla_\theta \mathcal L(\theta;x)
$$

æ¥å®ç°å¯¹å‚æ•°çš„å¿«é€Ÿæ›´æ–°çš„ã€‚è¿™é‡Œçš„â€œå¿«é€Ÿâ€æ„å³åˆ©ç”¨å°½å¯èƒ½å°‘çš„è¿­ä»£æ¬¡æ•°å¾—åˆ°å°½å¯èƒ½å°çš„æŸå¤±ï¼Œå³è®©æ¨¡å‹å‚æ•° $\theta$ åœ¨å›ºå®šçš„è¾“å…¥ $x$ æ„å»ºçš„æŸå¤±åœ°å½¢ä¸Šå®ç°å¿«é€Ÿä¸‹é™ã€‚

FGSM ä¹Ÿæ˜¯ç±»ä¼¼ï¼Œä¸è¿‡æ­¤æ—¶æˆ‘ä»¬é¢å¯¹ä¸€ä¸ªè®­ç»ƒå¥½çš„ï¼ˆå›ºå®šçš„ï¼‰æ¨¡å‹ $\theta$ï¼Œéœ€è¦æ„å»º $x$ æ¥æ›´æ”¹è¾“å‡ºã€‚æ­¤æ—¶æˆ‘ä»¬çš„ç›®æ ‡æ˜¯**åˆ©ç”¨å°½å¯èƒ½å°‘çš„è¿­ä»£æ¬¡æ•°å¾—åˆ°å°½å¯èƒ½å¤§çš„æŸå¤±**ã€‚å½“ç„¶è¿­ä»£æ³•ä¼šåœ¨åé¢ä»‹ç»ï¼ŒFGSM ä½œä¸ºä¸€ç§å¤è€çš„æ”»å‡»æ–¹æ³•ï¼Œæ˜¯**å•æ­¥**çš„ã€‚

ä¸€ä¸ªç›¸å½“æœ´ç´ çš„æ€è·¯æ˜¯ï¼Œæˆ‘ä»¬åœ¨å•æ­¥å†…**ç›´æ¥é€‰å–æ¢¯åº¦å˜åŒ–æœ€å¤§çš„é‚£ä¸ªæ–¹å‘è¿›è¡Œä¸Šå‡**ï¼Œä¹Ÿå°±æ˜¯ï¼š

$$
\tilde x_{L_2}=x+\epsilon g_x
$$

è¿™é‡Œçš„ $\epsilon$ å¯ä»¥ç±»æ¯”äºå­¦ä¹ ç‡ã€‚

å¦‚æœå¯¹ç°ä»£ä¼˜åŒ–å™¨ç†è®ºæ¯”è¾ƒç†Ÿæ‚‰çš„è¯ï¼Œå¯èƒ½ä¼šè€ƒè™‘ Adam ä¼˜åŒ–å™¨å¯¹åº”çš„ signSGDï¼Œæˆ–è€…æ˜¯ Muon çš„ $\mathrm{msign}(M)=UV^\top$ï¼Œä½†å®ƒä»¬æ›´å¤šä¾èµ–äº**æ›´å¹¿é˜”çš„æŸå¤±åœ°å½¢è§†é‡**ï¼Œæˆ–è®¸åœ¨å•æ­¥ä¸‹æ²¡é‚£ä¹ˆæœ‰æ•ˆï¼Ÿæ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å¯¹æ¢¯åº¦å…¶å®æœ‰ä¸‰ç§æ¯”è¾ƒç»å…¸çš„çº¦æŸå½¢å¼ï¼š

$$
\begin{align*}
    \tilde x_{L_2}&=x+\epsilon g_x/\|g_x\|\\
    \tilde x_{L_\infty}&=x+\epsilon \mathrm{sign}(g_x)\\
    \tilde x_{L_\mathrm{spec}}&=x+\epsilon \mathrm{msign}(g_x)
\end{align*}
$$

è¿™é‡Œé€‰æ‹© $L_2$ èŒƒæ•°ä½œä¸ºçº¦æŸåªæ˜¯å› ä¸ºæ¯”è¾ƒâ€œç»å…¸â€ï¼Œå…¶å®ä¹Ÿå¯ä»¥æ‹¿ç€ $L_p$ èŒƒæ•°æ¥è¯´äº‹çš„ã€‚

æˆ‘ä¸ªäººæ›´å€¾å‘äºæŠŠå®ƒä»¬å«åš**ä¸åŒèŒƒæ•°çº¦æŸä¸‹çš„ FGSM æ”»å‡»**ï¼Œå°½ç®¡ FGSM å…¨ç§°æ˜¯ Fast Gradient **Sign** Methodâ€¦â€¦

åˆ°åº•é€‰æ‹©å“ªä¸ªèŒƒæ•°è¿›è¡Œçº¦æŸï¼Œæˆ‘ä»¬æ”¾åœ¨åé¢è®²ã€‚ä¸‹é¢ä¸€èŠ‚æˆ‘ä»¬æ¥èŠèŠè¿™ä¸€æ”»å‡»æ–¹å¼çš„å¦å¤–ä¸€ç§çœ‹å¾…è§†è§’ã€‚

## çº¿æ€§è§’åº¦

è¿™ä¸ªè§†è§’æ˜¯åŸè®ºæ–‡çš„ç¬¬ä¸‰èŠ‚ç»™å‡ºæ¥çš„ã€‚ç”±äºæ·±å±‚ç¥ç»ç½‘ç»œä¾èµ–çŸ©é˜µä¹˜æ³•ï¼Œå› æ­¤è€ƒè™‘çŸ©é˜µæŸä¸€åˆ— $w^\top$ å’Œæ‰°åŠ¨åçš„è¾“å…¥ $\tilde x=x+\eta$ ç›¸ä¹˜ï¼š

$$
w^\top\tilde x=w^\top x+w^\top\eta
$$

é‚£ä½ é—®æˆ‘ $\eta$ å–å“ªä¸ªæ–¹å‘å¯ä»¥ä½¿å¾—æ‰°åŠ¨é¡¹æœ€å¤§åŒ–å‘¢ï¼Ÿè¿™ä¸å°±æ˜¯é«˜ä¸­å¤§å®¶éƒ½å­¦è¿‡çš„**æŸ¯è¥¿ä¸ç­‰å¼**å˜›â€”â€”å¦‚æœæ‰°åŠ¨é¡¹å’Œ $w$ â€œå¹³è¡Œâ€çš„æ—¶å€™èƒ½å¤Ÿæœ€å¤§åŒ–ã€‚

è¿™é‡Œçš„å¹³è¡Œè¦æ‰“å¼•å·ï¼Œå› ä¸ºä¸¥æ ¼æ„ä¹‰ä¸Šè¯´å®ƒçš„æ„æ€æ˜¯è¦è®©ä¸‹é¢çš„ç­‰å·å–åˆ°ï¼š

$$
|\langle u,v\rangle|\le\|u\|\cdot\|v\|
$$

è€Œä¸åŒçš„å†…ç§¯åˆä¸ºç©ºé—´èµ‹äºˆäº†ä¸åŒçš„èŒƒæ•°ã€‚

å¥½ï¼Œæˆ‘ä»¬ä¼¼ä¹å°±å¯ä»¥å¾—åˆ°ï¼šå–çŸ©é˜µå…ƒç´ ä¹˜ä»¥ $\epsilon$ï¼Œç„¶åè€ƒè™‘ä¸€ä¸‹å†…ç§¯çº¦æŸï¼Œå°±å¯ä»¥äº†â€¦â€¦å¯¹å—ï¼Ÿ

å¤§é”™ç‰¹é”™ï¼è°å‘Šè¯‰ä½ ï¼Œ**æ·±åº¦ç¥ç»ç½‘ç»œå°±æ˜¯çº¿æ€§çš„çŸ©é˜µä¹˜æ³•çš„**ï¼Ÿï¼

æ­£æ˜¯å¼•å…¥äº†éçº¿æ€§ï¼Œç¥ç»ç½‘ç»œæ‰å…·æœ‰ä¸°å¯Œçš„æ‹Ÿåˆèƒ½åŠ›å“¦ã€‚

äº‹å®ä¸Šï¼ŒåŸè®ºæ–‡è¿™ä¸€æ®µçš„æ„æ€æ˜¯ï¼Œ**å¦‚æœç¥ç»ç½‘ç»œåœ¨æ ·æœ¬é™„è¿‘è¿‘ä¼¼çº¿æ€§ï¼Œé‚£ä¹ˆå®ƒå¯ä»¥è¢«ç›¸å½“é«˜æ•ˆåœ°æ‰°åŠ¨**ï¼

ä¸ºä»€ä¹ˆå‘¢ï¼Ÿæˆ‘ä»¬æ¥ä¼°ç®—ä¸€ä¸‹ $w^\top\eta$ã€‚å‡å®š $w\sim\mathcal{N}(0,I_n)$ï¼Œ$\eta=\epsilon w$ï¼Œé‚£ä¹ˆè¿™ä¸ªç‚¹ä¹˜çš„ç»“æœå°±æ˜¯ $\epsilon n\mathrm{Var}[w_i]=\epsilon n$ï¼Œä¹Ÿå°±æ˜¯è¯´ç»´åº¦è¶Šé«˜ï¼Œå³ä½¿ä¿æŒä¸€ä¸ªæ¯”è¾ƒå°çš„ $\epsilon$ï¼Œä¹Ÿå¯ä»¥ç§¯ç´¯èµ·å¾ˆå¤§çš„æ‰°åŠ¨ã€‚

åŸè®ºæ–‡è¿™ä¸€èŠ‚é‡Œé¢æ˜¯è¿™æ ·è¯´æ˜çš„ï¼š

> If $w$ has $n$ dimensions and the average magnitude of an element of the weight vector is $m$, then the activation will grow by $\epsilon mn$.

è¿™é‡Œæ˜¯æ‹¿ $\eta=\mathrm{sign}(w)$ ç®—çš„ï¼Œä½†æ˜¯æŒ‰ç†è¯´ä¸€ä¸ªé™ç»´æ˜ å°„çš„ä¸­é—´å±‚ï¼Œå…¶å‚æ•°å¤§å°åˆ†å¸ƒç†åº”è¿‘ä¼¼æœä»æ­£æ€åˆ†å¸ƒçš„ï¼Œè¿™é‡Œå–ç¬¦å·å‡½æ•°ç›¸å½“äºæŠŠè¿™ä¸ªæ‰°åŠ¨é¡¹å…¨éƒ½å˜æˆæ­£å€¼ï¼Œä¹Ÿå°±æ˜¯å•ç®—å¤§äº 0 çš„éƒ¨åˆ†çš„å‡å€¼ã€‚å› æ­¤è¿™é‡Œçš„ "average magnitude" è¿˜ä¸èƒ½ç†è§£æˆå‡å€¼è€Œæ˜¯ç»å¯¹å€¼çš„å‡å€¼â€¦â€¦

è¿™ä¹Ÿå°±å¯¼è‡´äº†åŸºäº ReLU çš„æµ…å±‚ç¥ç»ç½‘ç»œç›¸å½“å®¹æ˜“è¢«æ”»å‡»ï¼Œè€ŒåŸºäº sigmoid çš„ç¥ç»ç½‘ç»œå‘¢ï¼Ÿå˜¿å˜¿æƒ³é€ƒæ˜¯é€ƒä¸æ‰çš„â€”â€”ä¸ºäº†é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼Œä½ åœ¨è®­ç»ƒçš„æ—¶å€™å°±è¦æŠŠæƒé‡å‹åˆ° 0 é™„è¿‘ï¼Œè¿™æ­£æ˜¯ sigmoid è¿‘ä¼¼çº¿æ€§çš„åœ°æ–¹ã€‚

## èŒƒæ•°é€‰æ‹©

è¿™ä¸€èŠ‚æˆ‘ä»¬æ¥è®¨è®ºä¸€ä¸‹ä¸ºä»€ä¹ˆè¿™ä¸ªæ–¹æ³•å« FGSM è€Œä¸æ˜¯ FGM æˆ–è€…å…¶ä»–ï¼Œä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆåŸä½œè€…è¦é€‰æ‹©ä½¿ç”¨**ç¬¦å·å‡½æ•°**ã€‚

è¿™é‡Œæˆ‘åœ¨ MNIST ä¸Šè®­ç»ƒäº†ä¸€ä¸ª LeNet æ¥å¯è§†åŒ–ä¸€ä¸‹ã€‚ï¼ˆåŸè®ºæ–‡è¿˜ä½¿ç”¨äº†æ›´å¤§è§„æ¨¡çš„æ•°æ®é›†å¦‚ CIFAR-10 å’Œ ImageNet ç­‰ï¼Œæˆ‘è¿™è¾¹ä¸ºäº†æ–¹ä¾¿å°±ç›´æ¥åœ¨ç¬”è®°æœ¬ä¸Šé¢è·‘å’¯ï¼‰

è®­ç»ƒçš„å‚æ•°æ˜¯ï¼šå­¦ä¹ ç‡ 5e-4 è·‘ 5 ä¸ª epochï¼Œæ— ä»»ä½•æ•°æ®å¢å¼ºï¼Œè®­ç»ƒé›†å‡†ç¡®ç‡ 0.9756ï¼Œæµ‹è¯•é›†å‡†ç¡®ç‡ 0.9775ã€‚

å¦‚ä¸‹ï¼Œæ˜¯ä½¿ç”¨ $L_\infty$ èŒƒæ•°çº¦æŸçš„ç»“æœï¼š

![alt text](image-3.png)

$L_2$ èŒƒæ•°ï¼š

![alt text](image-4.png)

è°±èŒƒæ•°ï¼š

![alt text](image-6.png)

è®©æˆ‘ä»¬æŠŠç›®å…‰èšç„¦åœ¨æœ€ä¸‹é¢ä¸€åˆ—ï¼Œå¯è§ $L_2$ èŒƒæ•°ä¸»è¦æ˜¯å¯¹æ ·æœ¬è¾¹ç¼˜æ·»åŠ ä¸€å¨ä¸å¯åçŠ¶çš„ç¬”è§¦ï¼Œè°±èŒƒæ•°å¼•å…¥çš„æ‰°åŠ¨å‡ ä¹ä¾µèš€äº†æ•´å¹…å›¾åƒï¼Œè€Œ $L_\infty$ çš„æ•ˆæœæ˜¯æœ€å¥½çš„ï¼Œåªæ˜¯å¼•å…¥äº†å¤§å¹…åº¦çš„ç°è‰²è‰²å—ï¼ŒåŸå§‹çš„æ•°å­—å®Œå…¨å¯ä»¥è¢«äººç±»è¾¨è®¤è€Œæ¨¡å‹å‡ ä¹å®Œå…¨å¤±æ•ˆã€‚

äº‹å®ä¸Šçš„ç¡®ï¼Œæ— ç©·èŒƒæ•°æ˜¯è¿™å‡ ä¸ªé‡Œé¢**æœ€é€‚åˆå•æ­¥æ”»å‡»**çš„ã€‚æˆ‘ä»¬æ€ä¹ˆåœ¨ç†è®ºä¸Šç†è§£å‘¢ï¼ŸåŸè®ºæ–‡æ²¡æœ‰è§£é‡Šï¼Œåœ¨è¿™é‡Œæ–—èƒ†ç»™å‡ºæˆ‘çš„ç†è§£ï¼šå¯ä»¥çœ‹åˆ°ä¸ºäº†è¾¾åˆ°ç›¸è¿‘çš„æ”»å‡»æ•ˆæœï¼Œ$L_2$ èŒƒæ•°å’Œè°±èŒƒæ•°çš„ $\epsilon$ éƒ½è¦å¼€ç‰¹åˆ«å¤§ï¼Œå› ä¸ºå¯¹åº”çš„**ä¸€å°éƒ¨åˆ†çš„åƒç´ **è´¡çŒ®äº†è¾ƒå¤§çš„æ‰°åŠ¨ï¼Œä½†æ˜¯å›¾åƒçš„è‰²é˜¶æ˜¯æœ‰ä¸Šé™çš„ï¼Œæ‰°åŠ¨å°é¡¶ä¹‹åï¼Œå°±åªå¥½è®©å…¶ä»–è´¡çŒ®å°çš„åƒç´ å¼ºè¡Œæ‹‰å¤§ï¼Œæœ€åå¯¼è‡´å¯¹åŸå›¾åƒçš„ç ´åç›¸å½“å¤§ï¼›è€Œç¬¦å·å‡½æ•°å¯ä»¥è®©æ‰€æœ‰æœ‰è´¡çŒ®çš„åƒç´ éƒ½æ‹‰å¹³åˆ°ä¸€ä¸ªæ°´å¹³ï¼Œè¿™å°±æ„å‘³ç€æˆ‘å¯ä»¥è®©å‡ ä¹å…¨å›¾çš„åƒç´ éƒ½å¯¹æ‰°åŠ¨é¡¹è¿›è¡Œè´¡çŒ®ã€‚

## â€œå¯¹æŠ—æ ·æœ¬â€å’Œâ€œåƒåœ¾æ ·æœ¬â€

å¾ˆå¤šç½‘ä¸Šçš„åšå®¢åœ¨å‰é¢å°±ç»“æŸäº†ï¼Œä¸è¿‡åŸè®ºæ–‡çš„è¿™ä¸€éƒ¨åˆ†è®¨è®ºçš„é™„å½•è¿˜æ˜¯å¾ˆæœ‰å¯å‘æ€§çš„ã€‚

ç¼˜èµ·æ˜¯æŸåŒå­¦ ~~COSæˆä¸œé£è°·æ—©è‹—~~ åœ¨ä¸€ä¸ªçº¿ä¸Šçš„ MNIST åˆ†ç±»å™¨ä¸Šé¢ç”»äº†ä¸ªæ˜Ÿæ˜Ÿï¼Œç»“æœè¿™ä¸ªåˆ†ç±»å™¨ï¼ˆåº”è¯¥ä¹Ÿæ˜¯ LeNet ä¹‹ç±»çš„ï¼‰ç…æœ‰ä»‹äº‹åœ°ä»¥ä¸€ä¸ªå¾ˆé«˜çš„ç½®ä¿¡åº¦è®¤ä¸ºè¿™ä¸ªæ˜Ÿæ˜Ÿæ˜¯æ•°å­— 8ã€‚

![alt text](e30e2023449f742f2eb1a36b4ee0962f.jpg)

ï¼ˆæ—©è‹—è¯·è‡ªè¡Œæƒ³è±¡ï¼‰

åŸè®ºæ–‡è®¤ä¸ºç¥ç»ç½‘ç»œä¸åŒäº RBF ç½‘ç»œï¼Œå®ƒä»¬åˆ†åˆ«åœ¨ P-R æ›²çº¿çš„ä¸¤ç«¯ï¼š

- ç¥ç»ç½‘ç»œï¼Œå¦‚ LeNet ç­‰ï¼Œå€¾å‘äº**é«˜å¬å›ï¼Œä½ç²¾ç¡®**ï¼Œè¿™å°±æ„å‘³ç€å…¶é¢å¯¹æ˜Ÿæ˜Ÿè¿™ç§å®Œå…¨ä¸åŒäºæ•°å­—çš„â€œåƒåœ¾æ ·æœ¬â€è€Œè¨€ï¼Œä»ç„¶ä¼šä»¥ä¸€ä¸ªå¾ˆé«˜çš„ç½®ä¿¡åº¦æ¥è¯•å›¾åˆ†ç±»ã€‚
- RBF ç½‘ç»œå€¾å‘äº**ä½å¬å›ï¼Œé«˜ç²¾ç¡®**ï¼Œä¹Ÿå°±æ˜¯æ›´è°¨æ…ï¼Œå¯èƒ½æœ‰äº›æ•°å­—æ²¡æ³•åˆ†ç±»ï¼Œä½†æ˜¯å¯¹äºéæ•°å­—çš„æ ·æœ¬ï¼Œä¼šåšå®šåœ°ç»™å‡ºä½ç½®ä¿¡åº¦çš„æ‰“åˆ†ã€‚

åŸè®ºæ–‡è®¤ä¸ºï¼Œæ­£æ˜¯å› ä¸ºç¥ç»ç½‘ç»œçš„å±€éƒ¨çº¿æ€§ï¼Œå¯¼è‡´äº†è¿™ä¸€å€¾å‘ï¼Œè€Œåœ¨â€œåƒåœ¾æ ·æœ¬â€ä¸­ç²¾å¿ƒé€‰æ‹©çš„é‚£äº›å’ŒåŸè¾“å…¥ç›¸å·®æ— å‡ çš„â€œå¯¹æŠ—æ ·æœ¬â€ï¼Œæˆä¸ºäº†å¨èƒå…¶å‡†ç¡®ç‡çš„æ¯’è¯ã€‚

åŸè®ºæ–‡è¿˜åŸºäºç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬çš„åŸç†å¯¹ç½‘ç»œè®­ç»ƒåšäº†æ­£åˆ™åŒ–ï¼Œå¹¶è¡¨ç¤ºè¿™æ¯”åŸºäº Dropout çš„æ­£åˆ™åŒ–æ•ˆæœæ›´å¥½ã€‚

## é™„å½•

å¯è§†åŒ–ä½¿ç”¨çš„ä»£ç å¦‚ä¸‹ï¼š

<details>

<summary> ä»£ç  </summary>

```python
# -*- coding: utf-8 -*-
import time
import random
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# ========== Reproducibility ==========
def set_seed(seed=0):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

set_seed(0)
device = torch.device("cpu")

# ========== Model: Small LeNet ==========
class LeNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)     # 28 -> 24
        self.pool = nn.MaxPool2d(2, 2)      # 24 -> 12
        self.conv2 = nn.Conv2d(6, 16, 5)    # 12 -> 8
        # 8 -> 4 after pool
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)               # 12x12
        x = F.relu(self.conv2(x))
        x = self.pool(x)               # 4x4
        x = torch.flatten(x, 1)        # B x (16*4*4)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)                # logits
        return x

# ========== Data ==========
transform = transforms.ToTensor()  # pixels in [0,1]
train_set = datasets.MNIST(root="./data", train=True, download=True, transform=transform)
test_set  = datasets.MNIST(root="./data", train=False, download=True, transform=transform)
train_loader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=2, pin_memory=False)
test_loader  = DataLoader(test_set,  batch_size=256, shuffle=False, num_workers=2, pin_memory=False)

# ========== Train / Eval ==========
def train(model, loader, epochs=5, lr=5e-4):
    model.train()
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    for ep in range(epochs):
        total, correct, loss_sum = 0, 0, 0.0
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            logits = model(x)
            loss = F.cross_entropy(logits, y)
            opt.zero_grad()
            loss.backward()
            opt.step()
            loss_sum += loss.item() * x.size(0)
            pred = logits.argmax(dim=1)
            correct += (pred == y).sum().item()
            total += x.size(0)
        print(f"Epoch {ep+1}/{epochs} - loss={loss_sum/total:.4f} acc={correct/total:.4f}")

@torch.no_grad()
def eval_clean_acc(model, loader):
    model.eval()
    total, correct = 0, 0
    for x, y in loader:
        x, y = x.to(device), y.to(device)
        logits = model(x)
        pred = logits.argmax(dim=1)
        total += x.size(0)
        correct += (pred == y).sum().item()
    return correct / total

# ========== Gradient + FGSM directions ==========
def grad_wrt_x(model, x, y):
    model.eval()
    x = x.clone().detach().to(device)
    x.requires_grad_(True)
    with torch.enable_grad():  # ç¡®ä¿æ„å»ºè®¡ç®—å›¾
        logits = model(x)
        loss = F.cross_entropy(logits, y.to(device))
        model.zero_grad(set_to_none=True)
        loss.backward()
        g = x.grad.detach()
    return g, logits.detach()

def dir_linf(g):
    return g.sign()

def dir_l2(g, eps=1e-12):
    g_flat = g.view(g.size(0), -1)
    g_norm = g_flat.norm(p=2, dim=1).view(-1, 1, 1, 1)
    return g / (g_norm + eps)

def dir_spec(g):
    # per-sample SVD on 28x28
    B, C, H, W = g.shape
    assert (C, H, W) == (1, 28, 28), "This demo assumes MNIST 1x28x28"
    d = torch.zeros_like(g)
    for i in range(B):
        Gi = g[i, 0]
        U, S, Vh = torch.linalg.svd(Gi, full_matrices=False)
        d[i, 0] = U @ Vh
    return d

def get_direction(method, g):
    if method == "linf":
        return dir_linf(g)
    elif method == "l2":
        return dir_l2(g)
    elif method == "spec":
        return dir_spec(g)
    else:
        raise ValueError("Unknown method")

# ========== Attack evaluation over a loader for a list of eps ==========
@torch.no_grad()
def eval_attack_grid(model, loader, method, eps_list):
    """
    Returns dict with per-eps: acc, mean_maxprob, mean_trueprob, time_sec
    """
    model.eval()
    eps_list = list(eps_list)
    K = len(eps_list)
    total = 0
    correct = [0 for _ in range(K)]
    sum_maxprob = [0.0 for _ in range(K)]
    sum_trueprob = [0.0 for _ in range(K)]
    times = [0.0 for _ in range(K)]

    for x, y in loader:
        x, y = x.to(device), y.to(device)
        # get gradient once per batch
        torch.set_grad_enabled(True)
        g, _ = grad_wrt_x(model, x, y)
        d = get_direction(method, g)
        torch.set_grad_enabled(False)

        for j, eps in enumerate(eps_list):
            t0 = time.perf_counter()
            x_adv = torch.clamp(x + eps * d, 0.0, 1.0)
            logits = model(x_adv)
            probs = logits.softmax(dim=1)
            pred = probs.argmax(dim=1)

            correct[j] += (pred == y).sum().item()
            sum_maxprob[j] += probs.max(dim=1).values.sum().item()
            sum_trueprob[j] += probs[torch.arange(y.size(0)), y].sum().item()
            times[j] += (time.perf_counter() - t0)

        total += x.size(0)

    out = []
    for j, eps in enumerate(eps_list):
        out.append({
            "eps": float(eps),
            "acc": correct[j] / total,
            "mean_maxprob": sum_maxprob[j] / total,
            "mean_trueprob": sum_trueprob[j] / total,
            "time_sec": times[j],
            "n_total": total
        })
    return out

# ========== Fixed sample picker ==========
@torch.no_grad()
def pick_fixed_samples(model, dataset, k=6, seed=0):
    """
    Pick k correctly-classified test samples with fixed seed; returns indices list.
    """
    set_seed(seed)
    idxs = list(range(len(dataset)))
    random.shuffle(idxs)
    chosen = []
    for idx in idxs:
        x, y = dataset[idx]
        x_in = x.unsqueeze(0).to(device)
        logits = model(x_in)
        pred = logits.argmax(dim=1).item()
        if pred == y:
            chosen.append(idx)
        if len(chosen) >= k:
            break
    return chosen

# ========== Build visualization figure per method ==========
@torch.no_grad()
def visualize_method(
    model, dataset, method, eps_list, fixed_indices,
    train_stats, test_stats, figsize_scale=2.0
):
    """
    Build a big figure:
      rows = len(eps_list)
      cols = len(fixed_indices) + 1 (last col is metrics summary)
    Each cell (sample) shows x_adv at the given eps; last col shows train/test acc, conf, time.
    """
    k = len(fixed_indices)
    R = len(eps_list)
    C = k + 1
    fig_w = max(8, int(figsize_scale * C))
    fig_h = max(4, int(figsize_scale * R))
    fig, axes = plt.subplots(R, C, figsize=(fig_w, fig_h))
    if R == 1:
        axes = np.expand_dims(axes, axis=0)
    if C == 1:
        axes = np.expand_dims(axes, axis=1)

    # Header titles (top row)
    for j, idx in enumerate(fixed_indices):
        x0, y0 = dataset[idx]
        # show clean label in column title
        axes[0, j].set_title(f"Sample {j+1}\nidx={idx}, true={y0}", fontsize=9)

    axes[0, -1].set_title("Summary (train/test acc, conf, time)", fontsize=9)

    # For each eps row
    for r, eps in enumerate(eps_list):
        # Left side: adversarial images for fixed samples
        for c, idx in enumerate(fixed_indices):
            x0, y0 = dataset[idx]
            x = x0.unsqueeze(0).to(device)
            y = torch.tensor([y0], dtype=torch.long).to(device)
            # grad & direction for this single sample
            g, _ = grad_wrt_x(model, x, y)
            d = get_direction(method, g)
            x_adv = torch.clamp(x + eps * d, 0.0, 1.0)
            logits = model(x_adv)
            probs = logits.softmax(dim=1)
            conf, pred = probs.max(dim=1)
            ax = axes[r, c]
            ax.imshow(x_adv[0, 0].cpu(), cmap="gray", vmin=0, vmax=1)
            ax.set_xticks([]); ax.set_yticks([])
            ax.set_xlabel(f"Îµ={eps:.3f}\n{pred.item()} ({conf.item()*100:.1f}%)", fontsize=8)

        # Rightmost summary cell
        ax_sum = axes[r, -1]
        ax_sum.axis("off")
        tr = train_stats[r]; te = test_stats[r]
        text = (
            f"Norm={method.upper()} | Îµ={eps:.3f}\n"
            f"Train acc: {tr['acc']*100:.2f}%  (N={tr['n_total']})\n"
            f"Test  acc: {te['acc']*100:.2f}%  (N={te['n_total']})\n"
            f"Test mean max prob: {te['mean_maxprob']*100:.1f}%\n"
            f"Time (train/test): {tr['time_sec']:.2f}s / {te['time_sec']:.2f}s"
        )
        ax_sum.text(0.02, 0.5, text, va="center", ha="left", fontsize=9, family="monospace")

    fig.suptitle(f"FGSM under {method.upper()} norm | rows: eps, cols: fixed samples + summary", fontsize=12)
    fig.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

# ========== Main Pipeline ==========
if __name__ == "__main__":
    set_seed(0)
    model = LeNet().to(device)

    print("Training LeNet on MNIST (CPU)...")
    train(model, train_loader, epochs=5, lr=5e-4)

    clean_train_acc = eval_clean_acc(model, train_loader)
    clean_test_acc  = eval_clean_acc(model, test_loader)
    print(f"Clean acc - train={clean_train_acc:.4f}, test={clean_test_acc:.4f}")

    # ---- Define epsilon grids per norm ----
    eps_grid = {
        "linf": [0.05, 0.10, 0.20, 0.30],
        "l2":   [0.50, 2.00, 3.00, 6.00],
        "spec": [0.10, 0.60, 1.50, 2.20],
    }

    # ---- Pick fixed samples (from test set) ----
    fixed_indices = pick_fixed_samples(model, test_set, k=6, seed=0)
    print("Fixed sample indices (test set):", fixed_indices)

    # ---- For each norm: evaluate grid on train/test, then visualize ----
    for method, eps_list in eps_grid.items():
        print(f"\n=== Evaluating {method.upper()} with eps list: {eps_list} ===")
        train_stats = eval_attack_grid(model, train_loader, method, eps_list)
        test_stats  = eval_attack_grid(model, test_loader,  method, eps_list)

        # Console summary
        print("eps | train_acc | test_acc | test_mean_max_prob | time_train(s) | time_test(s)")
        for tr, te in zip(train_stats, test_stats):
            print(f"{te['eps']:.3f} | {tr['acc']*100:8.2f}% | {te['acc']*100:7.2f}% | "
                  f"{te['mean_maxprob']*100:7.2f}% | {tr['time_sec']:.2f} | {te['time_sec']:.2f}")

        # Visualization big figure
        visualize_method(
            model, test_set, method, eps_list, fixed_indices,
            train_stats, test_stats, figsize_scale=2.0
        )
```

</details>
