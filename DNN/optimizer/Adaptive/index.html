
<!DOCTYPE html>

<html class="no-js" lang="zh">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="../SignGD/" rel="prev"/>
<link href="../SGD/" rel="next"/>
<link href="../../../favicon.png" rel="icon"/>
<meta content="mkdocs-1.6.1, mkdocs-material-9.6.22" name="generator"/>
<title>自适应学习率改进策略 - Dicaeopolis' Wiki</title>
<link href="../../../assets/stylesheets/main.84d31ad4.min.css" rel="stylesheet"/>
<link href="../../../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<link href="../../../themes/css/custom.css" rel="stylesheet"/>
<link href="../../../themes/css/simpleLightbox.min.css" rel="stylesheet"/>
<link href="../../../themes/css/pied_piper.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" rel="stylesheet"/>
<link href="../../../stylesheets/customize.css" rel="stylesheet"/>
<link href="../../../assets/css/custom.css" rel="stylesheet"/>
<script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
</head>
<body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="default" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#_1">
          跳转至
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="页眉" class="md-header__inner md-grid">
<a aria-label="Dicaeopolis' Wiki" class="md-header__button md-logo" data-md-component="logo" href="../../.." title="Dicaeopolis' Wiki">
<img alt="logo" src="../../../favicon.png"/>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Dicaeopolis' Wiki
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              自适应学习率改进策略
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_0" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg>
</label>
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
</label>
<input aria-label="Switch to system preference" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="indigo" data-md-color-scheme="slate" id="__palette_2" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_0" hidden="" title="Switch to system preference">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
</label>
</form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="搜索" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="搜索" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
</label>
<nav aria-label="查找" class="md-search__options">
<button aria-label="清空当前内容" class="md-search__icon md-icon" tabindex="-1" title="清空当前内容" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="" tabindex="0">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<nav aria-label="标签" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item md-tabs__item--active">
<a class="md-tabs__link" href="../../">
          
  
  
    
  
  深度神经网络

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../campus-sources/">
          
  
  
    
  
  校内课程知识

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../misc/">
          
  
  
    
  
  配置和杂谈笔记

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../algorithm/">
          
  
  
    
  
  传统算法与性能

        </a>
</li>
</ul>
</div>
</nav>
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="导航栏" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Dicaeopolis' Wiki" class="md-nav__button md-logo" data-md-component="logo" href="../../.." title="Dicaeopolis' Wiki">
<img alt="logo" src="../../../favicon.png"/>
</a>
    Dicaeopolis' Wiki
  </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_1" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../">
<span class="md-ellipsis">
    深度神经网络
    
  </span>
</a>
<label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_1_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_1">
<span class="md-nav__icon md-icon"></span>
            深度神经网络
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../RL/">
<span class="md-ellipsis">
    RL学习笔记 - 上篇
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../SVM_SMO/">
<span class="md-ellipsis">
    SMO 算法的推导
    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_1_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_1_4" id="__nav_1_4_label" tabindex="0">
<span class="md-ellipsis">
    Wp
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_1_4_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_1_4">
<span class="md-nav__icon md-icon"></span>
            Wp
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../WP/aiwp/">
<span class="md-ellipsis">
    MISC-AI 方向 WriteUp
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_1_5" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../model-attack/">
<span class="md-ellipsis">
    深度学习模型攻击理论与复现归档
    
  </span>
</a>
<label class="md-nav__link" for="__nav_1_5" id="__nav_1_5_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_1_5_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_1_5">
<span class="md-nav__icon md-icon"></span>
            深度学习模型攻击理论与复现归档
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../model-attack/hsja/">
<span class="md-ellipsis">
    HSJA 攻击
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../model-attack/BadNets/">
<span class="md-ellipsis">
    BadNets
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../model-attack/CandW/">
<span class="md-ellipsis">
    C&amp;W 攻击
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../model-attack/pgd/">
<span class="md-ellipsis">
    PGD 攻击
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../model-attack/fgsm/">
<span class="md-ellipsis">
    FGSM 攻击
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_1_6" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../model-expr/">
<span class="md-ellipsis">
    深度学习相关模型理论与复现归档
    
  </span>
</a>
<label class="md-nav__link" for="__nav_1_6" id="__nav_1_6_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_1_6_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_1_6">
<span class="md-nav__icon md-icon"></span>
            深度学习相关模型理论与复现归档
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../model-expr/DDPM/">
<span class="md-ellipsis">
    扩散模型理论篇: 从多阶段变分自编码器到概率流常微分方程采样器系列
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../model-expr/S-and-D-models-replication/">
<span class="md-ellipsis">
    图像语义分割和目标检测相关模型复现手记
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../model-expr/Image-models-replication/">
<span class="md-ellipsis">
    图像分类相关模型复现手记
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../model-expr/DC-GAN-%E8%80%81%E5%A9%86%E7%94%9F%E6%88%90%E5%99%A8/">
<span class="md-ellipsis">
    DC-GAN 老婆生成器
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_1_7" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../">
<span class="md-ellipsis">
    优化器
    
  </span>
</a>
<label class="md-nav__link" for="__nav_1_7" id="__nav_1_7_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_1_7_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_1_7">
<span class="md-nav__icon md-icon"></span>
            优化器
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../misc/">
<span class="md-ellipsis">
    后续的写作计划
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../SignGD/">
<span class="md-ellipsis">
    符号梯度下降
    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    自适应学习率改进策略
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
<span class="md-ellipsis">
    自适应学习率改进策略
    
  </span>
</a>
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      目录
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#adagrad">
<span class="md-ellipsis">
      AdaGrad
    </span>
</a>
<nav aria-label="AdaGrad" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#_2">
<span class="md-ellipsis">
      外积近似
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adagrad_1">
<span class="md-ellipsis">
      AdaGrad 的代码实现
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#rmsprop">
<span class="md-ellipsis">
      RMSprop
    </span>
</a>
<nav aria-label="RMSprop" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#rmsprop_1">
<span class="md-ellipsis">
      RMSprop 的代码实现
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adadelta">
<span class="md-ellipsis">
      AdaDelta
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adam">
<span class="md-ellipsis">
      Adam
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adam_1">
<span class="md-ellipsis">
      Adam 的变体们
    </span>
</a>
<nav aria-label="Adam 的变体们" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#amsgrad">
<span class="md-ellipsis">
      AMSGrad
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adamw">
<span class="md-ellipsis">
      AdamW
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adamax">
<span class="md-ellipsis">
      Adamax
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#nadam">
<span class="md-ellipsis">
      Nadam
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#lamb">
<span class="md-ellipsis">
      LAMB
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#shampoo">
<span class="md-ellipsis">
      Shampoo
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../SGD/">
<span class="md-ellipsis">
    SGD 系列算法
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../../campus-sources/">
<span class="md-ellipsis">
    校内课程知识
    
  </span>
</a>
<label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_2_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_2">
<span class="md-nav__icon md-icon"></span>
            校内课程知识
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../campus-sources/SDC_assignments/">
<span class="md-ellipsis">
    《计算机组成原理》理论课程作业与笔记归档
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../campus-sources/Descrete_assignments/">
<span class="md-ellipsis">
    《离散数学》课程作业与笔记归档
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../campus-sources/PU_Bii_assignments/">
<span class="md-ellipsis">
    《大学物理B下》课程作业与笔记归档
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../campus-sources/Prob_Stats_assignments/">
<span class="md-ellipsis">
    《概率论与数理统计》课程作业与笔记归档
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../campus-sources/binmul/">
<span class="md-ellipsis">
<i class="fas fa-calculator"></i> 二进制乘法可视化工具
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../campus-sources/textbook/">
<span class="md-ellipsis">
    本科教科书电子资源
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../campus-sources/ds-keynote/">
<span class="md-ellipsis">
    《数据结构》划重点笔记
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../campus-sources/ds-write-up/">
<span class="md-ellipsis">
    《数据结构》期末复习题题解
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../campus-sources/autosignin/">
<span class="md-ellipsis">
    自动签到脚本
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../../misc/">
<span class="md-ellipsis">
    配置和杂谈笔记
    
  </span>
</a>
<label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_3_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
            配置和杂谈笔记
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../misc/test/">
<span class="md-ellipsis">
    功能测试页面
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_4" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../../algorithm/">
<span class="md-ellipsis">
    传统算法与性能
    
  </span>
</a>
<label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_4_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_4">
<span class="md-nav__icon md-icon"></span>
            传统算法与性能
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../algorithm/benchmark-on-stl/">
<span class="md-ellipsis">
    STL的一些性能测试
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../algorithm/stl-wheels/">
<span class="md-ellipsis">
    嗯造轮子
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../algorithm/template-on-numeric-ring/">
<span class="md-ellipsis">
    整数环取模模板
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      目录
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#adagrad">
<span class="md-ellipsis">
      AdaGrad
    </span>
</a>
<nav aria-label="AdaGrad" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#_2">
<span class="md-ellipsis">
      外积近似
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adagrad_1">
<span class="md-ellipsis">
      AdaGrad 的代码实现
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#rmsprop">
<span class="md-ellipsis">
      RMSprop
    </span>
</a>
<nav aria-label="RMSprop" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#rmsprop_1">
<span class="md-ellipsis">
      RMSprop 的代码实现
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adadelta">
<span class="md-ellipsis">
      AdaDelta
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adam">
<span class="md-ellipsis">
      Adam
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adam_1">
<span class="md-ellipsis">
      Adam 的变体们
    </span>
</a>
<nav aria-label="Adam 的变体们" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#amsgrad">
<span class="md-ellipsis">
      AMSGrad
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adamw">
<span class="md-ellipsis">
      AdamW
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adamax">
<span class="md-ellipsis">
      Adamax
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#nadam">
<span class="md-ellipsis">
      Nadam
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#lamb">
<span class="md-ellipsis">
      LAMB
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#shampoo">
<span class="md-ellipsis">
      Shampoo
    </span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<h1 id="_1">自适应学习率改进策略<a class="headerlink" href="#_1" title="Permanent link">¶</a></h1>
<div class="admonition info">
<p class="admonition-title">📖 阅读信息</p>
<p>阅读时间约 <strong>49</strong> 分钟　|　约 <strong>4921</strong> 字　|　约 <strong>151</strong> 个公式　|　约 <strong>960</strong> 行代码</p>
</div>
<p>我没要求你一定得用那种最新最好的 Optimizer，我不是恶魔。</p>
<p>可是，用 SGD 优化 LLM 是什么意思？你才 21 岁吧？再这样下去，你 21 岁用 SGD，42 岁用 SGD with Momentum，84 岁就该问 Who is Adam 了。</p>
<p>作为 <span class="arithmatex">\(\theta\)</span>，我可能真该收敛到鞍点，真的。</p>
<h2 id="adagrad">AdaGrad<a class="headerlink" href="#adagrad" title="Permanent link">¶</a></h2>
<h3 id="_2">外积近似<a class="headerlink" href="#_2" title="Permanent link">¶</a></h3>
<p>AdaGrad 的精髓是拿梯度近似海森矩阵 <span class="arithmatex">\(H\)</span>，以此实现自适应调整。但是这需要我们对损失地形有更多的探索。</p>
<p>这一部分，我主要是参考 arXiv:2304.09871 和<a href="https://kexue.fm/archives/10588">苏剑林的这篇博客</a>的内容来推导。</p>
<p>在目标参数 <span class="arithmatex">\(\theta\)</span> 附近取近似解 <span class="arithmatex">\(\theta_n\)</span>，我们可以把梯度做一个一阶近似：<span class="arithmatex">\(g_n=H(\theta-\theta_n)\)</span></p>
<p>近似解可以随机选取，因此考虑其服从 <span class="arithmatex">\(N(\theta, \sigma^2 I)\)</span>，为了弄出平方我们把它乘上自己的转置：</p>
<div class="arithmatex">\[
g_ng_n^\top=H(\theta-\theta_n)(\theta-\theta_n)^\top H^\top
\]</div>
<p>事实上一开始 AdaGrad 就是考虑的采用的这种外积方案，但是计算量过大，我们考虑只取 <span class="arithmatex">\(H\)</span> 的对角元（这个在 SGD 中已经有效地使用过一次了），并且在期望意义下 <span class="arithmatex">\((\theta-\theta_n)(\theta-\theta_n)^\top=E=\sigma^2I\)</span>，这样就可以写成</p>
<div class="arithmatex">\[
H\approx\dfrac{1}{\sigma}\sqrt{g_n\odot g_n}
\]</div>
<p>由此，便可以祭出 AdaGrad 大法了：</p>
<div class="arithmatex">\[
\begin{align*}
    g_n&amp;=\nabla\mathcal{L({x};\theta_{n-1})}\\
    G_{n}&amp;=G_{n-1}+g_n\odot g_n\\
    \theta_n&amp;=\theta_{n-1}-\dfrac{\eta}{\sqrt{\epsilon+G_n}} g_n
\end{align*}
\]</div>
<p>为了防止除零错误，<span class="arithmatex">\(\epsilon\)</span> 是一个小正数。在实践上也会有把 <span class="arithmatex">\(\epsilon\)</span> 提到根号外的情况，都是等价的。</p>
<p>下面是 AdaGrad 的轨迹演示：</p>
<p><img alt="rastrigin_Adagrad" src="../optimizer_pics/rastrigin_Adagrad.gif"/></p>
<p><img alt="rosenbrock_Adagrad" src="../optimizer_pics/rosenbrock_Adagrad.gif"/></p>
<p>可见 AdaGrad 对于大梯度有更大的步长，并且随着进入平缓的部分逐渐衰减。但是这仅仅类似于 SGD 加上一个自适应，并没有对 rosenbrock 这种地形做很好的适应，尤其在后期一直在梯度方向横跳。</p>
<p>下面是 AdaGrad 在 Fashion-MNIST 上面的表现，可能是 CNN 和自适应学习率不大对付，这里的一系列算法的 train_loss 都降得比较难。具体可以看这篇论文：arXiv:1705.08292。所以下面比较就在这几个自适应学习率优化器内部比，毕竟存在这么一个不公平，我可用的 GPU 性能也不支持在一个可行的时间内训练多个大参数量的 Transformer 模型……所以大家将就看吧，有新的实验也欢迎补充数据。</p>
<p><img alt="Adagrad_performance_curves" src="../optimizer_pics/Adagrad_performance_curves.png"/></p>
<p><img alt="Adagrad_landscape_pca" src="../optimizer_pics/Adagrad_landscape_pca.png"/></p>
<p>可以看到就连 6000 个 Batch 后 train_loss 都没有降到 0.1 左右。不过我们确实看到了 AdaGrad 在努力自适应损失地形，相比 SGD 系列算法，AdaGrad 在开头的下降是相当迅速的。</p>
<h3 id="adagrad_1">AdaGrad 的代码实现<a class="headerlink" href="#adagrad_1" title="Permanent link">¶</a></h3>
<p>同样让我们看看 <code>PyTorch</code> 对这个算法的实现。</p>
<details>
<summary>AdaGrad 的实现</summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-0-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-0-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-0-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-0-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-0-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-0-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-0-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-0-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-0-10">10</a></span>
<span class="normal"><a href="#__codelineno-0-11">11</a></span>
<span class="normal"><a href="#__codelineno-0-12">12</a></span>
<span class="normal"><a href="#__codelineno-0-13">13</a></span>
<span class="normal"><a href="#__codelineno-0-14">14</a></span>
<span class="normal"><a href="#__codelineno-0-15">15</a></span>
<span class="normal"><a href="#__codelineno-0-16">16</a></span>
<span class="normal"><a href="#__codelineno-0-17">17</a></span>
<span class="normal"><a href="#__codelineno-0-18">18</a></span>
<span class="normal"><a href="#__codelineno-0-19">19</a></span>
<span class="normal"><a href="#__codelineno-0-20">20</a></span>
<span class="normal"><a href="#__codelineno-0-21">21</a></span>
<span class="normal"><a href="#__codelineno-0-22">22</a></span>
<span class="normal"><a href="#__codelineno-0-23">23</a></span>
<span class="normal"><a href="#__codelineno-0-24">24</a></span>
<span class="normal"><a href="#__codelineno-0-25">25</a></span>
<span class="normal"><a href="#__codelineno-0-26">26</a></span>
<span class="normal"><a href="#__codelineno-0-27">27</a></span>
<span class="normal"><a href="#__codelineno-0-28">28</a></span>
<span class="normal"><a href="#__codelineno-0-29">29</a></span>
<span class="normal"><a href="#__codelineno-0-30">30</a></span>
<span class="normal"><a href="#__codelineno-0-31">31</a></span>
<span class="normal"><a href="#__codelineno-0-32">32</a></span>
<span class="normal"><a href="#__codelineno-0-33">33</a></span>
<span class="normal"><a href="#__codelineno-0-34">34</a></span>
<span class="normal"><a href="#__codelineno-0-35">35</a></span>
<span class="normal"><a href="#__codelineno-0-36">36</a></span>
<span class="normal"><a href="#__codelineno-0-37">37</a></span>
<span class="normal"><a href="#__codelineno-0-38">38</a></span>
<span class="normal"><a href="#__codelineno-0-39">39</a></span>
<span class="normal"><a href="#__codelineno-0-40">40</a></span>
<span class="normal"><a href="#__codelineno-0-41">41</a></span>
<span class="normal"><a href="#__codelineno-0-42">42</a></span>
<span class="normal"><a href="#__codelineno-0-43">43</a></span>
<span class="normal"><a href="#__codelineno-0-44">44</a></span>
<span class="normal"><a href="#__codelineno-0-45">45</a></span>
<span class="normal"><a href="#__codelineno-0-46">46</a></span>
<span class="normal"><a href="#__codelineno-0-47">47</a></span>
<span class="normal"><a href="#__codelineno-0-48">48</a></span>
<span class="normal"><a href="#__codelineno-0-49">49</a></span>
<span class="normal"><a href="#__codelineno-0-50">50</a></span>
<span class="normal"><a href="#__codelineno-0-51">51</a></span>
<span class="normal"><a href="#__codelineno-0-52">52</a></span>
<span class="normal"><a href="#__codelineno-0-53">53</a></span>
<span class="normal"><a href="#__codelineno-0-54">54</a></span>
<span class="normal"><a href="#__codelineno-0-55">55</a></span>
<span class="normal"><a href="#__codelineno-0-56">56</a></span>
<span class="normal"><a href="#__codelineno-0-57">57</a></span>
<span class="normal"><a href="#__codelineno-0-58">58</a></span>
<span class="normal"><a href="#__codelineno-0-59">59</a></span>
<span class="normal"><a href="#__codelineno-0-60">60</a></span>
<span class="normal"><a href="#__codelineno-0-61">61</a></span>
<span class="normal"><a href="#__codelineno-0-62">62</a></span>
<span class="normal"><a href="#__codelineno-0-63">63</a></span>
<span class="normal"><a href="#__codelineno-0-64">64</a></span>
<span class="normal"><a href="#__codelineno-0-65">65</a></span>
<span class="normal"><a href="#__codelineno-0-66">66</a></span>
<span class="normal"><a href="#__codelineno-0-67">67</a></span>
<span class="normal"><a href="#__codelineno-0-68">68</a></span>
<span class="normal"><a href="#__codelineno-0-69">69</a></span>
<span class="normal"><a href="#__codelineno-0-70">70</a></span>
<span class="normal"><a href="#__codelineno-0-71">71</a></span>
<span class="normal"><a href="#__codelineno-0-72">72</a></span>
<span class="normal"><a href="#__codelineno-0-73">73</a></span>
<span class="normal"><a href="#__codelineno-0-74">74</a></span>
<span class="normal"><a href="#__codelineno-0-75">75</a></span>
<span class="normal"><a href="#__codelineno-0-76">76</a></span>
<span class="normal"><a href="#__codelineno-0-77">77</a></span>
<span class="normal"><a href="#__codelineno-0-78">78</a></span>
<span class="normal"><a href="#__codelineno-0-79">79</a></span>
<span class="normal"><a href="#__codelineno-0-80">80</a></span>
<span class="normal"><a href="#__codelineno-0-81">81</a></span>
<span class="normal"><a href="#__codelineno-0-82">82</a></span>
<span class="normal"><a href="#__codelineno-0-83">83</a></span>
<span class="normal"><a href="#__codelineno-0-84">84</a></span>
<span class="normal"><a href="#__codelineno-0-85">85</a></span>
<span class="normal"><a href="#__codelineno-0-86">86</a></span>
<span class="normal"><a href="#__codelineno-0-87">87</a></span>
<span class="normal"><a href="#__codelineno-0-88">88</a></span>
<span class="normal"><a href="#__codelineno-0-89">89</a></span>
<span class="normal"><a href="#__codelineno-0-90">90</a></span>
<span class="normal"><a href="#__codelineno-0-91">91</a></span>
<span class="normal"><a href="#__codelineno-0-92">92</a></span>
<span class="normal"><a href="#__codelineno-0-93">93</a></span>
<span class="normal"><a href="#__codelineno-0-94">94</a></span>
<span class="normal"><a href="#__codelineno-0-95">95</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1"></a><span class="c1"># _get_value(t: Tensor) -&gt; float: 从单元素张量中提取其浮点数值，类似于 t.item()</span>
<a id="__codelineno-0-2" name="__codelineno-0-2"></a><span class="c1"># _make_sparse(grad, grad_indices, grad_values): 使用给定的索引和值创建一个新的稀疏张量</span>
<a id="__codelineno-0-3" name="__codelineno-0-3"></a>
<a id="__codelineno-0-4" name="__codelineno-0-4"></a><span class="k">def</span><span class="w"> </span><span class="nf">_single_tensor_adagrad</span><span class="p">(</span>
<a id="__codelineno-0-5" name="__codelineno-0-5"></a>    <span class="n">params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-0-6" name="__codelineno-0-6"></a>    <span class="n">grads</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-0-7" name="__codelineno-0-7"></a>    <span class="n">state_sums</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-0-8" name="__codelineno-0-8"></a>    <span class="n">state_steps</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-0-9" name="__codelineno-0-9"></a>    <span class="n">grad_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-0-10" name="__codelineno-0-10"></a>    <span class="n">found_inf</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-0-11" name="__codelineno-0-11"></a>    <span class="o">*</span><span class="p">,</span>
<a id="__codelineno-0-12" name="__codelineno-0-12"></a>    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-0-13" name="__codelineno-0-13"></a>    <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-0-14" name="__codelineno-0-14"></a>    <span class="n">lr_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-0-15" name="__codelineno-0-15"></a>    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-0-16" name="__codelineno-0-16"></a>    <span class="n">has_sparse_grad</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-0-17" name="__codelineno-0-17"></a>    <span class="n">maximize</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-0-18" name="__codelineno-0-18"></a>    <span class="n">differentiable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-0-19" name="__codelineno-0-19"></a>    <span class="n">has_complex</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-0-20" name="__codelineno-0-20"></a><span class="p">):</span>
<a id="__codelineno-0-21" name="__codelineno-0-21"></a>    <span class="c1"># 这两个参数与自动混合精度（AMP）的梯度缩放有关，此特定实现不支持，因此断言它们为 None</span>
<a id="__codelineno-0-22" name="__codelineno-0-22"></a>    <span class="k">assert</span> <span class="n">grad_scale</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">found_inf</span> <span class="ow">is</span> <span class="kc">None</span>
<a id="__codelineno-0-23" name="__codelineno-0-23"></a>
<a id="__codelineno-0-24" name="__codelineno-0-24"></a>    <span class="c1"># 使用 zip 同时遍历参数、梯度、状态累加和、步数这四个列表</span>
<a id="__codelineno-0-25" name="__codelineno-0-25"></a>    <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">state_sum</span><span class="p">,</span> <span class="n">step_t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">state_sums</span><span class="p">,</span> <span class="n">state_steps</span><span class="p">):</span>
<a id="__codelineno-0-26" name="__codelineno-0-26"></a>        <span class="c1"># 更新步数计数器（原地操作）</span>
<a id="__codelineno-0-27" name="__codelineno-0-27"></a>        <span class="n">step_t</span> <span class="o">+=</span> <span class="mi">1</span>
<a id="__codelineno-0-28" name="__codelineno-0-28"></a>        <span class="c1"># 从 Tensor 中获取步数的标量值（例如通过 .item()）</span>
<a id="__codelineno-0-29" name="__codelineno-0-29"></a>        <span class="n">step</span> <span class="o">=</span> <span class="n">_get_value</span><span class="p">(</span><span class="n">step_t</span><span class="p">)</span>
<a id="__codelineno-0-30" name="__codelineno-0-30"></a>        <span class="c1"># 如果是最大化问题（maximize=True），则反转梯度方向，执行梯度上升</span>
<a id="__codelineno-0-31" name="__codelineno-0-31"></a>        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">maximize</span> <span class="k">else</span> <span class="o">-</span><span class="n">grad</span>
<a id="__codelineno-0-32" name="__codelineno-0-32"></a>
<a id="__codelineno-0-33" name="__codelineno-0-33"></a>        <span class="c1"># 应用权重衰减（L2 正则化）</span>
<a id="__codelineno-0-34" name="__codelineno-0-34"></a>        <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-0-35" name="__codelineno-0-35"></a>            <span class="c1"># Adagrad 的权重衰减与稀疏梯度不兼容，因为 add 操作在稀疏张量上定义不同</span>
<a id="__codelineno-0-36" name="__codelineno-0-36"></a>            <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
<a id="__codelineno-0-37" name="__codelineno-0-37"></a>                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
<a id="__codelineno-0-38" name="__codelineno-0-38"></a>                    <span class="s2">"weight_decay option is not compatible with sparse gradients"</span> <span class="c1"># 权重衰减选项与稀疏梯度不兼容</span>
<a id="__codelineno-0-39" name="__codelineno-0-39"></a>                <span class="p">)</span>
<a id="__codelineno-0-40" name="__codelineno-0-40"></a>            <span class="c1"># 对于稠密梯度，将权重衰减项加到梯度上。公式: grad = grad + param * weight_decay</span>
<a id="__codelineno-0-41" name="__codelineno-0-41"></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
<a id="__codelineno-0-42" name="__codelineno-0-42"></a>
<a id="__codelineno-0-43" name="__codelineno-0-43"></a>        <span class="c1"># 根据学习率衰减公式，计算当前步骤的有效学习率 (clr)</span>
<a id="__codelineno-0-44" name="__codelineno-0-44"></a>        <span class="c1"># 公式: clr = lr / (1 + (step - 1) * lr_decay)</span>
<a id="__codelineno-0-45" name="__codelineno-0-45"></a>        <span class="n">clr</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">step</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr_decay</span><span class="p">)</span>
<a id="__codelineno-0-46" name="__codelineno-0-46"></a>
<a id="__codelineno-0-47" name="__codelineno-0-47"></a>        <span class="c1"># 根据梯度是稀疏还是稠密，选择不同的更新路径</span>
<a id="__codelineno-0-48" name="__codelineno-0-48"></a>        <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
<a id="__codelineno-0-49" name="__codelineno-0-49"></a>            <span class="c1"># --- 稀疏梯度更新路径 ---</span>
<a id="__codelineno-0-50" name="__codelineno-0-50"></a>            <span class="c1"># 合并稀疏梯度中相同索引的值，确保索引唯一。这对于后续的非线性操作（如平方）是必需的。</span>
<a id="__codelineno-0-51" name="__codelineno-0-51"></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
<a id="__codelineno-0-52" name="__codelineno-0-52"></a>            <span class="n">grad_indices</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">_indices</span><span class="p">()</span>  <span class="c1"># 获取稀疏梯度的非零元素索引</span>
<a id="__codelineno-0-53" name="__codelineno-0-53"></a>            <span class="n">grad_values</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span>   <span class="c1"># 获取稀疏梯度的非零元素值</span>
<a id="__codelineno-0-54" name="__codelineno-0-54"></a>
<a id="__codelineno-0-55" name="__codelineno-0-55"></a>            <span class="c1"># 将当前梯度值的平方，以稀疏张量的形式，累加到历史状态 `state_sum` 中</span>
<a id="__codelineno-0-56" name="__codelineno-0-56"></a>            <span class="n">state_sum</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">_make_sparse</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad_indices</span><span class="p">,</span> <span class="n">grad_values</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
<a id="__codelineno-0-57" name="__codelineno-0-57"></a>            <span class="c1"># 从 `state_sum` 中仅抽取出与当前梯度非零位置相对应的累积值</span>
<a id="__codelineno-0-58" name="__codelineno-0-58"></a>            <span class="n">std</span> <span class="o">=</span> <span class="n">state_sum</span><span class="o">.</span><span class="n">sparse_mask</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
<a id="__codelineno-0-59" name="__codelineno-0-59"></a>            <span class="c1"># 计算分母：对抽出的累积值开方，然后加上 eps 以保证数值稳定性</span>
<a id="__codelineno-0-60" name="__codelineno-0-60"></a>            <span class="n">std_values</span> <span class="o">=</span> <span class="n">std</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span><span class="o">.</span><span class="n">sqrt_</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>
<a id="__codelineno-0-61" name="__codelineno-0-61"></a>            <span class="c1"># 更新参数：仅更新梯度中非零索引对应的参数元素</span>
<a id="__codelineno-0-62" name="__codelineno-0-62"></a>            <span class="c1"># 更新公式：param[indices] -= clr * (grad_values / std_values)</span>
<a id="__codelineno-0-63" name="__codelineno-0-63"></a>            <span class="n">param</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span>
<a id="__codelineno-0-64" name="__codelineno-0-64"></a>                <span class="n">_make_sparse</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad_indices</span><span class="p">,</span> <span class="n">grad_values</span> <span class="o">/</span> <span class="n">std_values</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">clr</span>
<a id="__codelineno-0-65" name="__codelineno-0-65"></a>            <span class="p">)</span>
<a id="__codelineno-0-66" name="__codelineno-0-66"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-67" name="__codelineno-0-67"></a>            <span class="c1"># --- 稠密梯度更新路径 ---</span>
<a id="__codelineno-0-68" name="__codelineno-0-68"></a>            <span class="c1"># 检查参数是否为复数类型</span>
<a id="__codelineno-0-69" name="__codelineno-0-69"></a>            <span class="n">is_complex</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<a id="__codelineno-0-70" name="__codelineno-0-70"></a>            <span class="k">if</span> <span class="n">is_complex</span><span class="p">:</span>
<a id="__codelineno-0-71" name="__codelineno-0-71"></a>                <span class="c1"># 如果是复数，则将其视为一个实数张量进行后续计算，其形状会增加一个维度2（实部和虚部）</span>
<a id="__codelineno-0-72" name="__codelineno-0-72"></a>                <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
<a id="__codelineno-0-73" name="__codelineno-0-73"></a>                <span class="n">state_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">state_sum</span><span class="p">)</span>
<a id="__codelineno-0-74" name="__codelineno-0-74"></a>                <span class="n">param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<a id="__codelineno-0-75" name="__codelineno-0-75"></a>
<a id="__codelineno-0-76" name="__codelineno-0-76"></a>            <span class="c1"># Adagrad 核心步骤：将梯度的平方累加到 state_sum 中（原地操作）</span>
<a id="__codelineno-0-77" name="__codelineno-0-77"></a>            <span class="c1"># 公式: state_sum = state_sum + grad * grad</span>
<a id="__codelineno-0-78" name="__codelineno-0-78"></a>            <span class="n">state_sum</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-79" name="__codelineno-0-79"></a>
<a id="__codelineno-0-80" name="__codelineno-0-80"></a>            <span class="c1"># 计算分母 std = sqrt(state_sum) + eps</span>
<a id="__codelineno-0-81" name="__codelineno-0-81"></a>            <span class="k">if</span> <span class="n">differentiable</span><span class="p">:</span>
<a id="__codelineno-0-82" name="__codelineno-0-82"></a>                <span class="c1"># 如果要求整个优化过程可微分，则使用返回新张量的 `+` 操作</span>
<a id="__codelineno-0-83" name="__codelineno-0-83"></a>                <span class="n">std</span> <span class="o">=</span> <span class="n">state_sum</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">+</span> <span class="n">eps</span>
<a id="__codelineno-0-84" name="__codelineno-0-84"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-85" name="__codelineno-0-85"></a>                <span class="c1"># 否则，使用原地操作 `add_` 以节省内存并可能提高速度</span>
<a id="__codelineno-0-86" name="__codelineno-0-86"></a>                <span class="n">std</span> <span class="o">=</span> <span class="n">state_sum</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>
<a id="__codelineno-0-87" name="__codelineno-0-87"></a>
<a id="__codelineno-0-88" name="__codelineno-0-88"></a>            <span class="c1"># 执行参数更新（原地操作）</span>
<a id="__codelineno-0-89" name="__codelineno-0-89"></a>            <span class="c1"># 公式: param = param - clr * (grad / std)</span>
<a id="__codelineno-0-90" name="__codelineno-0-90"></a>            <span class="n">param</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="n">clr</span><span class="p">)</span>
<a id="__codelineno-0-91" name="__codelineno-0-91"></a>
<a id="__codelineno-0-92" name="__codelineno-0-92"></a>            <span class="c1"># 如果参数是复数，需要将作为实数视图的变量转换回其复数表示</span>
<a id="__codelineno-0-93" name="__codelineno-0-93"></a>            <span class="k">if</span> <span class="n">is_complex</span><span class="p">:</span>
<a id="__codelineno-0-94" name="__codelineno-0-94"></a>                <span class="n">param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_complex</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<a id="__codelineno-0-95" name="__codelineno-0-95"></a>                <span class="n">state_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_complex</span><span class="p">(</span><span class="n">state_sum</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<p>AdaGrad 通过累积的 <span class="arithmatex">\(G\)</span> 来实现对 Hessian 的近似，<strong>按理说</strong>应该具有更加优秀的学习率调度。毕竟，AdaGrad 就是 Adaptive Gradient 的省略嘛！</p>
<p>但是事实上我们可以发现，如果在一个并不好的，梯度很大的初始位置开始进行优化，那累积在 <span class="arithmatex">\(G_n\)</span> 里面的梯度将会是“一辈子都抹不去的东西”，<span class="arithmatex">\(G_n\)</span> 的值只会越来越大，即使走出这样的地方，仍然会因为这个“历史包袱”而寸步难行（也就是初始梯度对全局影响过大）。尤其是刚刚的近似只是对靠近最优点能够很有效，有没有办法从梯度能够获得对 Hessian 矩阵的更好估计呢？这就要祭出 RMSprop 了。</p>
<h2 id="rmsprop">RMSprop<a class="headerlink" href="#rmsprop" title="Permanent link">¶</a></h2>
<p>其实我们想要的是一种“窗口平均”，因为 <span class="arithmatex">\(H\approx\dfrac{1}{\sigma}\sqrt{g_n\odot g_n}\)</span> 是在接近最优点的统计意义下近似的，如果离最优点比较远，那参数更新量大一些也无妨，离最优点比较近，就不要让之前的结果影响到。</p>
<p>这种窗口平均肯定不能直接保存最近 <span class="arithmatex">\(k\)</span> 个梯度的列表再求平均，这显然太费显存：</p>
<div class="arithmatex">\[
G_{n+1}=\frac 1k (kG_n+p_n-p_{n-k})
\]</div>
<p>其中 <span class="arithmatex">\(p_n = g_n\odot g_n\)</span>。不过我们可以把 <span class="arithmatex">\(p_{n-k}\)</span> 近似成 <span class="arithmatex">\(G_n\)</span>，也就是使用平均值来近似单一值，然后做一个变量替换 <span class="arithmatex">\(\beta_2=\dfrac{k-1}{k}\)</span> 来使式子好看，这样我们相比于 AdaGrad，就不用增加任何临时存储了！由此得到的是<strong>滑动窗口平均</strong>，即：</p>
<div class="arithmatex">\[
G_{n+1} = \beta_2 G_n + (1-\beta_2)g_n\odot g_n
\]</div>
<p>这种平均是不是似曾相识？回想起之前关于动量法的讨论（取 <span class="arithmatex">\(\beta_3=(1-\beta_1)\)</span> ）：</p>
<div class="arithmatex">\[
M_n=(1-\beta_1)g_n+\beta_1M_{n-1}
\]</div>
<p>看，这里的动量计算其实也是在取梯度的滑动窗口平均。</p>
<p>这就得到了 RMSprop 算法了：</p>
<div class="arithmatex">\[
\begin{align*}
    g_n&amp;=\nabla\mathcal{L({x};\theta_{n-1})}\\
    G_{n}&amp;=\beta_2 G_n + (1-\beta_2)g_n\odot g_n\\
    \theta_n&amp;=\theta_{n-1}-\dfrac{\eta}{\sqrt{\epsilon+G_n}} g_n
\end{align*}
\]</div>
<p>RMS 指的就是 <span class="arithmatex">\(\sqrt{\epsilon+G_n}\)</span>，既有滑动窗口的平方平均 (Mean Square)，又在最后开了根(Root)。</p>
<p>prop的意思就是传播了。毕竟我们是对神经网络做的优化。</p>
<p>让我们来看看 RMSprop 的轨迹演示：</p>
<p><img alt="rastrigin_RMSprop" src="../optimizer_pics/rastrigin_RMSprop.gif"/></p>
<p><img alt="rosenbrock_RMSprop" src="../optimizer_pics/rosenbrock_RMSprop.gif"/></p>
<p>RMSprop 相比于 AdaGrad 其实只是更改了学习率自适应程度，还是没有逃脱在 rosenbrock 下反复横跳的宿命。这已经不是一般的损失地形了，必须要出重拳（雾）必须要引入动量来调整参数更新方向！——不过这都是后话了，有关讨论敬请参阅 Adam 一节。</p>
<p>下面看看 RMSprop 在 Fashion-MNIST 上面的性能：</p>
<p><img alt="RMSprop_performance_curves" src="../optimizer_pics/RMSprop_performance_curves.png"/></p>
<p><img alt="RMSprop_landscape_pca" src="../optimizer_pics/RMSprop_landscape_pca.png"/></p>
<p>在约 5000 个 Batch 后 RMSprop 的 train_loss 降到了 0.1 附近；约 2000 个 Batch 后 acc 升到了 0.9 以上。相比于 AdaGrad 有相当的提升。</p>
<h3 id="rmsprop_1">RMSprop 的代码实现<a class="headerlink" href="#rmsprop_1" title="Permanent link">¶</a></h3>
<details>
<summary>RMSprop 的实现</summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-1-1">  1</a></span>
<span class="normal"><a href="#__codelineno-1-2">  2</a></span>
<span class="normal"><a href="#__codelineno-1-3">  3</a></span>
<span class="normal"><a href="#__codelineno-1-4">  4</a></span>
<span class="normal"><a href="#__codelineno-1-5">  5</a></span>
<span class="normal"><a href="#__codelineno-1-6">  6</a></span>
<span class="normal"><a href="#__codelineno-1-7">  7</a></span>
<span class="normal"><a href="#__codelineno-1-8">  8</a></span>
<span class="normal"><a href="#__codelineno-1-9">  9</a></span>
<span class="normal"><a href="#__codelineno-1-10"> 10</a></span>
<span class="normal"><a href="#__codelineno-1-11"> 11</a></span>
<span class="normal"><a href="#__codelineno-1-12"> 12</a></span>
<span class="normal"><a href="#__codelineno-1-13"> 13</a></span>
<span class="normal"><a href="#__codelineno-1-14"> 14</a></span>
<span class="normal"><a href="#__codelineno-1-15"> 15</a></span>
<span class="normal"><a href="#__codelineno-1-16"> 16</a></span>
<span class="normal"><a href="#__codelineno-1-17"> 17</a></span>
<span class="normal"><a href="#__codelineno-1-18"> 18</a></span>
<span class="normal"><a href="#__codelineno-1-19"> 19</a></span>
<span class="normal"><a href="#__codelineno-1-20"> 20</a></span>
<span class="normal"><a href="#__codelineno-1-21"> 21</a></span>
<span class="normal"><a href="#__codelineno-1-22"> 22</a></span>
<span class="normal"><a href="#__codelineno-1-23"> 23</a></span>
<span class="normal"><a href="#__codelineno-1-24"> 24</a></span>
<span class="normal"><a href="#__codelineno-1-25"> 25</a></span>
<span class="normal"><a href="#__codelineno-1-26"> 26</a></span>
<span class="normal"><a href="#__codelineno-1-27"> 27</a></span>
<span class="normal"><a href="#__codelineno-1-28"> 28</a></span>
<span class="normal"><a href="#__codelineno-1-29"> 29</a></span>
<span class="normal"><a href="#__codelineno-1-30"> 30</a></span>
<span class="normal"><a href="#__codelineno-1-31"> 31</a></span>
<span class="normal"><a href="#__codelineno-1-32"> 32</a></span>
<span class="normal"><a href="#__codelineno-1-33"> 33</a></span>
<span class="normal"><a href="#__codelineno-1-34"> 34</a></span>
<span class="normal"><a href="#__codelineno-1-35"> 35</a></span>
<span class="normal"><a href="#__codelineno-1-36"> 36</a></span>
<span class="normal"><a href="#__codelineno-1-37"> 37</a></span>
<span class="normal"><a href="#__codelineno-1-38"> 38</a></span>
<span class="normal"><a href="#__codelineno-1-39"> 39</a></span>
<span class="normal"><a href="#__codelineno-1-40"> 40</a></span>
<span class="normal"><a href="#__codelineno-1-41"> 41</a></span>
<span class="normal"><a href="#__codelineno-1-42"> 42</a></span>
<span class="normal"><a href="#__codelineno-1-43"> 43</a></span>
<span class="normal"><a href="#__codelineno-1-44"> 44</a></span>
<span class="normal"><a href="#__codelineno-1-45"> 45</a></span>
<span class="normal"><a href="#__codelineno-1-46"> 46</a></span>
<span class="normal"><a href="#__codelineno-1-47"> 47</a></span>
<span class="normal"><a href="#__codelineno-1-48"> 48</a></span>
<span class="normal"><a href="#__codelineno-1-49"> 49</a></span>
<span class="normal"><a href="#__codelineno-1-50"> 50</a></span>
<span class="normal"><a href="#__codelineno-1-51"> 51</a></span>
<span class="normal"><a href="#__codelineno-1-52"> 52</a></span>
<span class="normal"><a href="#__codelineno-1-53"> 53</a></span>
<span class="normal"><a href="#__codelineno-1-54"> 54</a></span>
<span class="normal"><a href="#__codelineno-1-55"> 55</a></span>
<span class="normal"><a href="#__codelineno-1-56"> 56</a></span>
<span class="normal"><a href="#__codelineno-1-57"> 57</a></span>
<span class="normal"><a href="#__codelineno-1-58"> 58</a></span>
<span class="normal"><a href="#__codelineno-1-59"> 59</a></span>
<span class="normal"><a href="#__codelineno-1-60"> 60</a></span>
<span class="normal"><a href="#__codelineno-1-61"> 61</a></span>
<span class="normal"><a href="#__codelineno-1-62"> 62</a></span>
<span class="normal"><a href="#__codelineno-1-63"> 63</a></span>
<span class="normal"><a href="#__codelineno-1-64"> 64</a></span>
<span class="normal"><a href="#__codelineno-1-65"> 65</a></span>
<span class="normal"><a href="#__codelineno-1-66"> 66</a></span>
<span class="normal"><a href="#__codelineno-1-67"> 67</a></span>
<span class="normal"><a href="#__codelineno-1-68"> 68</a></span>
<span class="normal"><a href="#__codelineno-1-69"> 69</a></span>
<span class="normal"><a href="#__codelineno-1-70"> 70</a></span>
<span class="normal"><a href="#__codelineno-1-71"> 71</a></span>
<span class="normal"><a href="#__codelineno-1-72"> 72</a></span>
<span class="normal"><a href="#__codelineno-1-73"> 73</a></span>
<span class="normal"><a href="#__codelineno-1-74"> 74</a></span>
<span class="normal"><a href="#__codelineno-1-75"> 75</a></span>
<span class="normal"><a href="#__codelineno-1-76"> 76</a></span>
<span class="normal"><a href="#__codelineno-1-77"> 77</a></span>
<span class="normal"><a href="#__codelineno-1-78"> 78</a></span>
<span class="normal"><a href="#__codelineno-1-79"> 79</a></span>
<span class="normal"><a href="#__codelineno-1-80"> 80</a></span>
<span class="normal"><a href="#__codelineno-1-81"> 81</a></span>
<span class="normal"><a href="#__codelineno-1-82"> 82</a></span>
<span class="normal"><a href="#__codelineno-1-83"> 83</a></span>
<span class="normal"><a href="#__codelineno-1-84"> 84</a></span>
<span class="normal"><a href="#__codelineno-1-85"> 85</a></span>
<span class="normal"><a href="#__codelineno-1-86"> 86</a></span>
<span class="normal"><a href="#__codelineno-1-87"> 87</a></span>
<span class="normal"><a href="#__codelineno-1-88"> 88</a></span>
<span class="normal"><a href="#__codelineno-1-89"> 89</a></span>
<span class="normal"><a href="#__codelineno-1-90"> 90</a></span>
<span class="normal"><a href="#__codelineno-1-91"> 91</a></span>
<span class="normal"><a href="#__codelineno-1-92"> 92</a></span>
<span class="normal"><a href="#__codelineno-1-93"> 93</a></span>
<span class="normal"><a href="#__codelineno-1-94"> 94</a></span>
<span class="normal"><a href="#__codelineno-1-95"> 95</a></span>
<span class="normal"><a href="#__codelineno-1-96"> 96</a></span>
<span class="normal"><a href="#__codelineno-1-97"> 97</a></span>
<span class="normal"><a href="#__codelineno-1-98"> 98</a></span>
<span class="normal"><a href="#__codelineno-1-99"> 99</a></span>
<span class="normal"><a href="#__codelineno-1-100">100</a></span>
<span class="normal"><a href="#__codelineno-1-101">101</a></span>
<span class="normal"><a href="#__codelineno-1-102">102</a></span>
<span class="normal"><a href="#__codelineno-1-103">103</a></span>
<span class="normal"><a href="#__codelineno-1-104">104</a></span>
<span class="normal"><a href="#__codelineno-1-105">105</a></span>
<span class="normal"><a href="#__codelineno-1-106">106</a></span>
<span class="normal"><a href="#__codelineno-1-107">107</a></span>
<span class="normal"><a href="#__codelineno-1-108">108</a></span>
<span class="normal"><a href="#__codelineno-1-109">109</a></span>
<span class="normal"><a href="#__codelineno-1-110">110</a></span>
<span class="normal"><a href="#__codelineno-1-111">111</a></span>
<span class="normal"><a href="#__codelineno-1-112">112</a></span>
<span class="normal"><a href="#__codelineno-1-113">113</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">_single_tensor_rmsprop</span><span class="p">(</span>
<a id="__codelineno-1-2" name="__codelineno-1-2"></a>    <span class="n">params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-1-3" name="__codelineno-1-3"></a>    <span class="n">grads</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-1-4" name="__codelineno-1-4"></a>    <span class="n">square_avgs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-1-5" name="__codelineno-1-5"></a>    <span class="n">grad_avgs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-1-6" name="__codelineno-1-6"></a>    <span class="n">momentum_buffer_list</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-1-7" name="__codelineno-1-7"></a>    <span class="n">state_steps</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-1-8" name="__codelineno-1-8"></a>    <span class="o">*</span><span class="p">,</span>
<a id="__codelineno-1-9" name="__codelineno-1-9"></a>    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-1-10" name="__codelineno-1-10"></a>    <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-1-11" name="__codelineno-1-11"></a>    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-1-12" name="__codelineno-1-12"></a>    <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-1-13" name="__codelineno-1-13"></a>    <span class="n">momentum</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-1-14" name="__codelineno-1-14"></a>    <span class="n">centered</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-1-15" name="__codelineno-1-15"></a>    <span class="n">maximize</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-1-16" name="__codelineno-1-16"></a>    <span class="n">differentiable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-1-17" name="__codelineno-1-17"></a>    <span class="n">capturable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-1-18" name="__codelineno-1-18"></a>    <span class="n">has_complex</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-1-19" name="__codelineno-1-19"></a><span class="p">):</span>
<a id="__codelineno-1-20" name="__codelineno-1-20"></a>    <span class="c1"># 循环遍历每一个参数及其对应的梯度和状态</span>
<a id="__codelineno-1-21" name="__codelineno-1-21"></a>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
<a id="__codelineno-1-22" name="__codelineno-1-22"></a>        <span class="c1"># 获取当前参数的更新步数</span>
<a id="__codelineno-1-23" name="__codelineno-1-23"></a>        <span class="n">step</span> <span class="o">=</span> <span class="n">state_steps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-1-24" name="__codelineno-1-24"></a>
<a id="__codelineno-1-25" name="__codelineno-1-25"></a>        <span class="c1"># --- CUDA Graph 捕获相关的检查 ---</span>
<a id="__codelineno-1-26" name="__codelineno-1-26"></a>        <span class="c1"># 如果代码正在被 torch.compile 编译，编译器会处理图捕获的检查。</span>
<a id="__codelineno-1-27" name="__codelineno-1-27"></a>        <span class="c1"># 见 note [torch.compile x capturable]</span>
<a id="__codelineno-1-28" name="__codelineno-1-28"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">is_compiling</span><span class="p">()</span> <span class="ow">and</span> <span class="n">capturable</span><span class="p">:</span>
<a id="__codelineno-1-29" name="__codelineno-1-29"></a>            <span class="c1"># 获取支持 CUDA Graph 捕获的设备列表（通常是 'cuda'）</span>
<a id="__codelineno-1-30" name="__codelineno-1-30"></a>            <span class="n">capturable_supported_devices</span> <span class="o">=</span> <span class="n">_get_capturable_supported_devices</span><span class="p">()</span>
<a id="__codelineno-1-31" name="__codelineno-1-31"></a>            <span class="c1"># 断言：如果启用了 capturable，参数和其状态必须在支持的设备上</span>
<a id="__codelineno-1-32" name="__codelineno-1-32"></a>            <span class="k">assert</span> <span class="p">(</span>
<a id="__codelineno-1-33" name="__codelineno-1-33"></a>                <span class="n">param</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">step</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span>
<a id="__codelineno-1-34" name="__codelineno-1-34"></a>                <span class="ow">and</span> <span class="n">param</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="n">capturable_supported_devices</span>
<a id="__codelineno-1-35" name="__codelineno-1-35"></a>            <span class="p">),</span> <span class="sa">f</span><span class="s2">"If capturable=True, params and state_steps must be on supported devices: </span><span class="si">{</span><span class="n">capturable_supported_devices</span><span class="si">}</span><span class="s2">."</span>
<a id="__codelineno-1-36" name="__codelineno-1-36"></a>
<a id="__codelineno-1-37" name="__codelineno-1-37"></a>        <span class="c1"># 获取当前参数的梯度</span>
<a id="__codelineno-1-38" name="__codelineno-1-38"></a>        <span class="n">grad</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-1-39" name="__codelineno-1-39"></a>        <span class="c1"># 如果是最大化问题 (maximize=True)，则反转梯度方向（梯度上升）</span>
<a id="__codelineno-1-40" name="__codelineno-1-40"></a>        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">maximize</span> <span class="k">else</span> <span class="o">-</span><span class="n">grad</span>
<a id="__codelineno-1-41" name="__codelineno-1-41"></a>        <span class="c1"># 获取当前参数的梯度平方的移动平均值</span>
<a id="__codelineno-1-42" name="__codelineno-1-42"></a>        <span class="n">square_avg</span> <span class="o">=</span> <span class="n">square_avgs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-1-43" name="__codelineno-1-43"></a>
<a id="__codelineno-1-44" name="__codelineno-1-44"></a>        <span class="c1"># 步数加 1</span>
<a id="__codelineno-1-45" name="__codelineno-1-45"></a>        <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
<a id="__codelineno-1-46" name="__codelineno-1-46"></a>
<a id="__codelineno-1-47" name="__codelineno-1-47"></a>        <span class="c1"># --- 权重衰减 (Weight Decay) ---</span>
<a id="__codelineno-1-48" name="__codelineno-1-48"></a>        <span class="c1"># 如果设置了权重衰减（L2 正则化）</span>
<a id="__codelineno-1-49" name="__codelineno-1-49"></a>        <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-1-50" name="__codelineno-1-50"></a>            <span class="c1"># 将权重衰减项加到梯度上。公式: grad = grad + param * weight_decay</span>
<a id="__codelineno-1-51" name="__codelineno-1-51"></a>            <span class="c1"># 也就是解耦的权重衰减。</span>
<a id="__codelineno-1-52" name="__codelineno-1-52"></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
<a id="__codelineno-1-53" name="__codelineno-1-53"></a>
<a id="__codelineno-1-54" name="__codelineno-1-54"></a>        <span class="c1"># --- 处理复数张量 ---</span>
<a id="__codelineno-1-55" name="__codelineno-1-55"></a>        <span class="c1"># 检查参数是否为复数类型</span>
<a id="__codelineno-1-56" name="__codelineno-1-56"></a>        <span class="n">is_complex_param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<a id="__codelineno-1-57" name="__codelineno-1-57"></a>        <span class="k">if</span> <span class="n">is_complex_param</span><span class="p">:</span>
<a id="__codelineno-1-58" name="__codelineno-1-58"></a>            <span class="c1"># 如果是复数，将其视为实数张量进行处理。</span>
<a id="__codelineno-1-59" name="__codelineno-1-59"></a>            <span class="c1"># 例如，一个形状为 [N] 的复数张量会变成形状为 [N, 2] 的实数张量，</span>
<a id="__codelineno-1-60" name="__codelineno-1-60"></a>            <span class="c1"># 最后一维分别代表实部和虚部。</span>
<a id="__codelineno-1-61" name="__codelineno-1-61"></a>            <span class="n">param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<a id="__codelineno-1-62" name="__codelineno-1-62"></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
<a id="__codelineno-1-63" name="__codelineno-1-63"></a>            <span class="n">square_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">square_avg</span><span class="p">)</span>
<a id="__codelineno-1-64" name="__codelineno-1-64"></a>
<a id="__codelineno-1-65" name="__codelineno-1-65"></a>        <span class="c1"># --- 更新梯度平方的移动平均值 (RMS) ---</span>
<a id="__codelineno-1-66" name="__codelineno-1-66"></a>        <span class="c1"># 公式: square_avg = alpha * square_avg + (1 - alpha) * grad^2</span>
<a id="__codelineno-1-67" name="__codelineno-1-67"></a>        <span class="n">square_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span>
<a id="__codelineno-1-68" name="__codelineno-1-68"></a>
<a id="__codelineno-1-69" name="__codelineno-1-69"></a>        <span class="c1"># --- 计算分母 `avg` ---</span>
<a id="__codelineno-1-70" name="__codelineno-1-70"></a>        <span class="k">if</span> <span class="n">centered</span><span class="p">:</span>
<a id="__codelineno-1-71" name="__codelineno-1-71"></a>            <span class="c1"># --- Centered RMSprop ---</span>
<a id="__codelineno-1-72" name="__codelineno-1-72"></a>            <span class="c1"># 获取梯度的移动平均值</span>
<a id="__codelineno-1-73" name="__codelineno-1-73"></a>            <span class="n">grad_avg</span> <span class="o">=</span> <span class="n">grad_avgs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-1-74" name="__codelineno-1-74"></a>            <span class="k">if</span> <span class="n">is_complex_param</span><span class="p">:</span>
<a id="__codelineno-1-75" name="__codelineno-1-75"></a>                <span class="c1"># 同样处理复数情况</span>
<a id="__codelineno-1-76" name="__codelineno-1-76"></a>                <span class="n">grad_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">grad_avg</span><span class="p">)</span>
<a id="__codelineno-1-77" name="__codelineno-1-77"></a>
<a id="__codelineno-1-78" name="__codelineno-1-78"></a>            <span class="c1"># 更新梯度的移动平均值。公式: grad_avg = alpha * grad_avg + (1 - alpha) * grad</span>
<a id="__codelineno-1-79" name="__codelineno-1-79"></a>            <span class="n">grad_avg</span><span class="o">.</span><span class="n">lerp_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span>
<a id="__codelineno-1-80" name="__codelineno-1-80"></a>
<a id="__codelineno-1-81" name="__codelineno-1-81"></a>            <span class="c1"># 计算分母。公式: avg = sqrt(square_avg - grad_avg^2)</span>
<a id="__codelineno-1-82" name="__codelineno-1-82"></a>            <span class="c1"># 这实际上是梯度的（移动）方差的平方根</span>
<a id="__codelineno-1-83" name="__codelineno-1-83"></a>            <span class="n">avg</span> <span class="o">=</span> <span class="n">square_avg</span><span class="o">.</span><span class="n">addcmul</span><span class="p">(</span><span class="n">grad_avg</span><span class="p">,</span> <span class="n">grad_avg</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt_</span><span class="p">()</span>
<a id="__codelineno-1-84" name="__codelineno-1-84"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-1-85" name="__codelineno-1-85"></a>            <span class="c1"># --- 标准 RMSprop ---</span>
<a id="__codelineno-1-86" name="__codelineno-1-86"></a>            <span class="c1"># 计算分母。公式: avg = sqrt(square_avg)</span>
<a id="__codelineno-1-87" name="__codelineno-1-87"></a>            <span class="n">avg</span> <span class="o">=</span> <span class="n">square_avg</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
<a id="__codelineno-1-88" name="__codelineno-1-88"></a>
<a id="__codelineno-1-89" name="__codelineno-1-89"></a>        <span class="c1"># --- 添加 epsilon 以保证数值稳定性 ---</span>
<a id="__codelineno-1-90" name="__codelineno-1-90"></a>        <span class="k">if</span> <span class="n">differentiable</span><span class="p">:</span>
<a id="__codelineno-1-91" name="__codelineno-1-91"></a>            <span class="c1"># 如果要求操作可微分，使用 `add` (返回新张量) 而不是 `add_` (原地修改)</span>
<a id="__codelineno-1-92" name="__codelineno-1-92"></a>            <span class="n">avg</span> <span class="o">=</span> <span class="n">avg</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>
<a id="__codelineno-1-93" name="__codelineno-1-93"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-1-94" name="__codelineno-1-94"></a>            <span class="c1"># 否则，使用原地操作 `add_` 以提高效率，防止分母为零</span>
<a id="__codelineno-1-95" name="__codelineno-1-95"></a>            <span class="n">avg</span> <span class="o">=</span> <span class="n">avg</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>
<a id="__codelineno-1-96" name="__codelineno-1-96"></a>
<a id="__codelineno-1-97" name="__codelineno-1-97"></a>        <span class="c1"># --- 参数更新步骤 ---</span>
<a id="__codelineno-1-98" name="__codelineno-1-98"></a>        <span class="k">if</span> <span class="n">momentum</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-1-99" name="__codelineno-1-99"></a>            <span class="c1"># --- 带动量的更新 ---</span>
<a id="__codelineno-1-100" name="__codelineno-1-100"></a>            <span class="c1"># 获取动量缓冲</span>
<a id="__codelineno-1-101" name="__codelineno-1-101"></a>            <span class="n">buf</span> <span class="o">=</span> <span class="n">momentum_buffer_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-1-102" name="__codelineno-1-102"></a>            <span class="k">if</span> <span class="n">is_complex_param</span><span class="p">:</span>
<a id="__codelineno-1-103" name="__codelineno-1-103"></a>                 <span class="c1"># 同样处理复数情况</span>
<a id="__codelineno-1-104" name="__codelineno-1-104"></a>                <span class="n">buf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
<a id="__codelineno-1-105" name="__codelineno-1-105"></a>
<a id="__codelineno-1-106" name="__codelineno-1-106"></a>            <span class="c1"># 更新动量缓冲。公式: buf = momentum * buf + grad / avg</span>
<a id="__codelineno-1-107" name="__codelineno-1-107"></a>            <span class="n">buf</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">momentum</span><span class="p">)</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">avg</span><span class="p">)</span>
<a id="__codelineno-1-108" name="__codelineno-1-108"></a>            <span class="c1"># 使用动量缓冲更新参数。公式: param = param - lr * buf</span>
<a id="__codelineno-1-109" name="__codelineno-1-109"></a>            <span class="n">param</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">buf</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">lr</span><span class="p">)</span>
<a id="__codelineno-1-110" name="__codelineno-1-110"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-1-111" name="__codelineno-1-111"></a>            <span class="c1"># --- 不带动量的标准更新 ---</span>
<a id="__codelineno-1-112" name="__codelineno-1-112"></a>            <span class="c1"># 直接更新参数。公式: param = param - lr * (grad / avg)</span>
<a id="__codelineno-1-113" name="__codelineno-1-113"></a>            <span class="n">param</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">avg</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="n">lr</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<p>代码里面提到了 Centered RMSprop，其实还是为了解决“不在最小值周围”的问题。因为我们在最小值周围选点，梯度的期望是 <span class="arithmatex">\(0\)</span>，但是如果不在周围，梯度的期望就要另行计算，怎么计算呢？和之前的思路一样，同步对梯度做滑动窗口平均即可，然后计算 <span class="arithmatex">\(G_n\)</span> 的适合，减去这个期望平方值，就相当于做了一次中心化了。</p>
<p>代码里面还提到了“动量缓冲”，可以这样理解：RMSprop 是自适应学习率的 SGD，那么我们用相同的方式给 SGDM 添加自适应学习率，就得到了 RMSprop with Momentum 了，具体实现参考刚刚的代码，其实就是使用动量项 <span class="arithmatex">\(M_n = \beta_3 M_{n-1} + \dfrac{\eta}{\sqrt{\epsilon+G_n}} g_n\)</span> 再乘以学习率作为参数更新量。</p>
<h3 id="adadelta">AdaDelta<a class="headerlink" href="#adadelta" title="Permanent link">¶</a></h3>
<p>让我们回到在 AdaGrad 里面讨论的海森矩阵近似：</p>
<div class="arithmatex">\[
H\approx\dfrac{1}{\sigma}\sqrt{g_n\odot g_n}
\]</div>
<p>在 RMSprop 中，我们能够高效计算 <span class="arithmatex">\(\sqrt{g_n\odot g_n}\)</span>，而对于 <span class="arithmatex">\(\sigma\)</span>，我们直接用学习率估计的，但考虑到 <span class="arithmatex">\(\sigma\)</span> 自身的意义（也就是 <span class="arithmatex">\(\mathbb{E}[(\theta_n-\theta)(\theta_n-\theta)^\top]\)</span> 即参数离最优解的期望欧几里得距离），如果当前预期参数比较远，<span class="arithmatex">\(\sigma\)</span> 就该比较大，反之则较小。怎么估计这个距离呢？AdaDelta 提出的方案是使用<strong>参数更新量的滑动窗口平均</strong>。也就是：</p>
<div class="arithmatex">\[
\begin{align*}
    g_n&amp;=\nabla\mathcal{L({x};\theta_{n-1})}\\
    G_{n}&amp;=\beta_2 G_n + (1-\beta_2)g_n\odot g_n\\
    X_n&amp;=\beta_4X_{n-1}+(1-\beta_4)\Delta\theta_{n-1}\odot\Delta\theta_{n-1}\\
    \theta_n&amp;=\theta_{n-1}-\dfrac{\sqrt{\epsilon+X_n}}{\sqrt{\epsilon+G_n}} g_n
\end{align*}
\]</div>
<p>可以看到 AdaDelta 已经完全实现了自适应调节，连学习率的估计都实现了自动化调整。</p>
<p>让我们看看轨迹：</p>
<p><img alt="rastrigin_Adadelta" src="../optimizer_pics/rastrigin_Adadelta.gif"/></p>
<p><img alt="rosenbrock_Adadelta" src="../optimizer_pics/rosenbrock_Adadelta.gif"/></p>
<p>可以看到相比于之前的几个 Ada（Adaptive 的省写）优化器，尽管 AdaDelta 的学习率大了好几倍，在参数更新量上面还是偏保守。</p>
<p><img alt="Adadelta_performance_curves" src="../optimizer_pics/Adadelta_performance_curves.png"/></p>
<p><img alt="Adadelta_landscape_pca" src="../optimizer_pics/Adadelta_landscape_pca.png"/></p>
<p>在 Fashion-MNIST 上面 AdaDelta 的效果仍然受限于保守的参数更新量，过了 6000 个 batch 后 train_loss 还没收敛到 0.1，不过大概在 3000 个 batch 后 acc 能上 0.9。</p>
<p>下面是 AdaDelta 的代码实现：</p>
<details>
<summary>AdaDelta 的实现</summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-2-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-2-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-2-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-2-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-2-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-2-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-2-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-2-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-2-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-2-10">10</a></span>
<span class="normal"><a href="#__codelineno-2-11">11</a></span>
<span class="normal"><a href="#__codelineno-2-12">12</a></span>
<span class="normal"><a href="#__codelineno-2-13">13</a></span>
<span class="normal"><a href="#__codelineno-2-14">14</a></span>
<span class="normal"><a href="#__codelineno-2-15">15</a></span>
<span class="normal"><a href="#__codelineno-2-16">16</a></span>
<span class="normal"><a href="#__codelineno-2-17">17</a></span>
<span class="normal"><a href="#__codelineno-2-18">18</a></span>
<span class="normal"><a href="#__codelineno-2-19">19</a></span>
<span class="normal"><a href="#__codelineno-2-20">20</a></span>
<span class="normal"><a href="#__codelineno-2-21">21</a></span>
<span class="normal"><a href="#__codelineno-2-22">22</a></span>
<span class="normal"><a href="#__codelineno-2-23">23</a></span>
<span class="normal"><a href="#__codelineno-2-24">24</a></span>
<span class="normal"><a href="#__codelineno-2-25">25</a></span>
<span class="normal"><a href="#__codelineno-2-26">26</a></span>
<span class="normal"><a href="#__codelineno-2-27">27</a></span>
<span class="normal"><a href="#__codelineno-2-28">28</a></span>
<span class="normal"><a href="#__codelineno-2-29">29</a></span>
<span class="normal"><a href="#__codelineno-2-30">30</a></span>
<span class="normal"><a href="#__codelineno-2-31">31</a></span>
<span class="normal"><a href="#__codelineno-2-32">32</a></span>
<span class="normal"><a href="#__codelineno-2-33">33</a></span>
<span class="normal"><a href="#__codelineno-2-34">34</a></span>
<span class="normal"><a href="#__codelineno-2-35">35</a></span>
<span class="normal"><a href="#__codelineno-2-36">36</a></span>
<span class="normal"><a href="#__codelineno-2-37">37</a></span>
<span class="normal"><a href="#__codelineno-2-38">38</a></span>
<span class="normal"><a href="#__codelineno-2-39">39</a></span>
<span class="normal"><a href="#__codelineno-2-40">40</a></span>
<span class="normal"><a href="#__codelineno-2-41">41</a></span>
<span class="normal"><a href="#__codelineno-2-42">42</a></span>
<span class="normal"><a href="#__codelineno-2-43">43</a></span>
<span class="normal"><a href="#__codelineno-2-44">44</a></span>
<span class="normal"><a href="#__codelineno-2-45">45</a></span>
<span class="normal"><a href="#__codelineno-2-46">46</a></span>
<span class="normal"><a href="#__codelineno-2-47">47</a></span>
<span class="normal"><a href="#__codelineno-2-48">48</a></span>
<span class="normal"><a href="#__codelineno-2-49">49</a></span>
<span class="normal"><a href="#__codelineno-2-50">50</a></span>
<span class="normal"><a href="#__codelineno-2-51">51</a></span>
<span class="normal"><a href="#__codelineno-2-52">52</a></span>
<span class="normal"><a href="#__codelineno-2-53">53</a></span>
<span class="normal"><a href="#__codelineno-2-54">54</a></span>
<span class="normal"><a href="#__codelineno-2-55">55</a></span>
<span class="normal"><a href="#__codelineno-2-56">56</a></span>
<span class="normal"><a href="#__codelineno-2-57">57</a></span>
<span class="normal"><a href="#__codelineno-2-58">58</a></span>
<span class="normal"><a href="#__codelineno-2-59">59</a></span>
<span class="normal"><a href="#__codelineno-2-60">60</a></span>
<span class="normal"><a href="#__codelineno-2-61">61</a></span>
<span class="normal"><a href="#__codelineno-2-62">62</a></span>
<span class="normal"><a href="#__codelineno-2-63">63</a></span>
<span class="normal"><a href="#__codelineno-2-64">64</a></span>
<span class="normal"><a href="#__codelineno-2-65">65</a></span>
<span class="normal"><a href="#__codelineno-2-66">66</a></span>
<span class="normal"><a href="#__codelineno-2-67">67</a></span>
<span class="normal"><a href="#__codelineno-2-68">68</a></span>
<span class="normal"><a href="#__codelineno-2-69">69</a></span>
<span class="normal"><a href="#__codelineno-2-70">70</a></span>
<span class="normal"><a href="#__codelineno-2-71">71</a></span>
<span class="normal"><a href="#__codelineno-2-72">72</a></span>
<span class="normal"><a href="#__codelineno-2-73">73</a></span>
<span class="normal"><a href="#__codelineno-2-74">74</a></span>
<span class="normal"><a href="#__codelineno-2-75">75</a></span>
<span class="normal"><a href="#__codelineno-2-76">76</a></span>
<span class="normal"><a href="#__codelineno-2-77">77</a></span>
<span class="normal"><a href="#__codelineno-2-78">78</a></span>
<span class="normal"><a href="#__codelineno-2-79">79</a></span>
<span class="normal"><a href="#__codelineno-2-80">80</a></span>
<span class="normal"><a href="#__codelineno-2-81">81</a></span>
<span class="normal"><a href="#__codelineno-2-82">82</a></span>
<span class="normal"><a href="#__codelineno-2-83">83</a></span>
<span class="normal"><a href="#__codelineno-2-84">84</a></span>
<span class="normal"><a href="#__codelineno-2-85">85</a></span>
<span class="normal"><a href="#__codelineno-2-86">86</a></span>
<span class="normal"><a href="#__codelineno-2-87">87</a></span>
<span class="normal"><a href="#__codelineno-2-88">88</a></span>
<span class="normal"><a href="#__codelineno-2-89">89</a></span>
<span class="normal"><a href="#__codelineno-2-90">90</a></span>
<span class="normal"><a href="#__codelineno-2-91">91</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">_single_tensor_adadelta</span><span class="p">(</span>
<a id="__codelineno-2-2" name="__codelineno-2-2"></a>    <span class="n">params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-2-3" name="__codelineno-2-3"></a>    <span class="n">grads</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-2-4" name="__codelineno-2-4"></a>    <span class="n">square_avgs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-2-5" name="__codelineno-2-5"></a>    <span class="n">acc_deltas</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-2-6" name="__codelineno-2-6"></a>    <span class="n">state_steps</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="c1"># 注意：此函数中 state_steps 仅被递增，但未在核心算法中使用</span>
<a id="__codelineno-2-7" name="__codelineno-2-7"></a>    <span class="o">*</span><span class="p">,</span>
<a id="__codelineno-2-8" name="__codelineno-2-8"></a>    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-2-9" name="__codelineno-2-9"></a>    <span class="n">rho</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-2-10" name="__codelineno-2-10"></a>    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-2-11" name="__codelineno-2-11"></a>    <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-2-12" name="__codelineno-2-12"></a>    <span class="n">maximize</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-2-13" name="__codelineno-2-13"></a>    <span class="n">differentiable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-2-14" name="__codelineno-2-14"></a>    <span class="n">capturable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-2-15" name="__codelineno-2-15"></a>    <span class="n">has_complex</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-2-16" name="__codelineno-2-16"></a><span class="p">):</span>
<a id="__codelineno-2-17" name="__codelineno-2-17"></a>    <span class="c1"># --- CUDA Graph 捕获相关的检查 ---</span>
<a id="__codelineno-2-18" name="__codelineno-2-18"></a>    <span class="c1"># 如果代码正在被 torch.compile 编译，编译器会处理图捕获的检查。</span>
<a id="__codelineno-2-19" name="__codelineno-2-19"></a>    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">is_compiling</span><span class="p">()</span> <span class="ow">and</span> <span class="n">capturable</span><span class="p">:</span>
<a id="__codelineno-2-20" name="__codelineno-2-20"></a>        <span class="c1"># 获取支持 CUDA Graph 捕获的设备列表</span>
<a id="__codelineno-2-21" name="__codelineno-2-21"></a>        <span class="n">capturable_supported_devices</span> <span class="o">=</span> <span class="n">_get_capturable_supported_devices</span><span class="p">(</span>
<a id="__codelineno-2-22" name="__codelineno-2-22"></a>            <span class="n">supports_xla</span><span class="o">=</span><span class="kc">False</span>
<a id="__codelineno-2-23" name="__codelineno-2-23"></a>        <span class="p">)</span>
<a id="__codelineno-2-24" name="__codelineno-2-24"></a>        <span class="c1"># 断言：如果启用 capturable，所有参数和状态都必须在支持的设备上</span>
<a id="__codelineno-2-25" name="__codelineno-2-25"></a>        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
<a id="__codelineno-2-26" name="__codelineno-2-26"></a>            <span class="n">p</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">step</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span>
<a id="__codelineno-2-27" name="__codelineno-2-27"></a>            <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="n">capturable_supported_devices</span>
<a id="__codelineno-2-28" name="__codelineno-2-28"></a>            <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">state_steps</span><span class="p">)</span>
<a id="__codelineno-2-29" name="__codelineno-2-29"></a>        <span class="p">),</span> <span class="sa">f</span><span class="s2">"如果 capturable=True, params 和 state_steps 必须在支持的设备上: </span><span class="si">{</span><span class="n">capturable_supported_devices</span><span class="si">}</span><span class="s2">."</span>
<a id="__codelineno-2-30" name="__codelineno-2-30"></a>
<a id="__codelineno-2-31" name="__codelineno-2-31"></a>    <span class="c1"># 循环遍历每一个参数及其对应的梯度和状态</span>
<a id="__codelineno-2-32" name="__codelineno-2-32"></a>    <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">square_avg</span><span class="p">,</span> <span class="n">acc_delta</span><span class="p">,</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
<a id="__codelineno-2-33" name="__codelineno-2-33"></a>        <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">square_avgs</span><span class="p">,</span> <span class="n">acc_deltas</span><span class="p">,</span> <span class="n">state_steps</span>
<a id="__codelineno-2-34" name="__codelineno-2-34"></a>    <span class="p">):</span>
<a id="__codelineno-2-35" name="__codelineno-2-35"></a>        <span class="c1"># 步数加 1 (在 Adadelta 核心算法中未使用，但为保持优化器接口一致性而保留)</span>
<a id="__codelineno-2-36" name="__codelineno-2-36"></a>        <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
<a id="__codelineno-2-37" name="__codelineno-2-37"></a>        <span class="c1"># 如果是最大化问题，则反转梯度</span>
<a id="__codelineno-2-38" name="__codelineno-2-38"></a>        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">maximize</span> <span class="k">else</span> <span class="o">-</span><span class="n">grad</span>
<a id="__codelineno-2-39" name="__codelineno-2-39"></a>
<a id="__codelineno-2-40" name="__codelineno-2-40"></a>        <span class="c1"># --- 应用权重衰减 ---</span>
<a id="__codelineno-2-41" name="__codelineno-2-41"></a>        <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-2-42" name="__codelineno-2-42"></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
<a id="__codelineno-2-43" name="__codelineno-2-43"></a>
<a id="__codelineno-2-44" name="__codelineno-2-44"></a>        <span class="c1"># --- 处理复数张量 ---</span>
<a id="__codelineno-2-45" name="__codelineno-2-45"></a>        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="n">param</span><span class="p">):</span>
<a id="__codelineno-2-46" name="__codelineno-2-46"></a>            <span class="c1"># 将所有状态和梯度都视为实数张量进行计算</span>
<a id="__codelineno-2-47" name="__codelineno-2-47"></a>            <span class="n">square_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">square_avg</span><span class="p">)</span>
<a id="__codelineno-2-48" name="__codelineno-2-48"></a>            <span class="n">acc_delta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">acc_delta</span><span class="p">)</span>
<a id="__codelineno-2-49" name="__codelineno-2-49"></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
<a id="__codelineno-2-50" name="__codelineno-2-50"></a>
<a id="__codelineno-2-51" name="__codelineno-2-51"></a>        <span class="c1"># --- Adadelta 算法核心步骤 ---</span>
<a id="__codelineno-2-52" name="__codelineno-2-52"></a>
<a id="__codelineno-2-53" name="__codelineno-2-53"></a>        <span class="c1"># 1. 更新梯度平方的移动平均值 E[g^2]_t</span>
<a id="__codelineno-2-54" name="__codelineno-2-54"></a>        <span class="c1"># 公式: E[g^2]_t = rho * E[g^2]_{t-1} + (1 - rho) * g_t^2</span>
<a id="__codelineno-2-55" name="__codelineno-2-55"></a>        <span class="n">square_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">rho</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">rho</span><span class="p">)</span>
<a id="__codelineno-2-56" name="__codelineno-2-56"></a>
<a id="__codelineno-2-57" name="__codelineno-2-57"></a>        <span class="c1"># 2. 计算梯度的均方根 RMS[g]_t</span>
<a id="__codelineno-2-58" name="__codelineno-2-58"></a>        <span class="c1"># 公式: RMS[g]_t = sqrt(E[g^2]_t + eps)</span>
<a id="__codelineno-2-59" name="__codelineno-2-59"></a>        <span class="n">std</span> <span class="o">=</span> <span class="n">square_avg</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt_</span><span class="p">()</span>
<a id="__codelineno-2-60" name="__codelineno-2-60"></a>
<a id="__codelineno-2-61" name="__codelineno-2-61"></a>        <span class="c1"># 3. 计算上一步参数更新量的均方根 RMS[Δx]_{t-1}</span>
<a id="__codelineno-2-62" name="__codelineno-2-62"></a>        <span class="c1"># 公式: RMS[Δx]_{t-1} = sqrt(E[Δx^2]_{t-1} + eps)</span>
<a id="__codelineno-2-63" name="__codelineno-2-63"></a>        <span class="c1"># 这里的 acc_delta 存储的是 E[Δx^2]_{t-1}</span>
<a id="__codelineno-2-64" name="__codelineno-2-64"></a>        <span class="n">delta</span> <span class="o">=</span> <span class="n">acc_delta</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt_</span><span class="p">()</span>
<a id="__codelineno-2-65" name="__codelineno-2-65"></a>
<a id="__codelineno-2-66" name="__codelineno-2-66"></a>        <span class="c1"># 为了可微性，如果需要，克隆 delta，以防后续的原地操作破坏计算图</span>
<a id="__codelineno-2-67" name="__codelineno-2-67"></a>        <span class="k">if</span> <span class="n">differentiable</span><span class="p">:</span>
<a id="__codelineno-2-68" name="__codelineno-2-68"></a>            <span class="n">delta</span> <span class="o">=</span> <span class="n">delta</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<a id="__codelineno-2-69" name="__codelineno-2-69"></a>
<a id="__codelineno-2-70" name="__codelineno-2-70"></a>        <span class="c1"># 4. 计算当前的参数更新量 Δx_t</span>
<a id="__codelineno-2-71" name="__codelineno-2-71"></a>        <span class="c1"># 公式: Δx_t = (RMS[Δx]_{t-1} / RMS[g]_t) * g_t</span>
<a id="__codelineno-2-72" name="__codelineno-2-72"></a>        <span class="c1"># delta.div_(std) 对应 -&gt; / RMS[g]_t</span>
<a id="__codelineno-2-73" name="__codelineno-2-73"></a>        <span class="c1"># .mul_(grad)    对应 -&gt; * g_t</span>
<a id="__codelineno-2-74" name="__codelineno-2-74"></a>        <span class="c1"># 此时，`delta` 变量存储的是计算出的更新量 Δx_t</span>
<a id="__codelineno-2-75" name="__codelineno-2-75"></a>        <span class="n">delta</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">std</span><span class="p">)</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
<a id="__codelineno-2-76" name="__codelineno-2-76"></a>
<a id="__codelineno-2-77" name="__codelineno-2-77"></a>        <span class="c1"># 5. 更新参数更新量平方的移动平均值 E[Δx^2]_t，为下一步做准备</span>
<a id="__codelineno-2-78" name="__codelineno-2-78"></a>        <span class="c1"># 公式: E[Δx^2]_t = rho * E[Δx^2]_{t-1} + (1 - rho) * (Δx_t)^2</span>
<a id="__codelineno-2-79" name="__codelineno-2-79"></a>        <span class="c1"># acc_delta 此时仍是 E[Δx^2]_{t-1}</span>
<a id="__codelineno-2-80" name="__codelineno-2-80"></a>        <span class="n">acc_delta</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">rho</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">rho</span><span class="p">)</span>
<a id="__codelineno-2-81" name="__codelineno-2-81"></a>
<a id="__codelineno-2-82" name="__codelineno-2-82"></a>        <span class="c1"># --- 应用最终更新 ---</span>
<a id="__codelineno-2-83" name="__codelineno-2-83"></a>
<a id="__codelineno-2-84" name="__codelineno-2-84"></a>        <span class="c1"># 如果是复数，将计算出的实数更新量转换回复杂的视图</span>
<a id="__codelineno-2-85" name="__codelineno-2-85"></a>        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="n">param</span><span class="p">):</span>
<a id="__codelineno-2-86" name="__codelineno-2-86"></a>            <span class="n">delta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_complex</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>
<a id="__codelineno-2-87" name="__codelineno-2-87"></a>
<a id="__codelineno-2-88" name="__codelineno-2-88"></a>        <span class="c1"># 6. 更新参数</span>
<a id="__codelineno-2-89" name="__codelineno-2-89"></a>        <span class="c1"># 公式: x_{t+1} = x_t - lr * Δx_t</span>
<a id="__codelineno-2-90" name="__codelineno-2-90"></a>        <span class="c1"># PyTorch 的实现中保留了 lr 作为缩放系数，默认为 1</span>
<a id="__codelineno-2-91" name="__codelineno-2-91"></a>        <span class="n">param</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">lr</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<p>回到我们刚刚在 RMSprop 的讨论上，其实我们已经在动量加速和自适应学习率两条道路上走了很远了，那么，有没有一种方法，能够无缝融合，真正集这两家武功之大成呢？有的，这就是接下来要讨论的 Adam 优化器，也就是目前最广泛使用的一个优化器。</p>
<h2 id="adam">Adam<a class="headerlink" href="#adam" title="Permanent link">¶</a></h2>
<p>我们已经知道，通过滑动窗口平均梯度的平方，可以得到学习率的一个自适应调整；通过引入动量，可以让我们有更快的收敛速率。如果我们将自适应学习率调整融入动量法之中，Adam 优化器就自然而然地诞生了。</p>
<p>具体来说，Adam 优化器是这样计算的：</p>
<div class="arithmatex">\[
\begin{align*}
    g_n&amp;=\nabla\mathcal{L({x};\theta_{n-1})}\\
    M_n&amp;=(1-\beta_1)g_n+\beta_1M_{n-1}\\
    G_{n}&amp;=\beta_2 G_n + (1-\beta_2)g_n\odot g_n\\
    \hat M_n&amp;=\dfrac{M_n}{1-\beta_1^{n}}\\
    \hat G_n&amp;=\dfrac{G_n}{1-\beta_2^{n}}\\
    \theta_n&amp;=\theta_{n-1}-\dfrac{\eta}{\sqrt{\epsilon+\hat G_n}} \hat M_n
\end{align*}
\]</div>
<p>可以看到，<span class="arithmatex">\(M_n\)</span> 和 <span class="arithmatex">\(G_n\)</span> 的计算与先前的优化器并无二致，自适应学习率调整也和 RMSprop 一样。但是 Adam 还额外做了一个<strong>随步数衰减</strong>的缩放，这是因为迭代初期时没有填满滑动窗口导致 <span class="arithmatex">\(M_n\)</span> 和 <span class="arithmatex">\(G_n\)</span> 事实上偏小，所以需要这个 <span class="arithmatex">\(\dfrac{1}{1-\beta^n}\)</span> 来补偿。</p>
<p>现在来看看两个函数下 Adam 优化器的轨迹：</p>
<p><img alt="rastrigin_Adam" src="../optimizer_pics/rastrigin_Adam.gif"/></p>
<p><img alt="rosenbrock_Adam" src="../optimizer_pics/rosenbrock_Adam.gif"/></p>
<p>在自适应学习率的基础上引入动量之后，Adam 的性能相比 RMSprop 可以说是突飞猛进！在 rastrigin 地形下通过初始的大学习率找到正确的谷地然后慢慢衰减学习率下降到精确解；在 rosenbrock 地形下不仅不再反复横跳，还能沿着谷底有效前进。</p>
<p>下面看看 Adam 在 Fashion-MNIST 上的表现：</p>
<p><img alt="Adam_performance_curves" src="../optimizer_pics/Adam_performance_curves.png"/></p>
<p><img alt="Adam_landscape_pca" src="../optimizer_pics/Adam_landscape_pca.png"/></p>
<p>可以看到 Adam 优化器取得了相当优秀的结果：在 3500 个 Batch 后 train_loss 降到了 0.1 附近；900 个 Batch 后 acc 稳定在 0.9 以上。</p>
<p>等着看代码吗？别急，Adam 优化器在提出之后，也是经历了如过山车一般起伏的波折，现在的 Adam 实现早就不是原来那个 Adam 了。</p>
<p>何以见得？且听下回分解。</p>
<h2 id="adam_1">Adam 的变体们<a class="headerlink" href="#adam_1" title="Permanent link">¶</a></h2>
<h3 id="amsgrad">AMSGrad<a class="headerlink" href="#amsgrad" title="Permanent link">¶</a></h3>
<p>人怕出名猪怕壮， Adam 自宣布自己拥有 SOTA 级别的收敛效果后，便遭到了许多批评，其中许多不无道理。第一个扔过来的炸弹是收敛性问题，在 <a href="https://openreview.net/forum?id=ryQu7f-RZ">On the Convergence of Adam and Beyond</a> 这篇文章里，作者认为学习率倒数的差分即</p>
<div class="arithmatex">\[
\Gamma_n = \dfrac{\sqrt{G_n}}{\eta}-\dfrac{\sqrt{G_{n+1}}}{\eta}
\]</div>
<p>由于滑动平均的缘故，没法做到像 SGD 和 AdaGrad 一样，让它恒为正。这意味着学习率虽然自适应调整了，但是一会调大一会调小，在这反复横跳，哪来的收敛？？？</p>
<p>不过存在一个简单粗暴的 clip 方案来解决这个问题。既然你嫌弃学习率一会大，一会小，而造成这个出现变动的核心原因就是 <span class="arithmatex">\(G_n\)</span> 不单调递增，那我直接让 <span class="arithmatex">\(G_n\)</span> 取目前所有 <span class="arithmatex">\(G\)</span> 的最大值，也就是只有出现新的最大值才更新 <span class="arithmatex">\(G_n\)</span>，不就完美解决了嘛！</p>
<p>也就是说相对 Adam，AMSGrad 只做了一点小修改：</p>
<div class="arithmatex">\[
\begin{align*}
    g_n&amp;=\nabla\mathcal{L({x};\theta_{n-1})}\\
    M_n&amp;=(1-\beta_1)g_n+\beta_1M_{n-1}\\
    G_{n}&amp;=\beta_2 G_n + (1-\beta_2)g_n\odot g_n\\
    \hat M_n&amp;=\dfrac{M_n}{1-\beta_1^n}\\
    \hat G_n&amp;=\max\{\hat G_{n-1},G_n\}\\
    \theta_n&amp;=\theta_{n-1}-\dfrac{\eta}{\sqrt{\epsilon+\hat G_n}} \hat M_n
\end{align*}
\]</div>
<p>也就相当于把 Adam 对 <span class="arithmatex">\(G_n\)</span> 的修偏估计换成了取极大值，这样不仅解决了嫌 <span class="arithmatex">\(G_n\)</span> 偏小的问题，还解决了学习率反复横跳的问题，可谓一石二鸟。</p>
<h3 id="adamw">AdamW<a class="headerlink" href="#adamw" title="Permanent link">¶</a></h3>
<p>不过一波未平一波又起，在 arXiv:1711.05101v1 这篇文章里面，作者揭露了 Adam 优化器和 <span class="arithmatex">\(L_2\)</span> 正则化一同使用时出现的问题。</p>
<p>让我们回顾一下怎么在 SGD 上面做权重衰减：</p>
<div class="arithmatex">\[
\begin{align*}
    g_{n} &amp;= -\eta\nabla\left(\mathcal{L}({x};\theta_{n-1})+\dfrac{\lambda}{2}|\theta_{n-1}|^2\right)\\
    &amp;=-\eta\nabla\mathcal{L}({x};\theta_{n-1})-\eta\lambda\theta_{n-1}\\
    \theta_n&amp;=\theta_{n-1}+g_n\\
    &amp;=(1-\eta\lambda)\theta_{n-1}-\eta\nabla\mathcal{L}(x;\theta_{n-1})
\end{align*}
\]</div>
<p>在 SGD 中，将 <span class="arithmatex">\(L_2\)</span> 正则化项的梯度（即 <span class="arithmatex">\(\lambda\theta_{n-1}\)</span>）加到损失梯度上，与最后对权重进行乘性衰减（即乘以 <span class="arithmatex">\((1-\eta\lambda)\)</span>）是等效的。然而，在 Adam 这样的自适应学习率优化器中，这种等效性被打破了。</p>
<p>当时几乎所有的深度学习框架，在实现 Adam 的权重衰减时，都采用了将 <span class="arithmatex">\(L_2\)</span> 正则项的梯度加到 <span class="arithmatex">\(\nabla\mathcal{L}\)</span> 上的方式。这意味着，权重衰减项 <span class="arithmatex">\(\lambda\theta_{n-1}\)</span> 也会被 Adam 的自适应学习率 <span class="arithmatex">\(\dfrac{\eta}{\sqrt{\epsilon+\hat G_n}}\)</span> 所缩放。这会产生一个意想不到的后果：对于那些历史梯度很大（即 <span class="arithmatex">\(G_n\)</span> 很大）的权重，它们获得的权重衰减效果会变小；而对于那些不经常更新、历史梯度很小（即 <span class="arithmatex">\(G_n\)</span> 很小）的权重，它们的权重衰减效果反而更强。这与我们使用权重衰减的初衷——对所有的大权重进行同等惩罚——是相悖的。</p>
<p>AdamW 的提出就是为了解决这个问题。它的核心思想是解耦权重衰减。它不再将权重衰减伪装成 <span class="arithmatex">\(L_2\)</span> 正则化并加入梯度计算，而是将其从梯度更新中分离出来，直接在参数更新的最后一步实现，就像在 SGD 中那样。</p>
<p>这样我们就得到了 AdamW 即带有<strong>解耦权重衰减</strong>的 Adam 优化器：</p>
<div class="arithmatex">\[
\begin{align*}
    g_n&amp;=\nabla\mathcal{L({x};\theta_{n-1})}\\
    M_n&amp;=(1-\beta_1)g_n+\beta_1M_{n-1}\\
    G_{n}&amp;=\beta_2 G_n + (1-\beta_2)g_n\odot g_n\\
    \hat M_n&amp;=\dfrac{M_n}{1-\beta_1^n}\\
    \hat G_n&amp;=\dfrac{G_n}{1-\beta_2^n}\\
    \theta_n&amp;=\theta_{n-1}-\dfrac{\eta}{\sqrt{\epsilon+\hat G_n}} \hat M_n-\eta\lambda\theta_{n-1}
\end{align*}
\]</div>
<p>至此，AdamW 大杀四方，现在已经成为transformer训练中的默认优化器了。</p>
<p>讲了这么多，让我们一窥代码真容：</p>
<details>
<summary> Adam, AMSGrad, AdamW 的实现</summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-3-1">  1</a></span>
<span class="normal"><a href="#__codelineno-3-2">  2</a></span>
<span class="normal"><a href="#__codelineno-3-3">  3</a></span>
<span class="normal"><a href="#__codelineno-3-4">  4</a></span>
<span class="normal"><a href="#__codelineno-3-5">  5</a></span>
<span class="normal"><a href="#__codelineno-3-6">  6</a></span>
<span class="normal"><a href="#__codelineno-3-7">  7</a></span>
<span class="normal"><a href="#__codelineno-3-8">  8</a></span>
<span class="normal"><a href="#__codelineno-3-9">  9</a></span>
<span class="normal"><a href="#__codelineno-3-10"> 10</a></span>
<span class="normal"><a href="#__codelineno-3-11"> 11</a></span>
<span class="normal"><a href="#__codelineno-3-12"> 12</a></span>
<span class="normal"><a href="#__codelineno-3-13"> 13</a></span>
<span class="normal"><a href="#__codelineno-3-14"> 14</a></span>
<span class="normal"><a href="#__codelineno-3-15"> 15</a></span>
<span class="normal"><a href="#__codelineno-3-16"> 16</a></span>
<span class="normal"><a href="#__codelineno-3-17"> 17</a></span>
<span class="normal"><a href="#__codelineno-3-18"> 18</a></span>
<span class="normal"><a href="#__codelineno-3-19"> 19</a></span>
<span class="normal"><a href="#__codelineno-3-20"> 20</a></span>
<span class="normal"><a href="#__codelineno-3-21"> 21</a></span>
<span class="normal"><a href="#__codelineno-3-22"> 22</a></span>
<span class="normal"><a href="#__codelineno-3-23"> 23</a></span>
<span class="normal"><a href="#__codelineno-3-24"> 24</a></span>
<span class="normal"><a href="#__codelineno-3-25"> 25</a></span>
<span class="normal"><a href="#__codelineno-3-26"> 26</a></span>
<span class="normal"><a href="#__codelineno-3-27"> 27</a></span>
<span class="normal"><a href="#__codelineno-3-28"> 28</a></span>
<span class="normal"><a href="#__codelineno-3-29"> 29</a></span>
<span class="normal"><a href="#__codelineno-3-30"> 30</a></span>
<span class="normal"><a href="#__codelineno-3-31"> 31</a></span>
<span class="normal"><a href="#__codelineno-3-32"> 32</a></span>
<span class="normal"><a href="#__codelineno-3-33"> 33</a></span>
<span class="normal"><a href="#__codelineno-3-34"> 34</a></span>
<span class="normal"><a href="#__codelineno-3-35"> 35</a></span>
<span class="normal"><a href="#__codelineno-3-36"> 36</a></span>
<span class="normal"><a href="#__codelineno-3-37"> 37</a></span>
<span class="normal"><a href="#__codelineno-3-38"> 38</a></span>
<span class="normal"><a href="#__codelineno-3-39"> 39</a></span>
<span class="normal"><a href="#__codelineno-3-40"> 40</a></span>
<span class="normal"><a href="#__codelineno-3-41"> 41</a></span>
<span class="normal"><a href="#__codelineno-3-42"> 42</a></span>
<span class="normal"><a href="#__codelineno-3-43"> 43</a></span>
<span class="normal"><a href="#__codelineno-3-44"> 44</a></span>
<span class="normal"><a href="#__codelineno-3-45"> 45</a></span>
<span class="normal"><a href="#__codelineno-3-46"> 46</a></span>
<span class="normal"><a href="#__codelineno-3-47"> 47</a></span>
<span class="normal"><a href="#__codelineno-3-48"> 48</a></span>
<span class="normal"><a href="#__codelineno-3-49"> 49</a></span>
<span class="normal"><a href="#__codelineno-3-50"> 50</a></span>
<span class="normal"><a href="#__codelineno-3-51"> 51</a></span>
<span class="normal"><a href="#__codelineno-3-52"> 52</a></span>
<span class="normal"><a href="#__codelineno-3-53"> 53</a></span>
<span class="normal"><a href="#__codelineno-3-54"> 54</a></span>
<span class="normal"><a href="#__codelineno-3-55"> 55</a></span>
<span class="normal"><a href="#__codelineno-3-56"> 56</a></span>
<span class="normal"><a href="#__codelineno-3-57"> 57</a></span>
<span class="normal"><a href="#__codelineno-3-58"> 58</a></span>
<span class="normal"><a href="#__codelineno-3-59"> 59</a></span>
<span class="normal"><a href="#__codelineno-3-60"> 60</a></span>
<span class="normal"><a href="#__codelineno-3-61"> 61</a></span>
<span class="normal"><a href="#__codelineno-3-62"> 62</a></span>
<span class="normal"><a href="#__codelineno-3-63"> 63</a></span>
<span class="normal"><a href="#__codelineno-3-64"> 64</a></span>
<span class="normal"><a href="#__codelineno-3-65"> 65</a></span>
<span class="normal"><a href="#__codelineno-3-66"> 66</a></span>
<span class="normal"><a href="#__codelineno-3-67"> 67</a></span>
<span class="normal"><a href="#__codelineno-3-68"> 68</a></span>
<span class="normal"><a href="#__codelineno-3-69"> 69</a></span>
<span class="normal"><a href="#__codelineno-3-70"> 70</a></span>
<span class="normal"><a href="#__codelineno-3-71"> 71</a></span>
<span class="normal"><a href="#__codelineno-3-72"> 72</a></span>
<span class="normal"><a href="#__codelineno-3-73"> 73</a></span>
<span class="normal"><a href="#__codelineno-3-74"> 74</a></span>
<span class="normal"><a href="#__codelineno-3-75"> 75</a></span>
<span class="normal"><a href="#__codelineno-3-76"> 76</a></span>
<span class="normal"><a href="#__codelineno-3-77"> 77</a></span>
<span class="normal"><a href="#__codelineno-3-78"> 78</a></span>
<span class="normal"><a href="#__codelineno-3-79"> 79</a></span>
<span class="normal"><a href="#__codelineno-3-80"> 80</a></span>
<span class="normal"><a href="#__codelineno-3-81"> 81</a></span>
<span class="normal"><a href="#__codelineno-3-82"> 82</a></span>
<span class="normal"><a href="#__codelineno-3-83"> 83</a></span>
<span class="normal"><a href="#__codelineno-3-84"> 84</a></span>
<span class="normal"><a href="#__codelineno-3-85"> 85</a></span>
<span class="normal"><a href="#__codelineno-3-86"> 86</a></span>
<span class="normal"><a href="#__codelineno-3-87"> 87</a></span>
<span class="normal"><a href="#__codelineno-3-88"> 88</a></span>
<span class="normal"><a href="#__codelineno-3-89"> 89</a></span>
<span class="normal"><a href="#__codelineno-3-90"> 90</a></span>
<span class="normal"><a href="#__codelineno-3-91"> 91</a></span>
<span class="normal"><a href="#__codelineno-3-92"> 92</a></span>
<span class="normal"><a href="#__codelineno-3-93"> 93</a></span>
<span class="normal"><a href="#__codelineno-3-94"> 94</a></span>
<span class="normal"><a href="#__codelineno-3-95"> 95</a></span>
<span class="normal"><a href="#__codelineno-3-96"> 96</a></span>
<span class="normal"><a href="#__codelineno-3-97"> 97</a></span>
<span class="normal"><a href="#__codelineno-3-98"> 98</a></span>
<span class="normal"><a href="#__codelineno-3-99"> 99</a></span>
<span class="normal"><a href="#__codelineno-3-100">100</a></span>
<span class="normal"><a href="#__codelineno-3-101">101</a></span>
<span class="normal"><a href="#__codelineno-3-102">102</a></span>
<span class="normal"><a href="#__codelineno-3-103">103</a></span>
<span class="normal"><a href="#__codelineno-3-104">104</a></span>
<span class="normal"><a href="#__codelineno-3-105">105</a></span>
<span class="normal"><a href="#__codelineno-3-106">106</a></span>
<span class="normal"><a href="#__codelineno-3-107">107</a></span>
<span class="normal"><a href="#__codelineno-3-108">108</a></span>
<span class="normal"><a href="#__codelineno-3-109">109</a></span>
<span class="normal"><a href="#__codelineno-3-110">110</a></span>
<span class="normal"><a href="#__codelineno-3-111">111</a></span>
<span class="normal"><a href="#__codelineno-3-112">112</a></span>
<span class="normal"><a href="#__codelineno-3-113">113</a></span>
<span class="normal"><a href="#__codelineno-3-114">114</a></span>
<span class="normal"><a href="#__codelineno-3-115">115</a></span>
<span class="normal"><a href="#__codelineno-3-116">116</a></span>
<span class="normal"><a href="#__codelineno-3-117">117</a></span>
<span class="normal"><a href="#__codelineno-3-118">118</a></span>
<span class="normal"><a href="#__codelineno-3-119">119</a></span>
<span class="normal"><a href="#__codelineno-3-120">120</a></span>
<span class="normal"><a href="#__codelineno-3-121">121</a></span>
<span class="normal"><a href="#__codelineno-3-122">122</a></span>
<span class="normal"><a href="#__codelineno-3-123">123</a></span>
<span class="normal"><a href="#__codelineno-3-124">124</a></span>
<span class="normal"><a href="#__codelineno-3-125">125</a></span>
<span class="normal"><a href="#__codelineno-3-126">126</a></span>
<span class="normal"><a href="#__codelineno-3-127">127</a></span>
<span class="normal"><a href="#__codelineno-3-128">128</a></span>
<span class="normal"><a href="#__codelineno-3-129">129</a></span>
<span class="normal"><a href="#__codelineno-3-130">130</a></span>
<span class="normal"><a href="#__codelineno-3-131">131</a></span>
<span class="normal"><a href="#__codelineno-3-132">132</a></span>
<span class="normal"><a href="#__codelineno-3-133">133</a></span>
<span class="normal"><a href="#__codelineno-3-134">134</a></span>
<span class="normal"><a href="#__codelineno-3-135">135</a></span>
<span class="normal"><a href="#__codelineno-3-136">136</a></span>
<span class="normal"><a href="#__codelineno-3-137">137</a></span>
<span class="normal"><a href="#__codelineno-3-138">138</a></span>
<span class="normal"><a href="#__codelineno-3-139">139</a></span>
<span class="normal"><a href="#__codelineno-3-140">140</a></span>
<span class="normal"><a href="#__codelineno-3-141">141</a></span>
<span class="normal"><a href="#__codelineno-3-142">142</a></span>
<span class="normal"><a href="#__codelineno-3-143">143</a></span>
<span class="normal"><a href="#__codelineno-3-144">144</a></span>
<span class="normal"><a href="#__codelineno-3-145">145</a></span>
<span class="normal"><a href="#__codelineno-3-146">146</a></span>
<span class="normal"><a href="#__codelineno-3-147">147</a></span>
<span class="normal"><a href="#__codelineno-3-148">148</a></span>
<span class="normal"><a href="#__codelineno-3-149">149</a></span>
<span class="normal"><a href="#__codelineno-3-150">150</a></span>
<span class="normal"><a href="#__codelineno-3-151">151</a></span>
<span class="normal"><a href="#__codelineno-3-152">152</a></span>
<span class="normal"><a href="#__codelineno-3-153">153</a></span>
<span class="normal"><a href="#__codelineno-3-154">154</a></span>
<span class="normal"><a href="#__codelineno-3-155">155</a></span>
<span class="normal"><a href="#__codelineno-3-156">156</a></span>
<span class="normal"><a href="#__codelineno-3-157">157</a></span>
<span class="normal"><a href="#__codelineno-3-158">158</a></span>
<span class="normal"><a href="#__codelineno-3-159">159</a></span>
<span class="normal"><a href="#__codelineno-3-160">160</a></span>
<span class="normal"><a href="#__codelineno-3-161">161</a></span>
<span class="normal"><a href="#__codelineno-3-162">162</a></span>
<span class="normal"><a href="#__codelineno-3-163">163</a></span>
<span class="normal"><a href="#__codelineno-3-164">164</a></span>
<span class="normal"><a href="#__codelineno-3-165">165</a></span>
<span class="normal"><a href="#__codelineno-3-166">166</a></span>
<span class="normal"><a href="#__codelineno-3-167">167</a></span>
<span class="normal"><a href="#__codelineno-3-168">168</a></span>
<span class="normal"><a href="#__codelineno-3-169">169</a></span>
<span class="normal"><a href="#__codelineno-3-170">170</a></span>
<span class="normal"><a href="#__codelineno-3-171">171</a></span>
<span class="normal"><a href="#__codelineno-3-172">172</a></span>
<span class="normal"><a href="#__codelineno-3-173">173</a></span>
<span class="normal"><a href="#__codelineno-3-174">174</a></span>
<span class="normal"><a href="#__codelineno-3-175">175</a></span>
<span class="normal"><a href="#__codelineno-3-176">176</a></span>
<span class="normal"><a href="#__codelineno-3-177">177</a></span>
<span class="normal"><a href="#__codelineno-3-178">178</a></span>
<span class="normal"><a href="#__codelineno-3-179">179</a></span>
<span class="normal"><a href="#__codelineno-3-180">180</a></span>
<span class="normal"><a href="#__codelineno-3-181">181</a></span>
<span class="normal"><a href="#__codelineno-3-182">182</a></span>
<span class="normal"><a href="#__codelineno-3-183">183</a></span>
<span class="normal"><a href="#__codelineno-3-184">184</a></span>
<span class="normal"><a href="#__codelineno-3-185">185</a></span>
<span class="normal"><a href="#__codelineno-3-186">186</a></span>
<span class="normal"><a href="#__codelineno-3-187">187</a></span>
<span class="normal"><a href="#__codelineno-3-188">188</a></span>
<span class="normal"><a href="#__codelineno-3-189">189</a></span>
<span class="normal"><a href="#__codelineno-3-190">190</a></span>
<span class="normal"><a href="#__codelineno-3-191">191</a></span>
<span class="normal"><a href="#__codelineno-3-192">192</a></span>
<span class="normal"><a href="#__codelineno-3-193">193</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">_single_tensor_adam</span><span class="p">(</span>
<a id="__codelineno-3-2" name="__codelineno-3-2"></a>    <span class="n">params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-3-3" name="__codelineno-3-3"></a>    <span class="n">grads</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-3-4" name="__codelineno-3-4"></a>    <span class="n">exp_avgs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>             <span class="c1"># 一阶矩估计（动量） m_t</span>
<a id="__codelineno-3-5" name="__codelineno-3-5"></a>    <span class="n">exp_avg_sqs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>          <span class="c1"># 二阶矩估计（自适应学习率项） v_t</span>
<a id="__codelineno-3-6" name="__codelineno-3-6"></a>    <span class="n">max_exp_avg_sqs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>      <span class="c1"># AMSGrad 用的历史最大二阶矩</span>
<a id="__codelineno-3-7" name="__codelineno-3-7"></a>    <span class="n">state_steps</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>          <span class="c1"># 步数 t</span>
<a id="__codelineno-3-8" name="__codelineno-3-8"></a>    <span class="n">grad_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-3-9" name="__codelineno-3-9"></a>    <span class="n">found_inf</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-3-10" name="__codelineno-3-10"></a>    <span class="o">*</span><span class="p">,</span>
<a id="__codelineno-3-11" name="__codelineno-3-11"></a>    <span class="n">amsgrad</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>                      <span class="c1"># 是否启用 AMSGrad</span>
<a id="__codelineno-3-12" name="__codelineno-3-12"></a>    <span class="n">has_complex</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-3-13" name="__codelineno-3-13"></a>    <span class="n">beta1</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>        <span class="c1"># 一阶矩的指数衰减率</span>
<a id="__codelineno-3-14" name="__codelineno-3-14"></a>    <span class="n">beta2</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>        <span class="c1"># 二阶矩的指数衰减率</span>
<a id="__codelineno-3-15" name="__codelineno-3-15"></a>    <span class="n">lr</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>           <span class="c1"># 学习率</span>
<a id="__codelineno-3-16" name="__codelineno-3-16"></a>    <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>                <span class="c1"># 权重衰减系数</span>
<a id="__codelineno-3-17" name="__codelineno-3-17"></a>    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>                         <span class="c1"># 防止除以零的极小值</span>
<a id="__codelineno-3-18" name="__codelineno-3-18"></a>    <span class="n">maximize</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-3-19" name="__codelineno-3-19"></a>    <span class="n">capturable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>                   <span class="c1"># 是否支持 CUDA Graph 捕获</span>
<a id="__codelineno-3-20" name="__codelineno-3-20"></a>    <span class="n">differentiable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>               <span class="c1"># 是否要求操作可微分</span>
<a id="__codelineno-3-21" name="__codelineno-3-21"></a>    <span class="n">decoupled_weight_decay</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>       <span class="c1"># 是否使用 AdamW 的解耦权重衰减</span>
<a id="__codelineno-3-22" name="__codelineno-3-22"></a><span class="p">):</span>
<a id="__codelineno-3-23" name="__codelineno-3-23"></a>    <span class="k">assert</span> <span class="n">grad_scale</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">found_inf</span> <span class="ow">is</span> <span class="kc">None</span>
<a id="__codelineno-3-24" name="__codelineno-3-24"></a>
<a id="__codelineno-3-25" name="__codelineno-3-25"></a>    <span class="c1"># 如果在 TorchScript (JIT) 环境下，由于 JIT 对类型推断的限制，直接断言超参数为 float</span>
<a id="__codelineno-3-26" name="__codelineno-3-26"></a>    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
<a id="__codelineno-3-27" name="__codelineno-3-27"></a>        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span>
<a id="__codelineno-3-28" name="__codelineno-3-28"></a>        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">beta1</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span>
<a id="__codelineno-3-29" name="__codelineno-3-29"></a>        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">beta2</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span>
<a id="__codelineno-3-30" name="__codelineno-3-30"></a>
<a id="__codelineno-3-31" name="__codelineno-3-31"></a>    <span class="c1"># 为了优化，如果 beta1 是 Tensor，预先将其按设备和类型存入字典，避免循环内重复转换</span>
<a id="__codelineno-3-32" name="__codelineno-3-32"></a>    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">beta1</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
<a id="__codelineno-3-33" name="__codelineno-3-33"></a>        <span class="n">beta1_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DeviceDtypeDict</span><span class="p">]</span> <span class="o">=</span> <span class="p">{(</span><span class="n">beta1</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">beta1</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span> <span class="n">beta1</span><span class="p">}</span>
<a id="__codelineno-3-34" name="__codelineno-3-34"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-3-35" name="__codelineno-3-35"></a>        <span class="n">beta1_dict</span> <span class="o">=</span> <span class="kc">None</span>
<a id="__codelineno-3-36" name="__codelineno-3-36"></a>
<a id="__codelineno-3-37" name="__codelineno-3-37"></a>    <span class="c1"># 循环处理每个参数</span>
<a id="__codelineno-3-38" name="__codelineno-3-38"></a>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
<a id="__codelineno-3-39" name="__codelineno-3-39"></a>        <span class="n">grad</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">maximize</span> <span class="k">else</span> <span class="o">-</span><span class="n">grads</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-3-40" name="__codelineno-3-40"></a>        <span class="n">exp_avg</span> <span class="o">=</span> <span class="n">exp_avgs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-3-41" name="__codelineno-3-41"></a>        <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">exp_avg_sqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-3-42" name="__codelineno-3-42"></a>        <span class="n">step_t</span> <span class="o">=</span> <span class="n">state_steps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-3-43" name="__codelineno-3-43"></a>
<a id="__codelineno-3-44" name="__codelineno-3-44"></a>        <span class="c1"># --- CUDA Graph 捕获检查 ---</span>
<a id="__codelineno-3-45" name="__codelineno-3-45"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">is_compiling</span><span class="p">()</span> <span class="ow">and</span> <span class="n">capturable</span><span class="p">:</span>
<a id="__codelineno-3-46" name="__codelineno-3-46"></a>            <span class="n">capturable_supported_devices</span> <span class="o">=</span> <span class="n">_get_capturable_supported_devices</span><span class="p">()</span>
<a id="__codelineno-3-47" name="__codelineno-3-47"></a>            <span class="k">assert</span> <span class="p">(</span>
<a id="__codelineno-3-48" name="__codelineno-3-48"></a>                <span class="n">param</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">step_t</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span>
<a id="__codelineno-3-49" name="__codelineno-3-49"></a>                <span class="ow">and</span> <span class="n">param</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="n">capturable_supported_devices</span>
<a id="__codelineno-3-50" name="__codelineno-3-50"></a>            <span class="p">),</span> <span class="sa">f</span><span class="s2">"If capturable=True, params and state_steps must be on supported devices: </span><span class="si">{</span><span class="n">capturable_supported_devices</span><span class="si">}</span><span class="s2">."</span>
<a id="__codelineno-3-51" name="__codelineno-3-51"></a>
<a id="__codelineno-3-52" name="__codelineno-3-52"></a>        <span class="c1"># 步数加 1</span>
<a id="__codelineno-3-53" name="__codelineno-3-53"></a>        <span class="n">step_t</span> <span class="o">+=</span> <span class="mi">1</span>
<a id="__codelineno-3-54" name="__codelineno-3-54"></a>
<a id="__codelineno-3-55" name="__codelineno-3-55"></a>        <span class="c1"># --- 步骤 1: 应用权重衰减 ---</span>
<a id="__codelineno-3-56" name="__codelineno-3-56"></a>        <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-3-57" name="__codelineno-3-57"></a>            <span class="k">if</span> <span class="n">decoupled_weight_decay</span><span class="p">:</span>
<a id="__codelineno-3-58" name="__codelineno-3-58"></a>                <span class="c1"># AdamW: 解耦权重衰减。直接在参数上乘以一个衰减因子。</span>
<a id="__codelineno-3-59" name="__codelineno-3-59"></a>                <span class="c1"># 公式: param_t = param_t * (1 - lr * weight_decay)</span>
<a id="__codelineno-3-60" name="__codelineno-3-60"></a>                <span class="n">param</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">weight_decay</span><span class="p">)</span>
<a id="__codelineno-3-61" name="__codelineno-3-61"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-3-62" name="__codelineno-3-62"></a>                <span class="c1"># 标准 Adam: 权重衰减作为 L2 正则化项加入梯度。</span>
<a id="__codelineno-3-63" name="__codelineno-3-63"></a>                <span class="c1"># 公式: grad_t = grad_t + weight_decay * param_{t-1}</span>
<a id="__codelineno-3-64" name="__codelineno-3-64"></a>                <span class="c1"># 嵌套 if 是为了处理可微分和 JIT 的情况</span>
<a id="__codelineno-3-65" name="__codelineno-3-65"></a>                <span class="k">if</span> <span class="n">differentiable</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_decay</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
<a id="__codelineno-3-66" name="__codelineno-3-66"></a>                    <span class="k">if</span> <span class="n">weight_decay</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
<a id="__codelineno-3-67" name="__codelineno-3-67"></a>                        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">addcmul</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">weight_decay</span><span class="p">)</span>
<a id="__codelineno-3-68" name="__codelineno-3-68"></a>                    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-3-69" name="__codelineno-3-69"></a>                        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
<a id="__codelineno-3-70" name="__codelineno-3-70"></a>                <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-3-71" name="__codelineno-3-71"></a>                    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
<a id="__codelineno-3-72" name="__codelineno-3-72"></a>
<a id="__codelineno-3-73" name="__codelineno-3-73"></a>        <span class="c1"># --- 处理复数 ---</span>
<a id="__codelineno-3-74" name="__codelineno-3-74"></a>        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="n">param</span><span class="p">):</span>
<a id="__codelineno-3-75" name="__codelineno-3-75"></a>            <span class="c1"># 将所有相关张量都视为实数进行计算</span>
<a id="__codelineno-3-76" name="__codelineno-3-76"></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
<a id="__codelineno-3-77" name="__codelineno-3-77"></a>            <span class="n">exp_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">)</span>
<a id="__codelineno-3-78" name="__codelineno-3-78"></a>            <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">exp_avg_sq</span><span class="p">)</span>
<a id="__codelineno-3-79" name="__codelineno-3-79"></a>            <span class="k">if</span> <span class="n">amsgrad</span><span class="p">:</span>
<a id="__codelineno-3-80" name="__codelineno-3-80"></a>                <span class="n">max_exp_avg_sqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">max_exp_avg_sqs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<a id="__codelineno-3-81" name="__codelineno-3-81"></a>            <span class="n">param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<a id="__codelineno-3-82" name="__codelineno-3-82"></a>
<a id="__codelineno-3-83" name="__codelineno-3-83"></a>        <span class="n">device</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">device</span>
<a id="__codelineno-3-84" name="__codelineno-3-84"></a>
<a id="__codelineno-3-85" name="__codelineno-3-85"></a>        <span class="c1"># 如果 beta1 是 Tensor，从字典中获取对应设备和类型的版本</span>
<a id="__codelineno-3-86" name="__codelineno-3-86"></a>        <span class="k">if</span> <span class="n">beta1_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-3-87" name="__codelineno-3-87"></a>            <span class="n">dtype</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">dtype</span>
<a id="__codelineno-3-88" name="__codelineno-3-88"></a>            <span class="n">key</span> <span class="o">=</span> <span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
<a id="__codelineno-3-89" name="__codelineno-3-89"></a>            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">beta1_dict</span><span class="p">:</span>
<a id="__codelineno-3-90" name="__codelineno-3-90"></a>                <span class="n">beta1_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta1</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-3-91" name="__codelineno-3-91"></a>            <span class="n">device_beta1</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta1_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
<a id="__codelineno-3-92" name="__codelineno-3-92"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-3-93" name="__codelineno-3-93"></a>            <span class="n">device_beta1</span> <span class="o">=</span> <span class="n">beta1</span>
<a id="__codelineno-3-94" name="__codelineno-3-94"></a>
<a id="__codelineno-3-95" name="__codelineno-3-95"></a>        <span class="c1"># --- 步骤 2: 更新一阶和二阶矩估计 ---</span>
<a id="__codelineno-3-96" name="__codelineno-3-96"></a>        <span class="c1"># 更新一阶矩估计 m_t (exp_avg)</span>
<a id="__codelineno-3-97" name="__codelineno-3-97"></a>        <span class="c1"># 公式: m_t = beta1 * m_{t-1} + (1 - beta1) * grad_t</span>
<a id="__codelineno-3-98" name="__codelineno-3-98"></a>        <span class="n">exp_avg</span><span class="o">.</span><span class="n">lerp_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">device_beta1</span><span class="p">)</span>
<a id="__codelineno-3-99" name="__codelineno-3-99"></a>
<a id="__codelineno-3-100" name="__codelineno-3-100"></a>        <span class="c1"># 更新二阶矩估计 v_t (exp_avg_sq)</span>
<a id="__codelineno-3-101" name="__codelineno-3-101"></a>        <span class="c1"># 公式: v_t = beta2 * v_{t-1} + (1 - beta2) * grad_t^2</span>
<a id="__codelineno-3-102" name="__codelineno-3-102"></a>        <span class="c1"># 同样，嵌套 if 是为了处理可微分情况</span>
<a id="__codelineno-3-103" name="__codelineno-3-103"></a>        <span class="k">if</span> <span class="n">differentiable</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">beta2</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
<a id="__codelineno-3-104" name="__codelineno-3-104"></a>            <span class="k">if</span> <span class="n">beta2</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
<a id="__codelineno-3-105" name="__codelineno-3-105"></a>                <span class="c1"># 使用 lerp 实现可微分的更新，数学上等价于下面的 addcmul</span>
<a id="__codelineno-3-106" name="__codelineno-3-106"></a>                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">lerp_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">grad</span><span class="p">),</span> <span class="n">weight</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>
<a id="__codelineno-3-107" name="__codelineno-3-107"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-3-108" name="__codelineno-3-108"></a>                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>
<a id="__codelineno-3-109" name="__codelineno-3-109"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-3-110" name="__codelineno-3-110"></a>            <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>
<a id="__codelineno-3-111" name="__codelineno-3-111"></a>
<a id="__codelineno-3-112" name="__codelineno-3-112"></a>        <span class="c1"># --- 步骤 3: 参数更新 ---</span>
<a id="__codelineno-3-113" name="__codelineno-3-113"></a>        <span class="c1"># capturable 或 differentiable 模式下，所有计算都使用张量操作以保留计算图</span>
<a id="__codelineno-3-114" name="__codelineno-3-114"></a>        <span class="k">if</span> <span class="n">capturable</span> <span class="ow">or</span> <span class="n">differentiable</span><span class="p">:</span>
<a id="__codelineno-3-115" name="__codelineno-3-115"></a>            <span class="n">step</span> <span class="o">=</span> <span class="n">step_t</span>
<a id="__codelineno-3-116" name="__codelineno-3-116"></a>
<a id="__codelineno-3-117" name="__codelineno-3-117"></a>            <span class="c1"># --- 计算偏差修正项 ---</span>
<a id="__codelineno-3-118" name="__codelineno-3-118"></a>            <span class="c1"># 嵌套 if 用于处理 beta 是可微张量的情况</span>
<a id="__codelineno-3-119" name="__codelineno-3-119"></a>            <span class="k">if</span> <span class="n">differentiable</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">beta1</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
<a id="__codelineno-3-120" name="__codelineno-3-120"></a>                <span class="k">if</span> <span class="n">beta1</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
<a id="__codelineno-3-121" name="__codelineno-3-121"></a>                    <span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">step</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<a id="__codelineno-3-122" name="__codelineno-3-122"></a>                <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-3-123" name="__codelineno-3-123"></a>                    <span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="o">**</span><span class="n">step</span>
<a id="__codelineno-3-124" name="__codelineno-3-124"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-3-125" name="__codelineno-3-125"></a>                <span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="o">**</span><span class="n">step</span>
<a id="__codelineno-3-126" name="__codelineno-3-126"></a>
<a id="__codelineno-3-127" name="__codelineno-3-127"></a>            <span class="k">if</span> <span class="n">differentiable</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">beta2</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
<a id="__codelineno-3-128" name="__codelineno-3-128"></a>                <span class="k">if</span> <span class="n">beta2</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
<a id="__codelineno-3-129" name="__codelineno-3-129"></a>                    <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">step</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<a id="__codelineno-3-130" name="__codelineno-3-130"></a>                <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-3-131" name="__codelineno-3-131"></a>                    <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="o">**</span><span class="n">step</span>
<a id="__codelineno-3-132" name="__codelineno-3-132"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-3-133" name="__codelineno-3-133"></a>                <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="o">**</span><span class="n">step</span>
<a id="__codelineno-3-134" name="__codelineno-3-134"></a>
<a id="__codelineno-3-135" name="__codelineno-3-135"></a>            <span class="c1"># --- 计算更新步长和分母 ---</span>
<a id="__codelineno-3-136" name="__codelineno-3-136"></a>            <span class="n">step_size</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">/</span> <span class="n">bias_correction1</span>
<a id="__codelineno-3-137" name="__codelineno-3-137"></a>            <span class="n">step_size_neg</span> <span class="o">=</span> <span class="n">step_size</span><span class="o">.</span><span class="n">neg</span><span class="p">()</span>
<a id="__codelineno-3-138" name="__codelineno-3-138"></a>            <span class="n">bias_correction2_sqrt</span> <span class="o">=</span> <span class="n">bias_correction2</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
<a id="__codelineno-3-139" name="__codelineno-3-139"></a>
<a id="__codelineno-3-140" name="__codelineno-3-140"></a>            <span class="k">if</span> <span class="n">amsgrad</span><span class="p">:</span>
<a id="__codelineno-3-141" name="__codelineno-3-141"></a>                <span class="c1"># AMSGrad: 维护历史最大二阶矩</span>
<a id="__codelineno-3-142" name="__codelineno-3-142"></a>                <span class="k">if</span> <span class="n">differentiable</span><span class="p">:</span>
<a id="__codelineno-3-143" name="__codelineno-3-143"></a>                    <span class="n">max_exp_avg_sq</span> <span class="o">=</span> <span class="n">max_exp_avg_sqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<a id="__codelineno-3-144" name="__codelineno-3-144"></a>                <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-3-145" name="__codelineno-3-145"></a>                    <span class="n">max_exp_avg_sq</span> <span class="o">=</span> <span class="n">max_exp_avg_sqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-3-146" name="__codelineno-3-146"></a>
<a id="__codelineno-3-147" name="__codelineno-3-147"></a>                <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">max_exp_avg_sq</span><span class="p">,</span> <span class="n">exp_avg_sq</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">max_exp_avg_sqs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<a id="__codelineno-3-148" name="__codelineno-3-148"></a>
<a id="__codelineno-3-149" name="__codelineno-3-149"></a>                <span class="c1"># 使用最大二阶矩计算分母</span>
<a id="__codelineno-3-150" name="__codelineno-3-150"></a>                <span class="c1"># 这里做了一些数学变换，将 step_size 合并计算，以减少张量读写</span>
<a id="__codelineno-3-151" name="__codelineno-3-151"></a>                <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-3-152" name="__codelineno-3-152"></a>                    <span class="n">max_exp_avg_sqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">bias_correction2_sqrt</span> <span class="o">*</span> <span class="n">step_size_neg</span><span class="p">)</span>
<a id="__codelineno-3-153" name="__codelineno-3-153"></a>                <span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">eps</span> <span class="o">/</span> <span class="n">step_size_neg</span><span class="p">)</span>
<a id="__codelineno-3-154" name="__codelineno-3-154"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-3-155" name="__codelineno-3-155"></a>                <span class="c1"># 标准 Adam: 使用当前二阶矩计算分母</span>
<a id="__codelineno-3-156" name="__codelineno-3-156"></a>                <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-3-157" name="__codelineno-3-157"></a>                    <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">bias_correction2_sqrt</span> <span class="o">*</span> <span class="n">step_size_neg</span><span class="p">)</span>
<a id="__codelineno-3-158" name="__codelineno-3-158"></a>                <span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">eps</span> <span class="o">/</span> <span class="n">step_size_neg</span><span class="p">)</span>
<a id="__codelineno-3-159" name="__codelineno-3-159"></a>
<a id="__codelineno-3-160" name="__codelineno-3-160"></a>            <span class="c1"># 执行最终更新</span>
<a id="__codelineno-3-161" name="__codelineno-3-161"></a>            <span class="k">if</span> <span class="n">differentiable</span><span class="p">:</span>
<a id="__codelineno-3-162" name="__codelineno-3-162"></a>                <span class="n">param</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="n">exp_avg</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">denom</span><span class="p">)</span>
<a id="__codelineno-3-163" name="__codelineno-3-163"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-3-164" name="__codelineno-3-164"></a>                <span class="n">param</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">,</span> <span class="n">denom</span><span class="p">)</span>
<a id="__codelineno-3-165" name="__codelineno-3-165"></a>
<a id="__codelineno-3-166" name="__codelineno-3-166"></a>        <span class="c1"># 非 capturable/differentiable 的常规路径（效率更高）</span>
<a id="__codelineno-3-167" name="__codelineno-3-167"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-3-168" name="__codelineno-3-168"></a>            <span class="n">step</span> <span class="o">=</span> <span class="n">_get_value</span><span class="p">(</span><span class="n">step_t</span><span class="p">)</span>
<a id="__codelineno-3-169" name="__codelineno-3-169"></a>
<a id="__codelineno-3-170" name="__codelineno-3-170"></a>            <span class="c1"># --- 计算偏差修正项 ---</span>
<a id="__codelineno-3-171" name="__codelineno-3-171"></a>            <span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="o">**</span><span class="n">step</span>
<a id="__codelineno-3-172" name="__codelineno-3-172"></a>            <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="o">**</span><span class="n">step</span>
<a id="__codelineno-3-173" name="__codelineno-3-173"></a>
<a id="__codelineno-3-174" name="__codelineno-3-174"></a>            <span class="c1"># --- 计算步长和分母 ---</span>
<a id="__codelineno-3-175" name="__codelineno-3-175"></a>            <span class="n">step_size</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">/</span> <span class="n">bias_correction1</span>
<a id="__codelineno-3-176" name="__codelineno-3-176"></a>            <span class="n">bias_correction2_sqrt</span> <span class="o">=</span> <span class="n">bias_correction2</span><span class="o">**</span><span class="mf">0.5</span>
<a id="__codelineno-3-177" name="__codelineno-3-177"></a>
<a id="__codelineno-3-178" name="__codelineno-3-178"></a>            <span class="k">if</span> <span class="n">amsgrad</span><span class="p">:</span>
<a id="__codelineno-3-179" name="__codelineno-3-179"></a>                <span class="c1"># AMSGrad: 更新并使用历史最大二阶矩</span>
<a id="__codelineno-3-180" name="__codelineno-3-180"></a>                <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">max_exp_avg_sqs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">exp_avg_sq</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">max_exp_avg_sqs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<a id="__codelineno-3-181" name="__codelineno-3-181"></a>                <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_exp_avg_sqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">bias_correction2_sqrt</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>
<a id="__codelineno-3-182" name="__codelineno-3-182"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-3-183" name="__codelineno-3-183"></a>                <span class="c1"># 标准 Adam: 使用当前二阶矩</span>
<a id="__codelineno-3-184" name="__codelineno-3-184"></a>                <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">bias_correction2_sqrt</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>
<a id="__codelineno-3-185" name="__codelineno-3-185"></a>
<a id="__codelineno-3-186" name="__codelineno-3-186"></a>            <span class="c1"># --- 执行最终更新 ---</span>
<a id="__codelineno-3-187" name="__codelineno-3-187"></a>            <span class="c1"># 公式: param_t = param_{t-1} - step_size * (m_hat / (sqrt(v_hat) + eps))</span>
<a id="__codelineno-3-188" name="__codelineno-3-188"></a>            <span class="n">param</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">,</span> <span class="n">denom</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="n">step_size</span><span class="p">)</span>
<a id="__codelineno-3-189" name="__codelineno-3-189"></a>
<a id="__codelineno-3-190" name="__codelineno-3-190"></a>        <span class="c1"># --- 复数转换回来 ---</span>
<a id="__codelineno-3-191" name="__codelineno-3-191"></a>        <span class="c1"># 如果启用了 AMSGrad 并且参数是复数，将 max_exp_avg_sqs 视图转换回来</span>
<a id="__codelineno-3-192" name="__codelineno-3-192"></a>        <span class="k">if</span> <span class="n">amsgrad</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
<a id="__codelineno-3-193" name="__codelineno-3-193"></a>            <span class="n">max_exp_avg_sqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_complex</span><span class="p">(</span><span class="n">max_exp_avg_sqs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre></div></td></tr></table></div>
</details>

<h3 id="adamax">Adamax<a class="headerlink" href="#adamax" title="Permanent link">¶</a></h3>
<p>先前提到 Adam 由于无法控制 <span class="arithmatex">\(G_n\)</span> 的单调性而可能陷入无法收敛的状况，并且也介绍了 AMSGrad 提出的 clip 方案。而 Adamax 却提出了一个有所不同的 clip 方案。</p>
<p>Adamax 的思想，最初是想把 <span class="arithmatex">\(G_n\)</span> 对平均梯度的 <span class="arithmatex">\(L_2\)</span> 估计（也就是 <span class="arithmatex">\(g_n\odot g_n\)</span> 项）扩展到 <span class="arithmatex">\(L_p\)</span> 估计：</p>
<div class="arithmatex">\[
G_{n}=\beta_2 G_n + (1-\beta_2)g_n^p\\
\theta_n=\theta_{n-1}-\dfrac{\eta}{G_n^{\frac 1p}} M_n
\]</div>
<p>我们单独把学习率自适应权重 <span class="arithmatex">\(G_n^{\frac 1p}\)</span> 提取出来展开算：</p>
<div class="arithmatex">\[
\begin{align*}
    G_n^{\frac 1p} &amp;= \beta_2 G_n + (1-\beta_2)g_n^p\\
    &amp;=(1-\beta_2)^{\frac 1p}\left(\sum_{i=1}^{n}\beta_2^i g^p_{n-i}\right)^{\frac 1p}
\end{align*}
\]</div>
<p>显然这种推广在任意的 <span class="arithmatex">\(p\)</span> 下是无法解决任何问题的，但是如果我们让 <span class="arithmatex">\(p\rightarrow\infty\)</span> 也就是取 <span class="arithmatex">\(L_\infty\)</span> 范数，就会得到：</p>
<div class="arithmatex">\[
\begin{align*}
    \lim_{p\rightarrow\infty}(1-\beta_2)^{\frac 1p}\left(\sum_{i=1}^{n}\beta_2^i g^p_{n-i}\right)^{\frac 1p}&amp;=\lim_{p\rightarrow\infty}\left(\sum_{i=1}^{n}\beta_2^i g^p_{n-i}\right)^{\frac 1p}\\
    &amp;=\max\left\{\beta_2^i |g_{n-i}|\right\}_{i=1\dots n}
\end{align*}
\]</div>
<p>写成递推式子就是</p>
<div class="arithmatex">\[
G_n = \max\{\beta_2G_{n-1}, |g_n|\}
\]</div>
<p>因此 Adamax 宣称自己相对 Adam，能够解决不收敛问题，还可以简省计算量。不过这样魔改，真的能对 Hessian 做更好的估计吗……</p>
<p>看它在这两个损失地形上的表现，其实还不错：</p>
<p><img alt="rastrigin_Adamax" src="../optimizer_pics/rastrigin_Adamax.gif"/></p>
<p><img alt="rosenbrock_Adamax" src="../optimizer_pics/rosenbrock_Adamax.gif"/></p>
<p>再看看实际任务上面的表现：</p>
<p><img alt="Adamax_performance_curves" src="../optimizer_pics/Adamax_performance_curves.png"/></p>
<p><img alt="Adamax_landscape_pca" src="../optimizer_pics/Adamax_landscape_pca.png"/></p>
<p>还是较 Adam 略逊一筹啊，train_loss 降到 0.1 附近要花费接近 6000 个 Batch；acc 升到 0.9 附近需要接近 1500 个 Batch。</p>
<p>还是来看看代码实现吧：</p>
<details>
<summary> Adamax 的实现</summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-4-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-4-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-4-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-4-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-4-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-4-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-4-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-4-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-4-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-4-10">10</a></span>
<span class="normal"><a href="#__codelineno-4-11">11</a></span>
<span class="normal"><a href="#__codelineno-4-12">12</a></span>
<span class="normal"><a href="#__codelineno-4-13">13</a></span>
<span class="normal"><a href="#__codelineno-4-14">14</a></span>
<span class="normal"><a href="#__codelineno-4-15">15</a></span>
<span class="normal"><a href="#__codelineno-4-16">16</a></span>
<span class="normal"><a href="#__codelineno-4-17">17</a></span>
<span class="normal"><a href="#__codelineno-4-18">18</a></span>
<span class="normal"><a href="#__codelineno-4-19">19</a></span>
<span class="normal"><a href="#__codelineno-4-20">20</a></span>
<span class="normal"><a href="#__codelineno-4-21">21</a></span>
<span class="normal"><a href="#__codelineno-4-22">22</a></span>
<span class="normal"><a href="#__codelineno-4-23">23</a></span>
<span class="normal"><a href="#__codelineno-4-24">24</a></span>
<span class="normal"><a href="#__codelineno-4-25">25</a></span>
<span class="normal"><a href="#__codelineno-4-26">26</a></span>
<span class="normal"><a href="#__codelineno-4-27">27</a></span>
<span class="normal"><a href="#__codelineno-4-28">28</a></span>
<span class="normal"><a href="#__codelineno-4-29">29</a></span>
<span class="normal"><a href="#__codelineno-4-30">30</a></span>
<span class="normal"><a href="#__codelineno-4-31">31</a></span>
<span class="normal"><a href="#__codelineno-4-32">32</a></span>
<span class="normal"><a href="#__codelineno-4-33">33</a></span>
<span class="normal"><a href="#__codelineno-4-34">34</a></span>
<span class="normal"><a href="#__codelineno-4-35">35</a></span>
<span class="normal"><a href="#__codelineno-4-36">36</a></span>
<span class="normal"><a href="#__codelineno-4-37">37</a></span>
<span class="normal"><a href="#__codelineno-4-38">38</a></span>
<span class="normal"><a href="#__codelineno-4-39">39</a></span>
<span class="normal"><a href="#__codelineno-4-40">40</a></span>
<span class="normal"><a href="#__codelineno-4-41">41</a></span>
<span class="normal"><a href="#__codelineno-4-42">42</a></span>
<span class="normal"><a href="#__codelineno-4-43">43</a></span>
<span class="normal"><a href="#__codelineno-4-44">44</a></span>
<span class="normal"><a href="#__codelineno-4-45">45</a></span>
<span class="normal"><a href="#__codelineno-4-46">46</a></span>
<span class="normal"><a href="#__codelineno-4-47">47</a></span>
<span class="normal"><a href="#__codelineno-4-48">48</a></span>
<span class="normal"><a href="#__codelineno-4-49">49</a></span>
<span class="normal"><a href="#__codelineno-4-50">50</a></span>
<span class="normal"><a href="#__codelineno-4-51">51</a></span>
<span class="normal"><a href="#__codelineno-4-52">52</a></span>
<span class="normal"><a href="#__codelineno-4-53">53</a></span>
<span class="normal"><a href="#__codelineno-4-54">54</a></span>
<span class="normal"><a href="#__codelineno-4-55">55</a></span>
<span class="normal"><a href="#__codelineno-4-56">56</a></span>
<span class="normal"><a href="#__codelineno-4-57">57</a></span>
<span class="normal"><a href="#__codelineno-4-58">58</a></span>
<span class="normal"><a href="#__codelineno-4-59">59</a></span>
<span class="normal"><a href="#__codelineno-4-60">60</a></span>
<span class="normal"><a href="#__codelineno-4-61">61</a></span>
<span class="normal"><a href="#__codelineno-4-62">62</a></span>
<span class="normal"><a href="#__codelineno-4-63">63</a></span>
<span class="normal"><a href="#__codelineno-4-64">64</a></span>
<span class="normal"><a href="#__codelineno-4-65">65</a></span>
<span class="normal"><a href="#__codelineno-4-66">66</a></span>
<span class="normal"><a href="#__codelineno-4-67">67</a></span>
<span class="normal"><a href="#__codelineno-4-68">68</a></span>
<span class="normal"><a href="#__codelineno-4-69">69</a></span>
<span class="normal"><a href="#__codelineno-4-70">70</a></span>
<span class="normal"><a href="#__codelineno-4-71">71</a></span>
<span class="normal"><a href="#__codelineno-4-72">72</a></span>
<span class="normal"><a href="#__codelineno-4-73">73</a></span>
<span class="normal"><a href="#__codelineno-4-74">74</a></span>
<span class="normal"><a href="#__codelineno-4-75">75</a></span>
<span class="normal"><a href="#__codelineno-4-76">76</a></span>
<span class="normal"><a href="#__codelineno-4-77">77</a></span>
<span class="normal"><a href="#__codelineno-4-78">78</a></span>
<span class="normal"><a href="#__codelineno-4-79">79</a></span>
<span class="normal"><a href="#__codelineno-4-80">80</a></span>
<span class="normal"><a href="#__codelineno-4-81">81</a></span>
<span class="normal"><a href="#__codelineno-4-82">82</a></span>
<span class="normal"><a href="#__codelineno-4-83">83</a></span>
<span class="normal"><a href="#__codelineno-4-84">84</a></span>
<span class="normal"><a href="#__codelineno-4-85">85</a></span>
<span class="normal"><a href="#__codelineno-4-86">86</a></span>
<span class="normal"><a href="#__codelineno-4-87">87</a></span>
<span class="normal"><a href="#__codelineno-4-88">88</a></span>
<span class="normal"><a href="#__codelineno-4-89">89</a></span>
<span class="normal"><a href="#__codelineno-4-90">90</a></span>
<span class="normal"><a href="#__codelineno-4-91">91</a></span>
<span class="normal"><a href="#__codelineno-4-92">92</a></span>
<span class="normal"><a href="#__codelineno-4-93">93</a></span>
<span class="normal"><a href="#__codelineno-4-94">94</a></span>
<span class="normal"><a href="#__codelineno-4-95">95</a></span>
<span class="normal"><a href="#__codelineno-4-96">96</a></span>
<span class="normal"><a href="#__codelineno-4-97">97</a></span>
<span class="normal"><a href="#__codelineno-4-98">98</a></span>
<span class="normal"><a href="#__codelineno-4-99">99</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">_single_tensor_adamax</span><span class="p">(</span>
<a id="__codelineno-4-2" name="__codelineno-4-2"></a>    <span class="n">params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-4-3" name="__codelineno-4-3"></a>    <span class="n">grads</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-4-4" name="__codelineno-4-4"></a>    <span class="n">exp_avgs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-4-5" name="__codelineno-4-5"></a>    <span class="n">exp_infs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-4-6" name="__codelineno-4-6"></a>    <span class="n">state_steps</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-4-7" name="__codelineno-4-7"></a>    <span class="o">*</span><span class="p">,</span>
<a id="__codelineno-4-8" name="__codelineno-4-8"></a>    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-4-9" name="__codelineno-4-9"></a>    <span class="n">beta1</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-4-10" name="__codelineno-4-10"></a>    <span class="n">beta2</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-4-11" name="__codelineno-4-11"></a>    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-4-12" name="__codelineno-4-12"></a>    <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-4-13" name="__codelineno-4-13"></a>    <span class="n">maximize</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-4-14" name="__codelineno-4-14"></a>    <span class="n">differentiable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-4-15" name="__codelineno-4-15"></a>    <span class="n">capturable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-4-16" name="__codelineno-4-16"></a>    <span class="n">has_complex</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-4-17" name="__codelineno-4-17"></a><span class="p">):</span>
<a id="__codelineno-4-18" name="__codelineno-4-18"></a>    <span class="c1"># 循环处理每个参数</span>
<a id="__codelineno-4-19" name="__codelineno-4-19"></a>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
<a id="__codelineno-4-20" name="__codelineno-4-20"></a>        <span class="n">grad</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-4-21" name="__codelineno-4-21"></a>        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">maximize</span> <span class="k">else</span> <span class="o">-</span><span class="n">grad</span>
<a id="__codelineno-4-22" name="__codelineno-4-22"></a>        <span class="n">exp_avg</span> <span class="o">=</span> <span class="n">exp_avgs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># 一阶矩 m_t</span>
<a id="__codelineno-4-23" name="__codelineno-4-23"></a>        <span class="n">exp_inf</span> <span class="o">=</span> <span class="n">exp_infs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># 无穷范数 u_t</span>
<a id="__codelineno-4-24" name="__codelineno-4-24"></a>        <span class="n">step_t</span> <span class="o">=</span> <span class="n">state_steps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-4-25" name="__codelineno-4-25"></a>
<a id="__codelineno-4-26" name="__codelineno-4-26"></a>        <span class="c1"># --- CUDA Graph 捕获检查 ---</span>
<a id="__codelineno-4-27" name="__codelineno-4-27"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">is_compiling</span><span class="p">()</span> <span class="ow">and</span> <span class="n">capturable</span><span class="p">:</span>
<a id="__codelineno-4-28" name="__codelineno-4-28"></a>            <span class="n">capturable_supported_devices</span> <span class="o">=</span> <span class="n">_get_capturable_supported_devices</span><span class="p">()</span>
<a id="__codelineno-4-29" name="__codelineno-4-29"></a>            <span class="k">assert</span> <span class="p">(</span>
<a id="__codelineno-4-30" name="__codelineno-4-30"></a>                <span class="n">param</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">step_t</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span>
<a id="__codelineno-4-31" name="__codelineno-4-31"></a>                <span class="ow">and</span> <span class="n">param</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="n">capturable_supported_devices</span>
<a id="__codelineno-4-32" name="__codelineno-4-32"></a>            <span class="p">),</span> <span class="sa">f</span><span class="s2">"If capturable=True, params and state_steps must be on supported devices: </span><span class="si">{</span><span class="n">capturable_supported_devices</span><span class="si">}</span><span class="s2">."</span>
<a id="__codelineno-4-33" name="__codelineno-4-33"></a>
<a id="__codelineno-4-34" name="__codelineno-4-34"></a>        <span class="c1"># 步数加 1</span>
<a id="__codelineno-4-35" name="__codelineno-4-35"></a>        <span class="n">step_t</span> <span class="o">+=</span> <span class="mi">1</span>
<a id="__codelineno-4-36" name="__codelineno-4-36"></a>
<a id="__codelineno-4-37" name="__codelineno-4-37"></a>        <span class="c1"># --- 应用权重衰减 ---</span>
<a id="__codelineno-4-38" name="__codelineno-4-38"></a>        <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-4-39" name="__codelineno-4-39"></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
<a id="__codelineno-4-40" name="__codelineno-4-40"></a>
<a id="__codelineno-4-41" name="__codelineno-4-41"></a>        <span class="c1"># --- 处理复数 ---</span>
<a id="__codelineno-4-42" name="__codelineno-4-42"></a>        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="n">param</span><span class="p">):</span>
<a id="__codelineno-4-43" name="__codelineno-4-43"></a>            <span class="n">param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<a id="__codelineno-4-44" name="__codelineno-4-44"></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
<a id="__codelineno-4-45" name="__codelineno-4-45"></a>            <span class="n">exp_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">)</span>
<a id="__codelineno-4-46" name="__codelineno-4-46"></a>            <span class="n">exp_inf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">exp_inf</span><span class="p">)</span>
<a id="__codelineno-4-47" name="__codelineno-4-47"></a>
<a id="__codelineno-4-48" name="__codelineno-4-48"></a>        <span class="c1"># --- Adamax 算法核心步骤 ---</span>
<a id="__codelineno-4-49" name="__codelineno-4-49"></a>
<a id="__codelineno-4-50" name="__codelineno-4-50"></a>        <span class="c1"># 1. 更新有偏一阶矩估计 m_t (和 Adam 一样)</span>
<a id="__codelineno-4-51" name="__codelineno-4-51"></a>        <span class="c1"># 公式: m_t = beta1 * m_{t-1} + (1 - beta1) * g_t</span>
<a id="__codelineno-4-52" name="__codelineno-4-52"></a>        <span class="n">exp_avg</span><span class="o">.</span><span class="n">lerp_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>
<a id="__codelineno-4-53" name="__codelineno-4-53"></a>
<a id="__codelineno-4-54" name="__codelineno-4-54"></a>        <span class="c1"># 2. 更新指数加权无穷范数 u_t</span>
<a id="__codelineno-4-55" name="__codelineno-4-55"></a>        <span class="c1"># 公式: u_t = max(beta2 * u_{t-1}, |g_t|)</span>
<a id="__codelineno-4-56" name="__codelineno-4-56"></a>        <span class="c1"># 注意：PyTorch 的实现中，为了防止 u_t 在梯度为零时也为零，</span>
<a id="__codelineno-4-57" name="__codelineno-4-57"></a>        <span class="c1"># 实际比较的是 `beta2 * u_{t-1}` 和 `|g_t| + eps`。</span>
<a id="__codelineno-4-58" name="__codelineno-4-58"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">differentiable</span><span class="p">:</span>
<a id="__codelineno-4-59" name="__codelineno-4-59"></a>            <span class="c1"># 对于非可微模式，使用 torch.maximum 更高效</span>
<a id="__codelineno-4-60" name="__codelineno-4-60"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
<a id="__codelineno-4-61" name="__codelineno-4-61"></a>                <span class="n">exp_inf</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">),</span>      <span class="c1"># 计算 beta2 * u_{t-1}</span>
<a id="__codelineno-4-62" name="__codelineno-4-62"></a>                <span class="n">grad</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">eps</span><span class="p">),</span>     <span class="c1"># 计算 |g_t| + eps</span>
<a id="__codelineno-4-63" name="__codelineno-4-63"></a>                <span class="n">out</span><span class="o">=</span><span class="n">exp_inf</span><span class="p">,</span>              <span class="c1"># 将结果原地写入 exp_inf</span>
<a id="__codelineno-4-64" name="__codelineno-4-64"></a>            <span class="p">)</span>
<a id="__codelineno-4-65" name="__codelineno-4-65"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-4-66" name="__codelineno-4-66"></a>            <span class="c1"># 对于可微模式，需要构建一个可微分的操作序列</span>
<a id="__codelineno-4-67" name="__codelineno-4-67"></a>            <span class="c1"># 将两个要比较的张量在新的维度上拼接起来</span>
<a id="__codelineno-4-68" name="__codelineno-4-68"></a>            <span class="n">norm_buf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
<a id="__codelineno-4-69" name="__codelineno-4-69"></a>                <span class="p">[</span><span class="n">exp_inf</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">grad</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze_</span><span class="p">(</span><span class="mi">0</span><span class="p">)],</span>
<a id="__codelineno-4-70" name="__codelineno-4-70"></a>                <span class="mi">0</span><span class="p">,</span>
<a id="__codelineno-4-71" name="__codelineno-4-71"></a>            <span class="p">)</span>
<a id="__codelineno-4-72" name="__codelineno-4-72"></a>            <span class="c1"># 然后使用 amax（等价于 max）在那个新维度上求最大值</span>
<a id="__codelineno-4-73" name="__codelineno-4-73"></a>            <span class="n">exp_inf</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">norm_buf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<a id="__codelineno-4-74" name="__codelineno-4-74"></a>
<a id="__codelineno-4-75" name="__codelineno-4-75"></a>        <span class="c1"># --- 步骤 3: 参数更新 ---</span>
<a id="__codelineno-4-76" name="__codelineno-4-76"></a>
<a id="__codelineno-4-77" name="__codelineno-4-77"></a>        <span class="c1"># 针对 Capturable 模式的特殊处理路径</span>
<a id="__codelineno-4-78" name="__codelineno-4-78"></a>        <span class="k">if</span> <span class="n">capturable</span><span class="p">:</span>
<a id="__codelineno-4-79" name="__codelineno-4-79"></a>            <span class="c1"># 这里的数学变换是为了在 capturable 模式下避免某些操作的限制。</span>
<a id="__codelineno-4-80" name="__codelineno-4-80"></a>            <span class="c1"># 原始公式是: clr = lr / (1 - beta1^t), 更新量是 -clr * (m_t / u_t)</span>
<a id="__codelineno-4-81" name="__codelineno-4-81"></a>            <span class="c1"># 这里计算 neg_bias_correction = beta1^t - 1</span>
<a id="__codelineno-4-82" name="__codelineno-4-82"></a>            <span class="n">neg_bias_correction</span> <span class="o">=</span> <span class="n">beta1</span><span class="o">**</span><span class="n">step_t</span> <span class="o">-</span> <span class="mi">1</span>
<a id="__codelineno-4-83" name="__codelineno-4-83"></a>            <span class="c1"># 然后除以 lr，得到 (beta1^t - 1) / lr</span>
<a id="__codelineno-4-84" name="__codelineno-4-84"></a>            <span class="n">neg_bias_correction</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
<a id="__codelineno-4-85" name="__codelineno-4-85"></a>            <span class="c1"># 分母 denom = u_t * (beta1^t - 1) / lr</span>
<a id="__codelineno-4-86" name="__codelineno-4-86"></a>            <span class="n">denom</span> <span class="o">=</span> <span class="n">exp_inf</span> <span class="o">*</span> <span class="n">neg_bias_correction</span>
<a id="__codelineno-4-87" name="__codelineno-4-87"></a>            <span class="c1"># 更新: param += m_t / denom = param - lr * m_t / ((1 - beta1^t) * u_t)</span>
<a id="__codelineno-4-88" name="__codelineno-4-88"></a>            <span class="n">param</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">,</span> <span class="n">denom</span><span class="p">)</span>
<a id="__codelineno-4-89" name="__codelineno-4-89"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-4-90" name="__codelineno-4-90"></a>            <span class="c1"># 常规模式下的更新路径</span>
<a id="__codelineno-4-91" name="__codelineno-4-91"></a>            <span class="c1"># 计算偏差修正项</span>
<a id="__codelineno-4-92" name="__codelineno-4-92"></a>            <span class="n">bias_correction</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">_get_value</span><span class="p">(</span><span class="n">step_t</span><span class="p">)</span>
<a id="__codelineno-4-93" name="__codelineno-4-93"></a>            <span class="c1"># 计算修正后的学习率</span>
<a id="__codelineno-4-94" name="__codelineno-4-94"></a>            <span class="n">clr</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">/</span> <span class="n">bias_correction</span>
<a id="__codelineno-4-95" name="__codelineno-4-95"></a>
<a id="__codelineno-4-96" name="__codelineno-4-96"></a>            <span class="c1"># 执行参数更新</span>
<a id="__codelineno-4-97" name="__codelineno-4-97"></a>            <span class="c1"># 公式: θ_t = θ_{t-1} - (lr / (1 - beta1^t)) * (m_t / u_t)</span>
<a id="__codelineno-4-98" name="__codelineno-4-98"></a>            <span class="c1"># exp_inf 就是分母 u_t</span>
<a id="__codelineno-4-99" name="__codelineno-4-99"></a>            <span class="n">param</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_inf</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="n">clr</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<h3 id="nadam">Nadam<a class="headerlink" href="#nadam" title="Permanent link">¶</a></h3>
<p>读到这里，我相信任何一位读者都可以独立发明出 Nadam，毕竟我们在讲 SGDM 的时候花了大力气推导了 Nesterov 加速的式子，总不可能到了 Adam 这一块就完全不管了吧。是的，Nadam 的 novelty 就在于把 Nesterov 加速梯度引入到了 Adam 的计算之中。
我们考虑直接引入 Nesterov 加速项，也就是权重更新项从 <span class="arithmatex">\(\dfrac{\eta}{\sqrt{\epsilon+\hat G_n}}\hat M_n\)</span> 换成 <span class="arithmatex">\(\dfrac{\eta}{\sqrt{\epsilon+\hat G_n}}[\dfrac{\beta_1 M_n}{1-\beta_1^{n-1}}+\dfrac{(1-\beta_1) g_n}{1-\beta_1^{n-1}}]\)</span>。这个也是 arXiv/1609.04747 推导出来的的。</p>
<p>但是考虑到 Adam 增加了对梯度二阶矩的估计，因此如果一直使用固定的 <span class="arithmatex">\(\beta_1\)</span> 的话，其实是偏大的。如果我们看提出 Nadam 的原论文 <a href="https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ">Incorporating Nesterov Momentum into Adam</a>，就可以发现它的思路有一定的差异。</p>
<p>在后面一篇论文中，作者并没有固定规定一个 <span class="arithmatex">\(\beta_1\)</span>，而是使用 <span class="arithmatex">\(\mu_{n}\)</span> 来调整更新比例，也就是它认为动量和权重的更新应该是如下的：</p>
<div class="arithmatex">\[
\begin{align*}
    M_n&amp;=\mu_{n}M_{n-1}+\beta_3 g_n\\
    \theta_n&amp;=\theta_{n-1}-(\mu_{n+1}M_n+\beta_3 g_n)
\end{align*}
\]</div>
<p>这里 <span class="arithmatex">\(\mu_{n+1}\)</span> 代表的就是 Nesterov 加速的前瞻性。下面我们取 <span class="arithmatex">\(\beta_3=(1-\mu_n)\)</span>，由于是对 <span class="arithmatex">\(\mu_n\)</span> 进行连乘，权重更新项也就变成了 <span class="arithmatex">\(\dfrac{\eta}{\sqrt{\epsilon+\hat G_n}}[\dfrac{\mu_{n+1} M_n}{1-\prod_{i=1}^{n+1}\mu_i}+\dfrac{(1-\mu_{n}) g_n}{1-\prod_{i=1}^{n}\mu_i}]\)</span></p>
<p>为了解决之前偏大的问题，PyTorch 在 Nadam 的实现里对 <span class="arithmatex">\(\beta_1\)</span> 采用了衰减的策略。</p>
<p>具体而言，它引入了 <span class="arithmatex">\(\mu_n=\beta_1 \left(1 - 0.5 \cdot 0.96^{n \cdot d}\right)\)</span> 的估计，那么最后权重的更新方式变成：</p>
<div class="arithmatex">\[
\begin{align*}
    g_n&amp;=\nabla\mathcal{L({x};\theta_{n-1})}\\
    M_n&amp;=(1-\beta_1)g_n+\beta_1M_{n-1}\\
    G_{n}&amp;=\beta_2 G_n + (1-\beta_2)g_n\odot g_n\\
    \mu_n&amp;=\beta_1 \left(1 - 0.5 \cdot 0.96^{n \cdot d}\right)\\
    \mu_{n+1}&amp;=\beta_1 \left(1 - 0.5 \cdot 0.96^{(n+1) \cdot d}\right)\\
    \hat \mu_{n+1} &amp;=\hat\mu_n \mu_{n+1}\\
    \hat M_n&amp;=\dfrac{\mu_{n+1}M_n}{1-\hat \mu_{n+1}}\\
    \hat G_n&amp;=\dfrac{G_n}{1-\beta_2^{n-1}}\\
    \theta_n&amp;=\theta_{n-1}-\dfrac{\eta}{\sqrt{\epsilon+\hat G_n}} (\hat M_n+\dfrac{(1-\mu_n)g_n}{1-\hat\mu_n})
\end{align*}
\]</div>
<p>这是优化器的轨迹动图：</p>
<p><img alt="rastrigin_NAdam" src="../optimizer_pics/rastrigin_NAdam.gif"/></p>
<p><img alt="rosenbrock_NAdam" src="../optimizer_pics/rosenbrock_NAdam.gif"/></p>
<p>看来 Nadam 和 Adam 差不太多，并没有像 SGD 引入 NAG 那样惊艳。还是看看它在真实任务上面的表现吧：</p>
<p><img alt="NAdam_performance_curves" src="../optimizer_pics/NAdam_performance_curves.png"/></p>
<p><img alt="NAdam_landscape_pca" src="../optimizer_pics/NAdam_landscape_pca.png"/></p>
<p>事实上和 Adam 也没有特别大的区别：在约 2900 个 Batch 后 train_loss 降到了 0.1 附近；和 Adam 一样在约 900 个 Batch 后 acc 升到了 0.9。</p>
<p>下面是代码：</p>
<details>
<summary> Nadam 的实现</summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-5-1">  1</a></span>
<span class="normal"><a href="#__codelineno-5-2">  2</a></span>
<span class="normal"><a href="#__codelineno-5-3">  3</a></span>
<span class="normal"><a href="#__codelineno-5-4">  4</a></span>
<span class="normal"><a href="#__codelineno-5-5">  5</a></span>
<span class="normal"><a href="#__codelineno-5-6">  6</a></span>
<span class="normal"><a href="#__codelineno-5-7">  7</a></span>
<span class="normal"><a href="#__codelineno-5-8">  8</a></span>
<span class="normal"><a href="#__codelineno-5-9">  9</a></span>
<span class="normal"><a href="#__codelineno-5-10"> 10</a></span>
<span class="normal"><a href="#__codelineno-5-11"> 11</a></span>
<span class="normal"><a href="#__codelineno-5-12"> 12</a></span>
<span class="normal"><a href="#__codelineno-5-13"> 13</a></span>
<span class="normal"><a href="#__codelineno-5-14"> 14</a></span>
<span class="normal"><a href="#__codelineno-5-15"> 15</a></span>
<span class="normal"><a href="#__codelineno-5-16"> 16</a></span>
<span class="normal"><a href="#__codelineno-5-17"> 17</a></span>
<span class="normal"><a href="#__codelineno-5-18"> 18</a></span>
<span class="normal"><a href="#__codelineno-5-19"> 19</a></span>
<span class="normal"><a href="#__codelineno-5-20"> 20</a></span>
<span class="normal"><a href="#__codelineno-5-21"> 21</a></span>
<span class="normal"><a href="#__codelineno-5-22"> 22</a></span>
<span class="normal"><a href="#__codelineno-5-23"> 23</a></span>
<span class="normal"><a href="#__codelineno-5-24"> 24</a></span>
<span class="normal"><a href="#__codelineno-5-25"> 25</a></span>
<span class="normal"><a href="#__codelineno-5-26"> 26</a></span>
<span class="normal"><a href="#__codelineno-5-27"> 27</a></span>
<span class="normal"><a href="#__codelineno-5-28"> 28</a></span>
<span class="normal"><a href="#__codelineno-5-29"> 29</a></span>
<span class="normal"><a href="#__codelineno-5-30"> 30</a></span>
<span class="normal"><a href="#__codelineno-5-31"> 31</a></span>
<span class="normal"><a href="#__codelineno-5-32"> 32</a></span>
<span class="normal"><a href="#__codelineno-5-33"> 33</a></span>
<span class="normal"><a href="#__codelineno-5-34"> 34</a></span>
<span class="normal"><a href="#__codelineno-5-35"> 35</a></span>
<span class="normal"><a href="#__codelineno-5-36"> 36</a></span>
<span class="normal"><a href="#__codelineno-5-37"> 37</a></span>
<span class="normal"><a href="#__codelineno-5-38"> 38</a></span>
<span class="normal"><a href="#__codelineno-5-39"> 39</a></span>
<span class="normal"><a href="#__codelineno-5-40"> 40</a></span>
<span class="normal"><a href="#__codelineno-5-41"> 41</a></span>
<span class="normal"><a href="#__codelineno-5-42"> 42</a></span>
<span class="normal"><a href="#__codelineno-5-43"> 43</a></span>
<span class="normal"><a href="#__codelineno-5-44"> 44</a></span>
<span class="normal"><a href="#__codelineno-5-45"> 45</a></span>
<span class="normal"><a href="#__codelineno-5-46"> 46</a></span>
<span class="normal"><a href="#__codelineno-5-47"> 47</a></span>
<span class="normal"><a href="#__codelineno-5-48"> 48</a></span>
<span class="normal"><a href="#__codelineno-5-49"> 49</a></span>
<span class="normal"><a href="#__codelineno-5-50"> 50</a></span>
<span class="normal"><a href="#__codelineno-5-51"> 51</a></span>
<span class="normal"><a href="#__codelineno-5-52"> 52</a></span>
<span class="normal"><a href="#__codelineno-5-53"> 53</a></span>
<span class="normal"><a href="#__codelineno-5-54"> 54</a></span>
<span class="normal"><a href="#__codelineno-5-55"> 55</a></span>
<span class="normal"><a href="#__codelineno-5-56"> 56</a></span>
<span class="normal"><a href="#__codelineno-5-57"> 57</a></span>
<span class="normal"><a href="#__codelineno-5-58"> 58</a></span>
<span class="normal"><a href="#__codelineno-5-59"> 59</a></span>
<span class="normal"><a href="#__codelineno-5-60"> 60</a></span>
<span class="normal"><a href="#__codelineno-5-61"> 61</a></span>
<span class="normal"><a href="#__codelineno-5-62"> 62</a></span>
<span class="normal"><a href="#__codelineno-5-63"> 63</a></span>
<span class="normal"><a href="#__codelineno-5-64"> 64</a></span>
<span class="normal"><a href="#__codelineno-5-65"> 65</a></span>
<span class="normal"><a href="#__codelineno-5-66"> 66</a></span>
<span class="normal"><a href="#__codelineno-5-67"> 67</a></span>
<span class="normal"><a href="#__codelineno-5-68"> 68</a></span>
<span class="normal"><a href="#__codelineno-5-69"> 69</a></span>
<span class="normal"><a href="#__codelineno-5-70"> 70</a></span>
<span class="normal"><a href="#__codelineno-5-71"> 71</a></span>
<span class="normal"><a href="#__codelineno-5-72"> 72</a></span>
<span class="normal"><a href="#__codelineno-5-73"> 73</a></span>
<span class="normal"><a href="#__codelineno-5-74"> 74</a></span>
<span class="normal"><a href="#__codelineno-5-75"> 75</a></span>
<span class="normal"><a href="#__codelineno-5-76"> 76</a></span>
<span class="normal"><a href="#__codelineno-5-77"> 77</a></span>
<span class="normal"><a href="#__codelineno-5-78"> 78</a></span>
<span class="normal"><a href="#__codelineno-5-79"> 79</a></span>
<span class="normal"><a href="#__codelineno-5-80"> 80</a></span>
<span class="normal"><a href="#__codelineno-5-81"> 81</a></span>
<span class="normal"><a href="#__codelineno-5-82"> 82</a></span>
<span class="normal"><a href="#__codelineno-5-83"> 83</a></span>
<span class="normal"><a href="#__codelineno-5-84"> 84</a></span>
<span class="normal"><a href="#__codelineno-5-85"> 85</a></span>
<span class="normal"><a href="#__codelineno-5-86"> 86</a></span>
<span class="normal"><a href="#__codelineno-5-87"> 87</a></span>
<span class="normal"><a href="#__codelineno-5-88"> 88</a></span>
<span class="normal"><a href="#__codelineno-5-89"> 89</a></span>
<span class="normal"><a href="#__codelineno-5-90"> 90</a></span>
<span class="normal"><a href="#__codelineno-5-91"> 91</a></span>
<span class="normal"><a href="#__codelineno-5-92"> 92</a></span>
<span class="normal"><a href="#__codelineno-5-93"> 93</a></span>
<span class="normal"><a href="#__codelineno-5-94"> 94</a></span>
<span class="normal"><a href="#__codelineno-5-95"> 95</a></span>
<span class="normal"><a href="#__codelineno-5-96"> 96</a></span>
<span class="normal"><a href="#__codelineno-5-97"> 97</a></span>
<span class="normal"><a href="#__codelineno-5-98"> 98</a></span>
<span class="normal"><a href="#__codelineno-5-99"> 99</a></span>
<span class="normal"><a href="#__codelineno-5-100">100</a></span>
<span class="normal"><a href="#__codelineno-5-101">101</a></span>
<span class="normal"><a href="#__codelineno-5-102">102</a></span>
<span class="normal"><a href="#__codelineno-5-103">103</a></span>
<span class="normal"><a href="#__codelineno-5-104">104</a></span>
<span class="normal"><a href="#__codelineno-5-105">105</a></span>
<span class="normal"><a href="#__codelineno-5-106">106</a></span>
<span class="normal"><a href="#__codelineno-5-107">107</a></span>
<span class="normal"><a href="#__codelineno-5-108">108</a></span>
<span class="normal"><a href="#__codelineno-5-109">109</a></span>
<span class="normal"><a href="#__codelineno-5-110">110</a></span>
<span class="normal"><a href="#__codelineno-5-111">111</a></span>
<span class="normal"><a href="#__codelineno-5-112">112</a></span>
<span class="normal"><a href="#__codelineno-5-113">113</a></span>
<span class="normal"><a href="#__codelineno-5-114">114</a></span>
<span class="normal"><a href="#__codelineno-5-115">115</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">_single_tensor_nadam</span><span class="p">(</span>
<a id="__codelineno-5-2" name="__codelineno-5-2"></a>    <span class="n">params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-5-3" name="__codelineno-5-3"></a>    <span class="n">grads</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-5-4" name="__codelineno-5-4"></a>    <span class="n">exp_avgs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-5-5" name="__codelineno-5-5"></a>    <span class="n">exp_avg_sqs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-5-6" name="__codelineno-5-6"></a>    <span class="n">mu_products</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-5-7" name="__codelineno-5-7"></a>    <span class="n">state_steps</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-5-8" name="__codelineno-5-8"></a>    <span class="o">*</span><span class="p">,</span>
<a id="__codelineno-5-9" name="__codelineno-5-9"></a>    <span class="n">beta1</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-5-10" name="__codelineno-5-10"></a>    <span class="n">beta2</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-5-11" name="__codelineno-5-11"></a>    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-5-12" name="__codelineno-5-12"></a>    <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-5-13" name="__codelineno-5-13"></a>    <span class="n">momentum_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-5-14" name="__codelineno-5-14"></a>    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-5-15" name="__codelineno-5-15"></a>    <span class="n">decoupled_weight_decay</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-5-16" name="__codelineno-5-16"></a>    <span class="n">maximize</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-5-17" name="__codelineno-5-17"></a>    <span class="n">capturable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-5-18" name="__codelineno-5-18"></a>    <span class="n">differentiable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-5-19" name="__codelineno-5-19"></a>    <span class="n">has_complex</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-5-20" name="__codelineno-5-20"></a><span class="p">):</span>
<a id="__codelineno-5-21" name="__codelineno-5-21"></a>    <span class="c1"># 循环处理每个参数</span>
<a id="__codelineno-5-22" name="__codelineno-5-22"></a>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
<a id="__codelineno-5-23" name="__codelineno-5-23"></a>        <span class="n">grad</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">maximize</span> <span class="k">else</span> <span class="o">-</span><span class="n">grads</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-5-24" name="__codelineno-5-24"></a>        <span class="n">exp_avg</span> <span class="o">=</span> <span class="n">exp_avgs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-5-25" name="__codelineno-5-25"></a>        <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">exp_avg_sqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-5-26" name="__codelineno-5-26"></a>        <span class="n">mu_product</span> <span class="o">=</span> <span class="n">mu_products</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-5-27" name="__codelineno-5-27"></a>        <span class="n">step_t</span> <span class="o">=</span> <span class="n">state_steps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-5-28" name="__codelineno-5-28"></a>
<a id="__codelineno-5-29" name="__codelineno-5-29"></a>        <span class="c1"># --- 处理复数 ---</span>
<a id="__codelineno-5-30" name="__codelineno-5-30"></a>        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="n">param</span><span class="p">):</span>
<a id="__codelineno-5-31" name="__codelineno-5-31"></a>            <span class="n">param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<a id="__codelineno-5-32" name="__codelineno-5-32"></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
<a id="__codelineno-5-33" name="__codelineno-5-33"></a>            <span class="n">exp_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">)</span>
<a id="__codelineno-5-34" name="__codelineno-5-34"></a>            <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">exp_avg_sq</span><span class="p">)</span>
<a id="__codelineno-5-35" name="__codelineno-5-35"></a>
<a id="__codelineno-5-36" name="__codelineno-5-36"></a>        <span class="c1"># --- CUDA Graph 捕获检查 ---</span>
<a id="__codelineno-5-37" name="__codelineno-5-37"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">is_compiling</span><span class="p">()</span> <span class="ow">and</span> <span class="n">capturable</span><span class="p">:</span>
<a id="__codelineno-5-38" name="__codelineno-5-38"></a>            <span class="n">capturable_supported_devices</span> <span class="o">=</span> <span class="n">_get_capturable_supported_devices</span><span class="p">()</span>
<a id="__codelineno-5-39" name="__codelineno-5-39"></a>            <span class="k">assert</span> <span class="p">(</span>
<a id="__codelineno-5-40" name="__codelineno-5-40"></a>                <span class="n">param</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">mu_product</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">step_t</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span>
<a id="__codelineno-5-41" name="__codelineno-5-41"></a>                <span class="ow">and</span> <span class="n">param</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="n">capturable_supported_devices</span>
<a id="__codelineno-5-42" name="__codelineno-5-42"></a>            <span class="p">),</span> <span class="s2">"如果 capturable=True, params, mu_products 和 state_steps 必须在支持的设备上。"</span>
<a id="__codelineno-5-43" name="__codelineno-5-43"></a>
<a id="__codelineno-5-44" name="__codelineno-5-44"></a>        <span class="c1"># 步数加 1</span>
<a id="__codelineno-5-45" name="__codelineno-5-45"></a>        <span class="n">step_t</span> <span class="o">+=</span> <span class="mi">1</span>
<a id="__codelineno-5-46" name="__codelineno-5-46"></a>
<a id="__codelineno-5-47" name="__codelineno-5-47"></a>        <span class="c1"># 根据模式获取步数值（Tensor 或 float）</span>
<a id="__codelineno-5-48" name="__codelineno-5-48"></a>        <span class="k">if</span> <span class="n">capturable</span><span class="p">:</span>
<a id="__codelineno-5-49" name="__codelineno-5-49"></a>            <span class="n">step</span> <span class="o">=</span> <span class="n">step_t</span>
<a id="__codelineno-5-50" name="__codelineno-5-50"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-5-51" name="__codelineno-5-51"></a>            <span class="n">step</span> <span class="o">=</span> <span class="n">_get_value</span><span class="p">(</span><span class="n">step_t</span><span class="p">)</span>
<a id="__codelineno-5-52" name="__codelineno-5-52"></a>
<a id="__codelineno-5-53" name="__codelineno-5-53"></a>        <span class="c1"># 计算二阶矩的偏差修正项</span>
<a id="__codelineno-5-54" name="__codelineno-5-54"></a>        <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="o">**</span><span class="n">step</span>
<a id="__codelineno-5-55" name="__codelineno-5-55"></a>
<a id="__codelineno-5-56" name="__codelineno-5-56"></a>        <span class="c1"># --- 应用权重衰减 ---</span>
<a id="__codelineno-5-57" name="__codelineno-5-57"></a>        <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-5-58" name="__codelineno-5-58"></a>            <span class="k">if</span> <span class="n">decoupled_weight_decay</span><span class="p">:</span>
<a id="__codelineno-5-59" name="__codelineno-5-59"></a>                <span class="c1"># NAdamW: 使用解耦权重衰减</span>
<a id="__codelineno-5-60" name="__codelineno-5-60"></a>                <span class="n">param</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">weight_decay</span><span class="p">)</span>
<a id="__codelineno-5-61" name="__codelineno-5-61"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-5-62" name="__codelineno-5-62"></a>                <span class="c1"># 标准 NAdam: 将权重衰减作为 L2 正则化加入梯度</span>
<a id="__codelineno-5-63" name="__codelineno-5-63"></a>                <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
<a id="__codelineno-5-64" name="__codelineno-5-64"></a>
<a id="__codelineno-5-65" name="__codelineno-5-65"></a>        <span class="c1"># --- NAdam 核心步骤 ---</span>
<a id="__codelineno-5-66" name="__codelineno-5-66"></a>
<a id="__codelineno-5-67" name="__codelineno-5-67"></a>        <span class="c1"># 1. 计算当前步(t)和下一步(t+1)的动量衰减调度因子 μ</span>
<a id="__codelineno-5-68" name="__codelineno-5-68"></a>        <span class="c1"># 这个调度使得动量衰减率在训练初期较小，后期接近 beta1</span>
<a id="__codelineno-5-69" name="__codelineno-5-69"></a>        <span class="n">mu</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.96</span> <span class="o">**</span> <span class="p">(</span><span class="n">step</span> <span class="o">*</span> <span class="n">momentum_decay</span><span class="p">)))</span>
<a id="__codelineno-5-70" name="__codelineno-5-70"></a>        <span class="n">mu_next</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.96</span> <span class="o">**</span> <span class="p">((</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">momentum_decay</span><span class="p">)))</span>
<a id="__codelineno-5-71" name="__codelineno-5-71"></a>
<a id="__codelineno-5-72" name="__codelineno-5-72"></a>        <span class="c1"># 2. 更新动量衰减因子的累积乘积</span>
<a id="__codelineno-5-73" name="__codelineno-5-73"></a>        <span class="c1"># 公式: mu_product_t = mu_product_{t-1} * mu_t</span>
<a id="__codelineno-5-74" name="__codelineno-5-74"></a>        <span class="n">mu_product</span> <span class="o">*=</span> <span class="n">mu</span>
<a id="__codelineno-5-75" name="__codelineno-5-75"></a>
<a id="__codelineno-5-76" name="__codelineno-5-76"></a>        <span class="c1"># 3. 更新一阶矩 m_t 和二阶矩 v_t (和 Adam 相同)</span>
<a id="__codelineno-5-77" name="__codelineno-5-77"></a>        <span class="c1"># m_t = beta1 * m_{t-1} + (1 - beta1) * g_t</span>
<a id="__codelineno-5-78" name="__codelineno-5-78"></a>        <span class="n">exp_avg</span><span class="o">.</span><span class="n">lerp_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>
<a id="__codelineno-5-79" name="__codelineno-5-79"></a>        <span class="c1"># v_t = beta2 * v_{t-1} + (1 - beta2) * g_t^2</span>
<a id="__codelineno-5-80" name="__codelineno-5-80"></a>        <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>
<a id="__codelineno-5-81" name="__codelineno-5-81"></a>
<a id="__codelineno-5-82" name="__codelineno-5-82"></a>        <span class="c1"># 4. 计算归一化的分母</span>
<a id="__codelineno-5-83" name="__codelineno-5-83"></a>        <span class="c1"># denom = sqrt(v_t / bias_correction2)</span>
<a id="__codelineno-5-84" name="__codelineno-5-84"></a>        <span class="n">denom</span> <span class="o">=</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
<a id="__codelineno-5-85" name="__codelineno-5-85"></a>
<a id="__codelineno-5-86" name="__codelineno-5-86"></a>        <span class="c1"># --- 步骤 5: 参数更新 ---</span>
<a id="__codelineno-5-87" name="__codelineno-5-87"></a>        <span class="c1"># NAdam 的更新规则可以分解为两部分：一部分与当前梯度有关，一部分与动量有关。</span>
<a id="__codelineno-5-88" name="__codelineno-5-88"></a>        <span class="c1"># 更新公式: param_t = param_{t-1} - lr * ( (1-μ_t)*g_hat_t + μ_{t+1}*m_hat_t ) / (sqrt(v_hat_t) + eps)</span>
<a id="__codelineno-5-89" name="__codelineno-5-89"></a>        <span class="c1"># 其中 g_hat_t 和 m_hat_t 是经过偏差修正的梯度和动量。</span>
<a id="__codelineno-5-90" name="__codelineno-5-90"></a>        <span class="c1"># PyTorch 的实现将这个公式拆分成了两个 addcdiv 操作。</span>
<a id="__codelineno-5-91" name="__codelineno-5-91"></a>
<a id="__codelineno-5-92" name="__codelineno-5-92"></a>        <span class="c1"># 可微分或可捕获模式下的路径</span>
<a id="__codelineno-5-93" name="__codelineno-5-93"></a>        <span class="k">if</span> <span class="n">differentiable</span> <span class="ow">or</span> <span class="n">capturable</span><span class="p">:</span>
<a id="__codelineno-5-94" name="__codelineno-5-94"></a>            <span class="n">denom</span> <span class="o">=</span> <span class="n">denom</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>
<a id="__codelineno-5-95" name="__codelineno-5-95"></a>            <span class="c1"># 为了让 Autograd 跟踪操作，直接修改梯度和动量项，而不是作为 addcdiv 的标量值</span>
<a id="__codelineno-5-96" name="__codelineno-5-96"></a>            <span class="n">mu_product_next</span> <span class="o">=</span> <span class="n">mu_product</span> <span class="o">*</span> <span class="n">mu_next</span>
<a id="__codelineno-5-97" name="__codelineno-5-97"></a>            <span class="c1"># 计算与梯度相关的更新部分</span>
<a id="__codelineno-5-98" name="__codelineno-5-98"></a>            <span class="n">grad_update_part</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">mu_product</span><span class="p">))</span>
<a id="__codelineno-5-99" name="__codelineno-5-99"></a>            <span class="c1"># 计算与动量相关的更新部分</span>
<a id="__codelineno-5-100" name="__codelineno-5-100"></a>            <span class="n">exp_avg_update_part</span> <span class="o">=</span> <span class="n">exp_avg</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">mu_next</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">mu_product_next</span><span class="p">))</span>
<a id="__codelineno-5-101" name="__codelineno-5-101"></a>            <span class="c1"># 应用更新</span>
<a id="__codelineno-5-102" name="__codelineno-5-102"></a>            <span class="n">param</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="n">grad_update_part</span><span class="p">,</span> <span class="n">denom</span><span class="p">)</span>
<a id="__codelineno-5-103" name="__codelineno-5-103"></a>            <span class="n">param</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="n">exp_avg_update_part</span><span class="p">,</span> <span class="n">denom</span><span class="p">)</span>
<a id="__codelineno-5-104" name="__codelineno-5-104"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-5-105" name="__codelineno-5-105"></a>            <span class="c1"># 常规模式下的路径 (更高效)</span>
<a id="__codelineno-5-106" name="__codelineno-5-106"></a>            <span class="n">mu_product_next</span> <span class="o">=</span> <span class="n">_get_value</span><span class="p">(</span><span class="n">mu_product</span><span class="p">)</span> <span class="o">*</span> <span class="n">mu_next</span>
<a id="__codelineno-5-107" name="__codelineno-5-107"></a>            <span class="n">denom</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>
<a id="__codelineno-5-108" name="__codelineno-5-108"></a>            <span class="c1"># 应用与梯度相关的更新部分</span>
<a id="__codelineno-5-109" name="__codelineno-5-109"></a>            <span class="n">param</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span>
<a id="__codelineno-5-110" name="__codelineno-5-110"></a>                <span class="n">grad</span><span class="p">,</span> <span class="n">denom</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">_get_value</span><span class="p">(</span><span class="n">mu_product</span><span class="p">)))</span>
<a id="__codelineno-5-111" name="__codelineno-5-111"></a>            <span class="p">)</span>
<a id="__codelineno-5-112" name="__codelineno-5-112"></a>            <span class="c1"># 应用与动量相关的更新部分</span>
<a id="__codelineno-5-113" name="__codelineno-5-113"></a>            <span class="n">param</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span>
<a id="__codelineno-5-114" name="__codelineno-5-114"></a>                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">denom</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">mu_next</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">mu_product_next</span><span class="p">)</span>
<a id="__codelineno-5-115" name="__codelineno-5-115"></a>            <span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<h3 id="lamb">LAMB<a class="headerlink" href="#lamb" title="Permanent link">¶</a></h3>
<p>Researchers 的奇怪的命名品味啊……后面我们还能看到 Lion 优化器，不知道是不是专门吃 LAMB 的……无论如何让我们来看看吧。</p>
<p>LAMB 在 Adam 上面的改进点在于对“何时应该多更新参数”的一个先验估计：如果本来参数大，并且算出来的更新量小，那就意味着本来应该优化的参数没有得到有效优化，也就是，如果反过来我们取 <span class="arithmatex">\(r=\dfrac{|\theta_{n-1}|}{|\Delta\theta|}\)</span> （被称作信任比率）再乘以原来的参数更新量，就可以实现有效优化。具体而言，LAMB 优化器的更新公式是：</p>
<div class="arithmatex">\[
\begin{align*}
    g_n&amp;=\nabla\mathcal{L({x};\theta_{n-1})}\\
    M_n&amp;=(1-\beta_1)g_n+\beta_1M_{n-1}\\
    G_{n}&amp;=\beta_2 G_n + (1-\beta_2)g_n\odot g_n\\
    \hat M_n&amp;=\dfrac{M_n}{1-\beta_1^n}\\
    \hat G_n&amp;=\dfrac{G_n}{1-\beta_2^n}\\
    \tilde M_n&amp;=\dfrac{\eta}{\sqrt{\epsilon+\hat G_n}} \hat M_n-\eta\lambda\theta_{n-1}\\
    \theta_n&amp;=\theta_{n-1}-\dfrac{\Phi(\theta_{n-1})}{|\tilde M_n|}\tilde{M}_n
\end{align*}
\]</div>
<p>这里的 <span class="arithmatex">\(\Phi(x)\)</span> 可以取 <span class="arithmatex">\(x\)</span> 也可以做裁剪取 <span class="arithmatex">\(\max{(\min{(x,\gamma_a)},\gamma_b)}\)</span> 来把参数控制在这样一个范围内。</p>
<p>LAMB 的初衷就是解决 Adam 在大批量训练的时候梯度方差过小（就是前面提到的那种情况）导致训不动或者训炸的问题。这样做看来确实比较有用，让我们来看看表现吧：</p>
<p><img alt="rastrigin_Lamb" src="../optimizer_pics/rastrigin_Lamb.gif"/></p>
<p><img alt="rosenbrock_Lamb" src="../optimizer_pics/rosenbrock_Lamb.gif"/></p>
<p>这里 rosenbrock 疑似参数有些小了。下面是实际任务的测试：</p>
<p><img alt="Lamb_performance_curves" src="../optimizer_pics/Lamb_performance_curves.png"/></p>
<p><img alt="Lamb_landscape_pca" src="../optimizer_pics/Lamb_landscape_pca.png"/></p>
<p>可以看到 LAMB 取得了和 Adam 差不多的水平。在约 4000 个 Batch 后 train_loss 降到了 0.1 附近；约 1000 个 Batch 后 acc 稳定在 0.9 以上。</p>
<p>下面是 <code>torch_optimizer</code> 库对 LAMB 的实现：</p>
<details>
<summary> LAMB 优化器的实现 </summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-6-1">  1</a></span>
<span class="normal"><a href="#__codelineno-6-2">  2</a></span>
<span class="normal"><a href="#__codelineno-6-3">  3</a></span>
<span class="normal"><a href="#__codelineno-6-4">  4</a></span>
<span class="normal"><a href="#__codelineno-6-5">  5</a></span>
<span class="normal"><a href="#__codelineno-6-6">  6</a></span>
<span class="normal"><a href="#__codelineno-6-7">  7</a></span>
<span class="normal"><a href="#__codelineno-6-8">  8</a></span>
<span class="normal"><a href="#__codelineno-6-9">  9</a></span>
<span class="normal"><a href="#__codelineno-6-10"> 10</a></span>
<span class="normal"><a href="#__codelineno-6-11"> 11</a></span>
<span class="normal"><a href="#__codelineno-6-12"> 12</a></span>
<span class="normal"><a href="#__codelineno-6-13"> 13</a></span>
<span class="normal"><a href="#__codelineno-6-14"> 14</a></span>
<span class="normal"><a href="#__codelineno-6-15"> 15</a></span>
<span class="normal"><a href="#__codelineno-6-16"> 16</a></span>
<span class="normal"><a href="#__codelineno-6-17"> 17</a></span>
<span class="normal"><a href="#__codelineno-6-18"> 18</a></span>
<span class="normal"><a href="#__codelineno-6-19"> 19</a></span>
<span class="normal"><a href="#__codelineno-6-20"> 20</a></span>
<span class="normal"><a href="#__codelineno-6-21"> 21</a></span>
<span class="normal"><a href="#__codelineno-6-22"> 22</a></span>
<span class="normal"><a href="#__codelineno-6-23"> 23</a></span>
<span class="normal"><a href="#__codelineno-6-24"> 24</a></span>
<span class="normal"><a href="#__codelineno-6-25"> 25</a></span>
<span class="normal"><a href="#__codelineno-6-26"> 26</a></span>
<span class="normal"><a href="#__codelineno-6-27"> 27</a></span>
<span class="normal"><a href="#__codelineno-6-28"> 28</a></span>
<span class="normal"><a href="#__codelineno-6-29"> 29</a></span>
<span class="normal"><a href="#__codelineno-6-30"> 30</a></span>
<span class="normal"><a href="#__codelineno-6-31"> 31</a></span>
<span class="normal"><a href="#__codelineno-6-32"> 32</a></span>
<span class="normal"><a href="#__codelineno-6-33"> 33</a></span>
<span class="normal"><a href="#__codelineno-6-34"> 34</a></span>
<span class="normal"><a href="#__codelineno-6-35"> 35</a></span>
<span class="normal"><a href="#__codelineno-6-36"> 36</a></span>
<span class="normal"><a href="#__codelineno-6-37"> 37</a></span>
<span class="normal"><a href="#__codelineno-6-38"> 38</a></span>
<span class="normal"><a href="#__codelineno-6-39"> 39</a></span>
<span class="normal"><a href="#__codelineno-6-40"> 40</a></span>
<span class="normal"><a href="#__codelineno-6-41"> 41</a></span>
<span class="normal"><a href="#__codelineno-6-42"> 42</a></span>
<span class="normal"><a href="#__codelineno-6-43"> 43</a></span>
<span class="normal"><a href="#__codelineno-6-44"> 44</a></span>
<span class="normal"><a href="#__codelineno-6-45"> 45</a></span>
<span class="normal"><a href="#__codelineno-6-46"> 46</a></span>
<span class="normal"><a href="#__codelineno-6-47"> 47</a></span>
<span class="normal"><a href="#__codelineno-6-48"> 48</a></span>
<span class="normal"><a href="#__codelineno-6-49"> 49</a></span>
<span class="normal"><a href="#__codelineno-6-50"> 50</a></span>
<span class="normal"><a href="#__codelineno-6-51"> 51</a></span>
<span class="normal"><a href="#__codelineno-6-52"> 52</a></span>
<span class="normal"><a href="#__codelineno-6-53"> 53</a></span>
<span class="normal"><a href="#__codelineno-6-54"> 54</a></span>
<span class="normal"><a href="#__codelineno-6-55"> 55</a></span>
<span class="normal"><a href="#__codelineno-6-56"> 56</a></span>
<span class="normal"><a href="#__codelineno-6-57"> 57</a></span>
<span class="normal"><a href="#__codelineno-6-58"> 58</a></span>
<span class="normal"><a href="#__codelineno-6-59"> 59</a></span>
<span class="normal"><a href="#__codelineno-6-60"> 60</a></span>
<span class="normal"><a href="#__codelineno-6-61"> 61</a></span>
<span class="normal"><a href="#__codelineno-6-62"> 62</a></span>
<span class="normal"><a href="#__codelineno-6-63"> 63</a></span>
<span class="normal"><a href="#__codelineno-6-64"> 64</a></span>
<span class="normal"><a href="#__codelineno-6-65"> 65</a></span>
<span class="normal"><a href="#__codelineno-6-66"> 66</a></span>
<span class="normal"><a href="#__codelineno-6-67"> 67</a></span>
<span class="normal"><a href="#__codelineno-6-68"> 68</a></span>
<span class="normal"><a href="#__codelineno-6-69"> 69</a></span>
<span class="normal"><a href="#__codelineno-6-70"> 70</a></span>
<span class="normal"><a href="#__codelineno-6-71"> 71</a></span>
<span class="normal"><a href="#__codelineno-6-72"> 72</a></span>
<span class="normal"><a href="#__codelineno-6-73"> 73</a></span>
<span class="normal"><a href="#__codelineno-6-74"> 74</a></span>
<span class="normal"><a href="#__codelineno-6-75"> 75</a></span>
<span class="normal"><a href="#__codelineno-6-76"> 76</a></span>
<span class="normal"><a href="#__codelineno-6-77"> 77</a></span>
<span class="normal"><a href="#__codelineno-6-78"> 78</a></span>
<span class="normal"><a href="#__codelineno-6-79"> 79</a></span>
<span class="normal"><a href="#__codelineno-6-80"> 80</a></span>
<span class="normal"><a href="#__codelineno-6-81"> 81</a></span>
<span class="normal"><a href="#__codelineno-6-82"> 82</a></span>
<span class="normal"><a href="#__codelineno-6-83"> 83</a></span>
<span class="normal"><a href="#__codelineno-6-84"> 84</a></span>
<span class="normal"><a href="#__codelineno-6-85"> 85</a></span>
<span class="normal"><a href="#__codelineno-6-86"> 86</a></span>
<span class="normal"><a href="#__codelineno-6-87"> 87</a></span>
<span class="normal"><a href="#__codelineno-6-88"> 88</a></span>
<span class="normal"><a href="#__codelineno-6-89"> 89</a></span>
<span class="normal"><a href="#__codelineno-6-90"> 90</a></span>
<span class="normal"><a href="#__codelineno-6-91"> 91</a></span>
<span class="normal"><a href="#__codelineno-6-92"> 92</a></span>
<span class="normal"><a href="#__codelineno-6-93"> 93</a></span>
<span class="normal"><a href="#__codelineno-6-94"> 94</a></span>
<span class="normal"><a href="#__codelineno-6-95"> 95</a></span>
<span class="normal"><a href="#__codelineno-6-96"> 96</a></span>
<span class="normal"><a href="#__codelineno-6-97"> 97</a></span>
<span class="normal"><a href="#__codelineno-6-98"> 98</a></span>
<span class="normal"><a href="#__codelineno-6-99"> 99</a></span>
<span class="normal"><a href="#__codelineno-6-100">100</a></span>
<span class="normal"><a href="#__codelineno-6-101">101</a></span>
<span class="normal"><a href="#__codelineno-6-102">102</a></span>
<span class="normal"><a href="#__codelineno-6-103">103</a></span>
<span class="normal"><a href="#__codelineno-6-104">104</a></span>
<span class="normal"><a href="#__codelineno-6-105">105</a></span>
<span class="normal"><a href="#__codelineno-6-106">106</a></span>
<span class="normal"><a href="#__codelineno-6-107">107</a></span>
<span class="normal"><a href="#__codelineno-6-108">108</a></span>
<span class="normal"><a href="#__codelineno-6-109">109</a></span>
<span class="normal"><a href="#__codelineno-6-110">110</a></span>
<span class="normal"><a href="#__codelineno-6-111">111</a></span>
<span class="normal"><a href="#__codelineno-6-112">112</a></span>
<span class="normal"><a href="#__codelineno-6-113">113</a></span>
<span class="normal"><a href="#__codelineno-6-114">114</a></span>
<span class="normal"><a href="#__codelineno-6-115">115</a></span>
<span class="normal"><a href="#__codelineno-6-116">116</a></span>
<span class="normal"><a href="#__codelineno-6-117">117</a></span>
<span class="normal"><a href="#__codelineno-6-118">118</a></span>
<span class="normal"><a href="#__codelineno-6-119">119</a></span>
<span class="normal"><a href="#__codelineno-6-120">120</a></span>
<span class="normal"><a href="#__codelineno-6-121">121</a></span>
<span class="normal"><a href="#__codelineno-6-122">122</a></span>
<span class="normal"><a href="#__codelineno-6-123">123</a></span>
<span class="normal"><a href="#__codelineno-6-124">124</a></span>
<span class="normal"><a href="#__codelineno-6-125">125</a></span>
<span class="normal"><a href="#__codelineno-6-126">126</a></span>
<span class="normal"><a href="#__codelineno-6-127">127</a></span>
<span class="normal"><a href="#__codelineno-6-128">128</a></span>
<span class="normal"><a href="#__codelineno-6-129">129</a></span>
<span class="normal"><a href="#__codelineno-6-130">130</a></span>
<span class="normal"><a href="#__codelineno-6-131">131</a></span>
<span class="normal"><a href="#__codelineno-6-132">132</a></span>
<span class="normal"><a href="#__codelineno-6-133">133</a></span>
<span class="normal"><a href="#__codelineno-6-134">134</a></span>
<span class="normal"><a href="#__codelineno-6-135">135</a></span>
<span class="normal"><a href="#__codelineno-6-136">136</a></span>
<span class="normal"><a href="#__codelineno-6-137">137</a></span>
<span class="normal"><a href="#__codelineno-6-138">138</a></span>
<span class="normal"><a href="#__codelineno-6-139">139</a></span>
<span class="normal"><a href="#__codelineno-6-140">140</a></span>
<span class="normal"><a href="#__codelineno-6-141">141</a></span>
<span class="normal"><a href="#__codelineno-6-142">142</a></span>
<span class="normal"><a href="#__codelineno-6-143">143</a></span>
<span class="normal"><a href="#__codelineno-6-144">144</a></span>
<span class="normal"><a href="#__codelineno-6-145">145</a></span>
<span class="normal"><a href="#__codelineno-6-146">146</a></span>
<span class="normal"><a href="#__codelineno-6-147">147</a></span>
<span class="normal"><a href="#__codelineno-6-148">148</a></span>
<span class="normal"><a href="#__codelineno-6-149">149</a></span>
<span class="normal"><a href="#__codelineno-6-150">150</a></span>
<span class="normal"><a href="#__codelineno-6-151">151</a></span>
<span class="normal"><a href="#__codelineno-6-152">152</a></span>
<span class="normal"><a href="#__codelineno-6-153">153</a></span>
<span class="normal"><a href="#__codelineno-6-154">154</a></span>
<span class="normal"><a href="#__codelineno-6-155">155</a></span>
<span class="normal"><a href="#__codelineno-6-156">156</a></span>
<span class="normal"><a href="#__codelineno-6-157">157</a></span>
<span class="normal"><a href="#__codelineno-6-158">158</a></span>
<span class="normal"><a href="#__codelineno-6-159">159</a></span>
<span class="normal"><a href="#__codelineno-6-160">160</a></span>
<span class="normal"><a href="#__codelineno-6-161">161</a></span>
<span class="normal"><a href="#__codelineno-6-162">162</a></span>
<span class="normal"><a href="#__codelineno-6-163">163</a></span>
<span class="normal"><a href="#__codelineno-6-164">164</a></span>
<span class="normal"><a href="#__codelineno-6-165">165</a></span>
<span class="normal"><a href="#__codelineno-6-166">166</a></span>
<span class="normal"><a href="#__codelineno-6-167">167</a></span>
<span class="normal"><a href="#__codelineno-6-168">168</a></span>
<span class="normal"><a href="#__codelineno-6-169">169</a></span>
<span class="normal"><a href="#__codelineno-6-170">170</a></span>
<span class="normal"><a href="#__codelineno-6-171">171</a></span>
<span class="normal"><a href="#__codelineno-6-172">172</a></span>
<span class="normal"><a href="#__codelineno-6-173">173</a></span>
<span class="normal"><a href="#__codelineno-6-174">174</a></span>
<span class="normal"><a href="#__codelineno-6-175">175</a></span>
<span class="normal"><a href="#__codelineno-6-176">176</a></span>
<span class="normal"><a href="#__codelineno-6-177">177</a></span>
<span class="normal"><a href="#__codelineno-6-178">178</a></span>
<span class="normal"><a href="#__codelineno-6-179">179</a></span>
<span class="normal"><a href="#__codelineno-6-180">180</a></span>
<span class="normal"><a href="#__codelineno-6-181">181</a></span>
<span class="normal"><a href="#__codelineno-6-182">182</a></span>
<span class="normal"><a href="#__codelineno-6-183">183</a></span>
<span class="normal"><a href="#__codelineno-6-184">184</a></span>
<span class="normal"><a href="#__codelineno-6-185">185</a></span>
<span class="normal"><a href="#__codelineno-6-186">186</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<a id="__codelineno-6-2" name="__codelineno-6-2"></a>
<a id="__codelineno-6-3" name="__codelineno-6-3"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<a id="__codelineno-6-4" name="__codelineno-6-4"></a><span class="c1"># 从 PyTorch 优化器基类中导入 Optimizer</span>
<a id="__codelineno-6-5" name="__codelineno-6-5"></a><span class="kn">from</span><span class="w"> </span><span class="nn">torch.optim.optimizer</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optimizer</span>
<a id="__codelineno-6-6" name="__codelineno-6-6"></a>
<a id="__codelineno-6-7" name="__codelineno-6-7"></a><span class="c1"># 从本地类型定义文件中导入类型提示</span>
<a id="__codelineno-6-8" name="__codelineno-6-8"></a><span class="kn">from</span><span class="w"> </span><span class="nn">.types</span><span class="w"> </span><span class="kn">import</span> <span class="n">Betas2</span><span class="p">,</span> <span class="n">OptFloat</span><span class="p">,</span> <span class="n">OptLossClosure</span><span class="p">,</span> <span class="n">Params</span>
<a id="__codelineno-6-9" name="__codelineno-6-9"></a>
<a id="__codelineno-6-10" name="__codelineno-6-10"></a><span class="c1"># 定义当 `from module import *` 时，哪些对象会被导出</span>
<a id="__codelineno-6-11" name="__codelineno-6-11"></a><span class="n">__all__</span> <span class="o">=</span> <span class="p">(</span><span class="s1">'Lamb'</span><span class="p">,)</span>
<a id="__codelineno-6-12" name="__codelineno-6-12"></a>
<a id="__codelineno-6-13" name="__codelineno-6-13"></a>
<a id="__codelineno-6-14" name="__codelineno-6-14"></a><span class="c1"># 定义 Lamb 优化器类，继承自 Optimizer</span>
<a id="__codelineno-6-15" name="__codelineno-6-15"></a><span class="k">class</span><span class="w"> </span><span class="nc">Lamb</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
<a id="__codelineno-6-16" name="__codelineno-6-16"></a><span class="w">    </span><span class="sa">r</span><span class="sd">"""实现了 Lamb 算法。</span>
<a id="__codelineno-6-17" name="__codelineno-6-17"></a>
<a id="__codelineno-6-18" name="__codelineno-6-18"></a><span class="sd">    该算法在论文 `Large Batch Optimization for Deep Learning:</span>
<a id="__codelineno-6-19" name="__codelineno-6-19"></a><span class="sd">    Training BERT in 76 minutes`__ 中被提出。</span>
<a id="__codelineno-6-20" name="__codelineno-6-20"></a>
<a id="__codelineno-6-21" name="__codelineno-6-21"></a><span class="sd">    参数:</span>
<a id="__codelineno-6-22" name="__codelineno-6-22"></a><span class="sd">        params: 需要优化的、可迭代的参数，或定义了参数组的字典。</span>
<a id="__codelineno-6-23" name="__codelineno-6-23"></a><span class="sd">        lr: 学习率 (默认: 1e-3)。</span>
<a id="__codelineno-6-24" name="__codelineno-6-24"></a><span class="sd">        betas: 用于计算梯度的一阶和二阶矩的运行平均值的系数 (默认: (0.9, 0.999))。</span>
<a id="__codelineno-6-25" name="__codelineno-6-25"></a><span class="sd">        eps: 为了提高数值稳定性而加到分母上的一项 (默认: 1e-6)。</span>
<a id="__codelineno-6-26" name="__codelineno-6-26"></a><span class="sd">        weight_decay: 权重衰减 (L2 惩罚项) (默认: 0)。</span>
<a id="__codelineno-6-27" name="__codelineno-6-27"></a><span class="sd">        clamp_value: 将 weight_norm 裁剪（clamp）在 (0, clamp_value) 范围内 (默认: 10)。</span>
<a id="__codelineno-6-28" name="__codelineno-6-28"></a><span class="sd">                     可以将其设置为一个很大的值 (例如 10e3) 来避免裁剪。</span>
<a id="__codelineno-6-29" name="__codelineno-6-29"></a><span class="sd">        adam: 如果为 True，则总是使用 trust_ratio = 1，这会使算法退化为 AdamW。</span>
<a id="__codelineno-6-30" name="__codelineno-6-30"></a><span class="sd">              这对于进行性能比较很有用。(默认: False)。</span>
<a id="__codelineno-6-31" name="__codelineno-6-31"></a><span class="sd">        debias: 通过 (1 - beta**step) 来对 Adam 的矩估计进行偏差修正 (默认: False)。</span>
<a id="__codelineno-6-32" name="__codelineno-6-32"></a><span class="sd">                论文的最终版本没有使用此项。</span>
<a id="__codelineno-6-33" name="__codelineno-6-33"></a>
<a id="__codelineno-6-34" name="__codelineno-6-34"></a><span class="sd">    示例:</span>
<a id="__codelineno-6-35" name="__codelineno-6-35"></a><span class="sd">        &gt;&gt;&gt; import torch_optimizer as optim</span>
<a id="__codelineno-6-36" name="__codelineno-6-36"></a><span class="sd">        &gt;&gt;&gt; optimizer = optim.Lamb(model.parameters(), lr=0.1)</span>
<a id="__codelineno-6-37" name="__codelineno-6-37"></a><span class="sd">        &gt;&gt;&gt; optimizer.zero_grad()</span>
<a id="__codelineno-6-38" name="__codelineno-6-38"></a><span class="sd">        &gt;&gt;&gt; loss_fn(model(input), target).backward()</span>
<a id="__codelineno-6-39" name="__codelineno-6-39"></a><span class="sd">        &gt;&gt;&gt; optimizer.step()</span>
<a id="__codelineno-6-40" name="__codelineno-6-40"></a>
<a id="__codelineno-6-41" name="__codelineno-6-41"></a><span class="sd">    __ https://arxiv.org/abs/1904.00962</span>
<a id="__codelineno-6-42" name="__codelineno-6-42"></a>
<a id="__codelineno-6-43" name="__codelineno-6-43"></a><span class="sd">    注意:</span>
<a id="__codelineno-6-44" name="__codelineno-6-44"></a><span class="sd">        参考代码: https://github.com/cybertronai/pytorch-lamb</span>
<a id="__codelineno-6-45" name="__codelineno-6-45"></a><span class="sd">    """</span>
<a id="__codelineno-6-46" name="__codelineno-6-46"></a>
<a id="__codelineno-6-47" name="__codelineno-6-47"></a>    <span class="c1"># 类的构造函数</span>
<a id="__codelineno-6-48" name="__codelineno-6-48"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-6-49" name="__codelineno-6-49"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-6-50" name="__codelineno-6-50"></a>        <span class="n">params</span><span class="p">:</span> <span class="n">Params</span><span class="p">,</span>
<a id="__codelineno-6-51" name="__codelineno-6-51"></a>        <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
<a id="__codelineno-6-52" name="__codelineno-6-52"></a>        <span class="n">betas</span><span class="p">:</span> <span class="n">Betas2</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
<a id="__codelineno-6-53" name="__codelineno-6-53"></a>        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
<a id="__codelineno-6-54" name="__codelineno-6-54"></a>        <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<a id="__codelineno-6-55" name="__codelineno-6-55"></a>        <span class="n">clamp_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
<a id="__codelineno-6-56" name="__codelineno-6-56"></a>        <span class="n">adam</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-6-57" name="__codelineno-6-57"></a>        <span class="n">debias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-6-58" name="__codelineno-6-58"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-6-59" name="__codelineno-6-59"></a>        <span class="c1"># --- 输入参数合法性检查 ---</span>
<a id="__codelineno-6-60" name="__codelineno-6-60"></a>        <span class="k">if</span> <span class="n">lr</span> <span class="o">&lt;=</span> <span class="mf">0.0</span><span class="p">:</span>
<a id="__codelineno-6-61" name="__codelineno-6-61"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'无效的学习率: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
<a id="__codelineno-6-62" name="__codelineno-6-62"></a>        <span class="k">if</span> <span class="n">eps</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">:</span>
<a id="__codelineno-6-63" name="__codelineno-6-63"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'无效的 epsilon 值: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
<a id="__codelineno-6-64" name="__codelineno-6-64"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
<a id="__codelineno-6-65" name="__codelineno-6-65"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<a id="__codelineno-6-66" name="__codelineno-6-66"></a>                <span class="s1">'无效的 beta 参数 (索引 0): </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<a id="__codelineno-6-67" name="__codelineno-6-67"></a>            <span class="p">)</span>
<a id="__codelineno-6-68" name="__codelineno-6-68"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
<a id="__codelineno-6-69" name="__codelineno-6-69"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<a id="__codelineno-6-70" name="__codelineno-6-70"></a>                <span class="s1">'无效的 beta 参数 (索引 1): </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<a id="__codelineno-6-71" name="__codelineno-6-71"></a>            <span class="p">)</span>
<a id="__codelineno-6-72" name="__codelineno-6-72"></a>        <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-6-73" name="__codelineno-6-73"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<a id="__codelineno-6-74" name="__codelineno-6-74"></a>                <span class="s1">'无效的 weight_decay 值: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">weight_decay</span><span class="p">)</span>
<a id="__codelineno-6-75" name="__codelineno-6-75"></a>            <span class="p">)</span>
<a id="__codelineno-6-76" name="__codelineno-6-76"></a>        <span class="k">if</span> <span class="n">clamp_value</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">:</span>
<a id="__codelineno-6-77" name="__codelineno-6-77"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'无效的 clamp 值: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clamp_value</span><span class="p">))</span>
<a id="__codelineno-6-78" name="__codelineno-6-78"></a>
<a id="__codelineno-6-79" name="__codelineno-6-79"></a>        <span class="c1"># 将超参数打包成一个字典，作为默认配置</span>
<a id="__codelineno-6-80" name="__codelineno-6-80"></a>        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
<a id="__codelineno-6-81" name="__codelineno-6-81"></a>        <span class="c1"># 将 Lamb 特有的参数保存为类的属性</span>
<a id="__codelineno-6-82" name="__codelineno-6-82"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">clamp_value</span> <span class="o">=</span> <span class="n">clamp_value</span>
<a id="__codelineno-6-83" name="__codelineno-6-83"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adam</span> <span class="o">=</span> <span class="n">adam</span>
<a id="__codelineno-6-84" name="__codelineno-6-84"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">debias</span> <span class="o">=</span> <span class="n">debias</span>
<a id="__codelineno-6-85" name="__codelineno-6-85"></a>
<a id="__codelineno-6-86" name="__codelineno-6-86"></a>        <span class="c1"># 调用父类 (Optimizer) 的构造函数</span>
<a id="__codelineno-6-87" name="__codelineno-6-87"></a>        <span class="nb">super</span><span class="p">(</span><span class="n">Lamb</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>
<a id="__codelineno-6-88" name="__codelineno-6-88"></a>
<a id="__codelineno-6-89" name="__codelineno-6-89"></a>    <span class="c1"># `step` 方法是优化器的核心，`@torch.no_grad()` 装饰器禁用梯度计算</span>
<a id="__codelineno-6-90" name="__codelineno-6-90"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="p">:</span> <span class="n">OptLossClosure</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">OptFloat</span><span class="p">:</span>
<a id="__codelineno-6-91" name="__codelineno-6-91"></a><span class="w">        </span><span class="sa">r</span><span class="sd">"""执行单步优化。</span>
<a id="__codelineno-6-92" name="__codelineno-6-92"></a>
<a id="__codelineno-6-93" name="__codelineno-6-93"></a><span class="sd">        参数:</span>
<a id="__codelineno-6-94" name="__codelineno-6-94"></a><span class="sd">            closure: 一个可以重新评估模型并返回损失的闭包函数 (可选)。</span>
<a id="__codelineno-6-95" name="__codelineno-6-95"></a><span class="sd">        """</span>
<a id="__codelineno-6-96" name="__codelineno-6-96"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
<a id="__codelineno-6-97" name="__codelineno-6-97"></a>        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-6-98" name="__codelineno-6-98"></a>            <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>
<a id="__codelineno-6-99" name="__codelineno-6-99"></a>
<a id="__codelineno-6-100" name="__codelineno-6-100"></a>        <span class="c1"># 遍历所有参数组</span>
<a id="__codelineno-6-101" name="__codelineno-6-101"></a>        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
<a id="__codelineno-6-102" name="__codelineno-6-102"></a>            <span class="c1"># 遍历当前参数组中的每一个参数 (p)</span>
<a id="__codelineno-6-103" name="__codelineno-6-103"></a>            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">'params'</span><span class="p">]:</span>
<a id="__codelineno-6-104" name="__codelineno-6-104"></a>                <span class="c1"># 如果参数没有梯度，则跳过</span>
<a id="__codelineno-6-105" name="__codelineno-6-105"></a>                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-6-106" name="__codelineno-6-106"></a>                    <span class="k">continue</span>
<a id="__codelineno-6-107" name="__codelineno-6-107"></a>                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
<a id="__codelineno-6-108" name="__codelineno-6-108"></a>                <span class="c1"># Lamb 算法不支持稀疏梯度</span>
<a id="__codelineno-6-109" name="__codelineno-6-109"></a>                <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
<a id="__codelineno-6-110" name="__codelineno-6-110"></a>                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-6-111" name="__codelineno-6-111"></a>                        <span class="s1">'Lamb 不支持稀疏梯度, '</span>
<a id="__codelineno-6-112" name="__codelineno-6-112"></a>                        <span class="s1">'请考虑使用 SparseAdam'</span>
<a id="__codelineno-6-113" name="__codelineno-6-113"></a>                    <span class="p">)</span>
<a id="__codelineno-6-114" name="__codelineno-6-114"></a>                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
<a id="__codelineno-6-115" name="__codelineno-6-115"></a>
<a id="__codelineno-6-116" name="__codelineno-6-116"></a>                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="c1"># 获取该参数的状态字典</span>
<a id="__codelineno-6-117" name="__codelineno-6-117"></a>
<a id="__codelineno-6-118" name="__codelineno-6-118"></a>                <span class="c1"># --- 状态初始化 (State Initialization) ---</span>
<a id="__codelineno-6-119" name="__codelineno-6-119"></a>                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-6-120" name="__codelineno-6-120"></a>                    <span class="n">state</span><span class="p">[</span><span class="s1">'step'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-6-121" name="__codelineno-6-121"></a>                    <span class="c1"># 梯度的一阶矩（动量）</span>
<a id="__codelineno-6-122" name="__codelineno-6-122"></a>                    <span class="n">state</span><span class="p">[</span><span class="s1">'exp_avg'</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
<a id="__codelineno-6-123" name="__codelineno-6-123"></a>                        <span class="n">p</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span>
<a id="__codelineno-6-124" name="__codelineno-6-124"></a>                    <span class="p">)</span>
<a id="__codelineno-6-125" name="__codelineno-6-125"></a>                    <span class="c1"># 梯度的二阶矩（未开方的方差）</span>
<a id="__codelineno-6-126" name="__codelineno-6-126"></a>                    <span class="n">state</span><span class="p">[</span><span class="s1">'exp_avg_sq'</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
<a id="__codelineno-6-127" name="__codelineno-6-127"></a>                        <span class="n">p</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span>
<a id="__codelineno-6-128" name="__codelineno-6-128"></a>                    <span class="p">)</span>
<a id="__codelineno-6-129" name="__codelineno-6-129"></a>
<a id="__codelineno-6-130" name="__codelineno-6-130"></a>                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">'exp_avg'</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="s1">'exp_avg_sq'</span><span class="p">]</span>
<a id="__codelineno-6-131" name="__codelineno-6-131"></a>                <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">'betas'</span><span class="p">]</span>
<a id="__codelineno-6-132" name="__codelineno-6-132"></a>
<a id="__codelineno-6-133" name="__codelineno-6-133"></a>                <span class="n">state</span><span class="p">[</span><span class="s1">'step'</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<a id="__codelineno-6-134" name="__codelineno-6-134"></a>
<a id="__codelineno-6-135" name="__codelineno-6-135"></a>                <span class="c1"># --- Adam 核心计算部分 ---</span>
<a id="__codelineno-6-136" name="__codelineno-6-136"></a>                <span class="c1"># 1. 更新梯度的一阶和二阶矩估计</span>
<a id="__codelineno-6-137" name="__codelineno-6-137"></a>                <span class="c1"># 更新一阶矩 (动量): m_t = beta1 * m_{t-1} + (1 - beta1) * g_t</span>
<a id="__codelineno-6-138" name="__codelineno-6-138"></a>                <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>
<a id="__codelineno-6-139" name="__codelineno-6-139"></a>                <span class="c1"># 更新二阶矩: v_t = beta2 * v_{t-1} + (1 - beta2) * g_t^2</span>
<a id="__codelineno-6-140" name="__codelineno-6-140"></a>                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>
<a id="__codelineno-6-141" name="__codelineno-6-141"></a>
<a id="__codelineno-6-142" name="__codelineno-6-142"></a>                <span class="c1"># 2. 偏差修正 (可选)</span>
<a id="__codelineno-6-143" name="__codelineno-6-143"></a>                <span class="c1"># 论文的 v3 版本（最终版）并未使用偏差修正。</span>
<a id="__codelineno-6-144" name="__codelineno-6-144"></a>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debias</span><span class="p">:</span>
<a id="__codelineno-6-145" name="__codelineno-6-145"></a>                    <span class="n">bias_correction</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">'step'</span><span class="p">])</span>
<a id="__codelineno-6-146" name="__codelineno-6-146"></a>                    <span class="n">bias_correction</span> <span class="o">/=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">'step'</span><span class="p">]</span>
<a id="__codelineno-6-147" name="__codelineno-6-147"></a>                <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-6-148" name="__codelineno-6-148"></a>                    <span class="n">bias_correction</span> <span class="o">=</span> <span class="mi">1</span>
<a id="__codelineno-6-149" name="__codelineno-6-149"></a>
<a id="__codelineno-6-150" name="__codelineno-6-150"></a>                <span class="c1"># 将偏差修正项和学习率合并，避免后续的广播操作</span>
<a id="__codelineno-6-151" name="__codelineno-6-151"></a>                <span class="n">step_size</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">'lr'</span><span class="p">]</span> <span class="o">*</span> <span class="n">bias_correction</span>
<a id="__codelineno-6-152" name="__codelineno-6-152"></a>
<a id="__codelineno-6-153" name="__codelineno-6-153"></a>                <span class="c1"># --- LAMB 核心计算部分 ---</span>
<a id="__codelineno-6-154" name="__codelineno-6-154"></a>                <span class="c1"># 3. 计算权重的范数，并进行裁剪</span>
<a id="__codelineno-6-155" name="__codelineno-6-155"></a>                <span class="n">weight_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">clamp_value</span><span class="p">)</span>
<a id="__codelineno-6-156" name="__codelineno-6-156"></a>
<a id="__codelineno-6-157" name="__codelineno-6-157"></a>                <span class="c1"># 4. 计算 Adam 的更新步长 (Adam Step)，并加入解耦权重衰减 (AdamW)</span>
<a id="__codelineno-6-158" name="__codelineno-6-158"></a>                <span class="n">adam_step</span> <span class="o">=</span> <span class="n">exp_avg</span> <span class="o">/</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">'eps'</span><span class="p">])</span>
<a id="__codelineno-6-159" name="__codelineno-6-159"></a>                <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">'weight_decay'</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-6-160" name="__codelineno-6-160"></a>                    <span class="n">adam_step</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s1">'weight_decay'</span><span class="p">])</span>
<a id="__codelineno-6-161" name="__codelineno-6-161"></a>
<a id="__codelineno-6-162" name="__codelineno-6-162"></a>                <span class="c1"># 5. 计算 Adam 更新步长的范数</span>
<a id="__codelineno-6-163" name="__codelineno-6-163"></a>                <span class="n">adam_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">adam_step</span><span class="p">)</span>
<a id="__codelineno-6-164" name="__codelineno-6-164"></a>
<a id="__codelineno-6-165" name="__codelineno-6-165"></a>                <span class="c1"># 6. 计算信任比率 (Trust Ratio)</span>
<a id="__codelineno-6-166" name="__codelineno-6-166"></a>                <span class="c1"># 这是 LAMB 算法的精髓：trust_ratio = ||w|| / ||g_update||</span>
<a id="__codelineno-6-167" name="__codelineno-6-167"></a>                <span class="k">if</span> <span class="n">weight_norm</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">adam_norm</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-6-168" name="__codelineno-6-168"></a>                    <span class="n">trust_ratio</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># 避免除以零</span>
<a id="__codelineno-6-169" name="__codelineno-6-169"></a>                <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-6-170" name="__codelineno-6-170"></a>                    <span class="n">trust_ratio</span> <span class="o">=</span> <span class="n">weight_norm</span> <span class="o">/</span> <span class="n">adam_norm</span>
<a id="__codelineno-6-171" name="__codelineno-6-171"></a>
<a id="__codelineno-6-172" name="__codelineno-6-172"></a>                <span class="c1"># (可选) 将中间变量存入 state，便于调试</span>
<a id="__codelineno-6-173" name="__codelineno-6-173"></a>                <span class="n">state</span><span class="p">[</span><span class="s1">'weight_norm'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_norm</span>
<a id="__codelineno-6-174" name="__codelineno-6-174"></a>                <span class="n">state</span><span class="p">[</span><span class="s1">'adam_norm'</span><span class="p">]</span> <span class="o">=</span> <span class="n">adam_norm</span>
<a id="__codelineno-6-175" name="__codelineno-6-175"></a>                <span class="n">state</span><span class="p">[</span><span class="s1">'trust_ratio'</span><span class="p">]</span> <span class="o">=</span> <span class="n">trust_ratio</span>
<a id="__codelineno-6-176" name="__codelineno-6-176"></a>
<a id="__codelineno-6-177" name="__codelineno-6-177"></a>                <span class="c1"># 如果 adam 标志为 True，则强制 trust_ratio=1，使算法退化为 AdamW</span>
<a id="__codelineno-6-178" name="__codelineno-6-178"></a>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adam</span><span class="p">:</span>
<a id="__codelineno-6-179" name="__codelineno-6-179"></a>                    <span class="n">trust_ratio</span> <span class="o">=</span> <span class="mi">1</span>
<a id="__codelineno-6-180" name="__codelineno-6-180"></a>
<a id="__codelineno-6-181" name="__codelineno-6-181"></a>                <span class="c1"># 7. 应用最终更新</span>
<a id="__codelineno-6-182" name="__codelineno-6-182"></a>                <span class="c1"># 更新公式: p_new = p_old - (step_size * trust_ratio) * adam_step</span>
<a id="__codelineno-6-183" name="__codelineno-6-183"></a>                <span class="c1"># 这里的 `trust_ratio` 动态地缩放了每个参数（或每层）的学习率。</span>
<a id="__codelineno-6-184" name="__codelineno-6-184"></a>                <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">adam_step</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">step_size</span> <span class="o">*</span> <span class="n">trust_ratio</span><span class="p">)</span>
<a id="__codelineno-6-185" name="__codelineno-6-185"></a>
<a id="__codelineno-6-186" name="__codelineno-6-186"></a>        <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
</details>
<h2 id="shampoo">Shampoo<a class="headerlink" href="#shampoo" title="Permanent link">¶</a></h2>
<p>让我们回顾那个在最优点附近的 Hessian 近似： <span class="arithmatex">\(H\approx\dfrac{1}{\sigma^2} \sqrt{GG^\top}\)</span>，Shampoo 的思想是选取更精确的近似以逼进 <span class="arithmatex">\(GG^\top\)</span>。</p>
<p>提一嘴，这里的 <span class="arithmatex">\(GG^\top\)</span> 指的是将 <span class="arithmatex">\(g\)</span> 展平之后的外积，也就是 <span class="arithmatex">\(\mathrm{vec}(g)\mathrm{vec}(g)^\top\)</span>，鉴于之前我们一直研究的都是 <span class="arithmatex">\(g\)</span> 的对角线乘积近似，由于简化很多所以没有特别明确这个维度问题，因此在这里明确一下。</p>
<p>Shampoo 优化器的第一步，是考虑现在的多层神经网络内，层之间是相互独立的。因此可以把大型的 <span class="arithmatex">\(GG^\top\)</span> 给分块对角化，每一个对角块对应某个层的梯度外积。</p>
<p>但是即使这样，单层的参数量也很大，考虑一个 <span class="arithmatex">\(n\times m\)</span> 的 fc layer，<span class="arithmatex">\(GG^\top\)</span> 的参数量就来到了 <span class="arithmatex">\((mn)\times(nm)\)</span>，直接平方。而这就带领我们进入 Shampoo 优化器推导的真正精妙之处。</p>
<p>作者认为，海森矩阵可以由两个小矩阵的 Kronecker 积近似，也就是</p>
<div class="arithmatex">\[
H=(GG^\top)^{-1}=L\otimes R
\]</div>
<p>这样一拆开参数量暴降到 <span class="arithmatex">\(L\)</span> 的 <span class="arithmatex">\(n^2\)</span> 加上 <span class="arithmatex">\(R\)</span> 的 <span class="arithmatex">\(m^2\)</span>。可以理解成 <span class="arithmatex">\(L\)</span> 捕获输入维的信息，<span class="arithmatex">\(R\)</span> 捕获输出为的信息（不过我觉得有点强行解释了哈哈，因为关键是节省计算量，看一路过来我们都是在寻求尽可能<strong>高效</strong>而不是最有道理的优化器）。</p>
<p>下面推导 <span class="arithmatex">\(L\)</span> 和 <span class="arithmatex">\(R\)</span> 的更新式。先介绍 Kronecker 积的几个小性质：<span class="arithmatex">\(\mathrm{vec}(BXA^\top)=(A\otimes B)\mathrm{vec}(X)\)</span> 和 <span class="arithmatex">\((A\otimes B)^{-1}=(A^{-1}\otimes B^{-1})\)</span> （转置亦然）。</p>
<p>取 <span class="arithmatex">\(B=A=G\)</span>， <span class="arithmatex">\(X = I\)</span> 那么 <span class="arithmatex">\(\mathrm{vec}(GG^\top)=(G\otimes G)\mathrm{vec}(I)\)</span> 但这和我们期待的结构仍有距离，不过我们可以换成未展平的原矩阵也就是利用：</p>
<div class="arithmatex">\[
(g\otimes g)(g\otimes g)^\top=(g\otimes g)(g^\top\otimes g^\top)=gg^\top \otimes g^\top g
\]</div>
<p>来近似 <span class="arithmatex">\(H^2\)</span>。</p>
<p>上述推导非常类似 K-FAC 算法，具体请参考<a href="https://blog.csdn.net/xbinworld/article/details/105184601">这个博客</a>。</p>
<p>无论如何根据我们之前的经验，这里的 <span class="arithmatex">\(L\)</span> 和 <span class="arithmatex">\(R\)</span> 也必然是要取滑动平均的（或者利用类似 AdaGrad 的思路，如果下面 <span class="arithmatex">\(\beta=1\)</span> 的话），也就是</p>
<div class="arithmatex">\[
L_n = \beta L_{n-1} + g_n g_n^\top\\
R_n = \beta R_{n-1} + g_n^\top g_n
\]</div>
<p>那么我们对参数进行更新，就是计算 <span class="arithmatex">\(H^{-1}g\)</span>，展开，并利用 Kronecker 积的性质，得到</p>
<div class="arithmatex">\[
H^{-\frac 12}g=L^{- \frac 14}gR^{- \frac 14}
\]</div>
<p>其中 <span class="arithmatex">\(\frac 14 + \frac 14 = \frac 12\)</span>，这样就得到了我们的对单层的更新。</p>
<p>对于 <span class="arithmatex">\(k-1\)</span> 层的网络（即 <span class="arithmatex">\(k\)</span> 阶张量），我们需要重复计算 <span class="arithmatex">\(k\)</span> 次再组合成大的 <span class="arithmatex">\(H^{-\frac 12}\)</span>，那么每一次计算的量就应该是 <span class="arithmatex">\(H^{-\frac 1{2k}}\)</span>。</p>
<p>这里要对张量的情况做一些说明：由于 <span class="arithmatex">\(g_n\)</span> 是一个张量，所以在这个遍历张量 <span class="arithmatex">\(k\)</span> 个阶的过程中，要执行展平操作，即 <span class="arithmatex">\(\mathrm{Flatten}(i;g_n)\)</span> 的意思是取第 <span class="arithmatex">\(i\)</span> 阶的维度作为矩阵的第一个维度，再把其他阶的维度乘起来作为矩阵的第二个维度，由此将 <span class="arithmatex">\(k\)</span> 阶张量展平到二维的矩阵。这样就能把二维情况推广到 <span class="arithmatex">\(k\)</span> 阶张量。</p>
<div class="arithmatex">\[
\begin{align*}
g_n&amp;=\nabla\mathcal{L({x};\theta_{n-1})}\\
\tilde{G}_n&amp;:=g_n\\
\mathrm{for}\ i &amp;=1,\dots,k:\\
&amp;L^{(i)}_n = \beta L^{(i)}_{n-1} + \mathrm{Flatten}(i;g_n) \mathrm{Flatten}(i;g_n)^\top\\
&amp;P^{(i)}_{t}=(L^{(i)}_n)^{-\frac 1{2k}}\\
&amp;\tilde{G}_n\ \ = P^{(i)}_{t} \times_i \tilde{G}_n\\
\theta_{n} &amp;= \theta_{n-1} - \eta \tilde{G}_n
\end{align*}
\]</div>
<p>此外，<span class="arithmatex">\(P^{(i)}_{t} \times_i \tilde{G}_n\)</span> 的意思是 mode-i product，也就是沿着张量 <span class="arithmatex">\(\tilde G_n\)</span> 的第 <span class="arithmatex">\(i\)</span> 的维度取出向量分别和 <span class="arithmatex">\(P^{(i)}_{t}\)</span> 相乘然后放回，这个操作等价于对 <span class="arithmatex">\(\tilde G_n\)</span> 沿着第 <span class="arithmatex">\(i\)</span> 个维度展平之后再求乘积再折叠。所以也可以看到二维情况的 <span class="arithmatex">\(R\)</span> 在这里消失了，因为沿着第二个维度展开计算相当于第一个维度的转置，这样就可以统一记号。</p>
<p>这里下标出现了 <span class="arithmatex">\(t\)</span> 是因为考虑到取逆 $2k
$ 次根的复杂性，我们不必每一轮迭代都去计算这预条件子 <span class="arithmatex">\(P_{t}\)</span>，而是可以选择在多轮周期之后再更新。</p>
<p>很遗憾的是，<code>torch_optimizer</code> 库实现的 Shampoo 优化器慢到了几乎不可用的水平。怎么高效计算这个根呢？答案要等到后面的 Muon 优化器了。</p>
<p>下面是 Shampoo 优化器的轨迹：</p>
<p><img alt="rastrigin_Shampoo" src="../optimizer_pics/rastrigin_Shampoo.gif"/></p>
<p><img alt="rosenbrock_Shampoo" src="../optimizer_pics/rosenbrock_Shampoo.gif"/></p>
<p>可以看到，有了对二阶信息更精确的估计，Shampoo 的效果不输 Adam。在谷底处 Shampoo 基本上没有了横跳现象。不过，我们能不能把参数更新方向再优化一下？欲知如何优化，且看后文“符号梯度下降”。</p>
<p>由于库实现太慢了，跑一个 batch 就要好几秒，这里求逆根采用的是苏剑林<a href="https://kexue.fm/archives/11175">这个博客</a>提到的 Newton-Schulz 迭代，如果看不明白，可以先去读 Muon 那一章，再去读这个博客即可。</p>
<p><img alt="Shampoo_performance_curves" src="../optimizer_pics/Shampoo_performance_curves.png"/></p>
<p><img alt="Shampoo_landscape_pca" src="../optimizer_pics/Shampoo_landscape_pca.png"/></p>
<p>尽管做了优化，这个版本的 Shampoo 仍然跑得非常慢以至于跑满 P100 也只能有 5.6 it/s 的训练速度。它在第 6000 多 batch 后 train_loss 才勉强收敛到 0.1，不过倒是在第 1200 左右 batch 就能让 acc 上升到 0.9 以上。不过损失地形上面优化器倒是出现了奇怪的横跳，不知道是不是实现的问题，感觉没有发挥出理论的潜力啊……</p>
<details>
<summary> Shampoo 的实现（Newton-Schulz 迭代版）</summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-7-1">  1</a></span>
<span class="normal"><a href="#__codelineno-7-2">  2</a></span>
<span class="normal"><a href="#__codelineno-7-3">  3</a></span>
<span class="normal"><a href="#__codelineno-7-4">  4</a></span>
<span class="normal"><a href="#__codelineno-7-5">  5</a></span>
<span class="normal"><a href="#__codelineno-7-6">  6</a></span>
<span class="normal"><a href="#__codelineno-7-7">  7</a></span>
<span class="normal"><a href="#__codelineno-7-8">  8</a></span>
<span class="normal"><a href="#__codelineno-7-9">  9</a></span>
<span class="normal"><a href="#__codelineno-7-10"> 10</a></span>
<span class="normal"><a href="#__codelineno-7-11"> 11</a></span>
<span class="normal"><a href="#__codelineno-7-12"> 12</a></span>
<span class="normal"><a href="#__codelineno-7-13"> 13</a></span>
<span class="normal"><a href="#__codelineno-7-14"> 14</a></span>
<span class="normal"><a href="#__codelineno-7-15"> 15</a></span>
<span class="normal"><a href="#__codelineno-7-16"> 16</a></span>
<span class="normal"><a href="#__codelineno-7-17"> 17</a></span>
<span class="normal"><a href="#__codelineno-7-18"> 18</a></span>
<span class="normal"><a href="#__codelineno-7-19"> 19</a></span>
<span class="normal"><a href="#__codelineno-7-20"> 20</a></span>
<span class="normal"><a href="#__codelineno-7-21"> 21</a></span>
<span class="normal"><a href="#__codelineno-7-22"> 22</a></span>
<span class="normal"><a href="#__codelineno-7-23"> 23</a></span>
<span class="normal"><a href="#__codelineno-7-24"> 24</a></span>
<span class="normal"><a href="#__codelineno-7-25"> 25</a></span>
<span class="normal"><a href="#__codelineno-7-26"> 26</a></span>
<span class="normal"><a href="#__codelineno-7-27"> 27</a></span>
<span class="normal"><a href="#__codelineno-7-28"> 28</a></span>
<span class="normal"><a href="#__codelineno-7-29"> 29</a></span>
<span class="normal"><a href="#__codelineno-7-30"> 30</a></span>
<span class="normal"><a href="#__codelineno-7-31"> 31</a></span>
<span class="normal"><a href="#__codelineno-7-32"> 32</a></span>
<span class="normal"><a href="#__codelineno-7-33"> 33</a></span>
<span class="normal"><a href="#__codelineno-7-34"> 34</a></span>
<span class="normal"><a href="#__codelineno-7-35"> 35</a></span>
<span class="normal"><a href="#__codelineno-7-36"> 36</a></span>
<span class="normal"><a href="#__codelineno-7-37"> 37</a></span>
<span class="normal"><a href="#__codelineno-7-38"> 38</a></span>
<span class="normal"><a href="#__codelineno-7-39"> 39</a></span>
<span class="normal"><a href="#__codelineno-7-40"> 40</a></span>
<span class="normal"><a href="#__codelineno-7-41"> 41</a></span>
<span class="normal"><a href="#__codelineno-7-42"> 42</a></span>
<span class="normal"><a href="#__codelineno-7-43"> 43</a></span>
<span class="normal"><a href="#__codelineno-7-44"> 44</a></span>
<span class="normal"><a href="#__codelineno-7-45"> 45</a></span>
<span class="normal"><a href="#__codelineno-7-46"> 46</a></span>
<span class="normal"><a href="#__codelineno-7-47"> 47</a></span>
<span class="normal"><a href="#__codelineno-7-48"> 48</a></span>
<span class="normal"><a href="#__codelineno-7-49"> 49</a></span>
<span class="normal"><a href="#__codelineno-7-50"> 50</a></span>
<span class="normal"><a href="#__codelineno-7-51"> 51</a></span>
<span class="normal"><a href="#__codelineno-7-52"> 52</a></span>
<span class="normal"><a href="#__codelineno-7-53"> 53</a></span>
<span class="normal"><a href="#__codelineno-7-54"> 54</a></span>
<span class="normal"><a href="#__codelineno-7-55"> 55</a></span>
<span class="normal"><a href="#__codelineno-7-56"> 56</a></span>
<span class="normal"><a href="#__codelineno-7-57"> 57</a></span>
<span class="normal"><a href="#__codelineno-7-58"> 58</a></span>
<span class="normal"><a href="#__codelineno-7-59"> 59</a></span>
<span class="normal"><a href="#__codelineno-7-60"> 60</a></span>
<span class="normal"><a href="#__codelineno-7-61"> 61</a></span>
<span class="normal"><a href="#__codelineno-7-62"> 62</a></span>
<span class="normal"><a href="#__codelineno-7-63"> 63</a></span>
<span class="normal"><a href="#__codelineno-7-64"> 64</a></span>
<span class="normal"><a href="#__codelineno-7-65"> 65</a></span>
<span class="normal"><a href="#__codelineno-7-66"> 66</a></span>
<span class="normal"><a href="#__codelineno-7-67"> 67</a></span>
<span class="normal"><a href="#__codelineno-7-68"> 68</a></span>
<span class="normal"><a href="#__codelineno-7-69"> 69</a></span>
<span class="normal"><a href="#__codelineno-7-70"> 70</a></span>
<span class="normal"><a href="#__codelineno-7-71"> 71</a></span>
<span class="normal"><a href="#__codelineno-7-72"> 72</a></span>
<span class="normal"><a href="#__codelineno-7-73"> 73</a></span>
<span class="normal"><a href="#__codelineno-7-74"> 74</a></span>
<span class="normal"><a href="#__codelineno-7-75"> 75</a></span>
<span class="normal"><a href="#__codelineno-7-76"> 76</a></span>
<span class="normal"><a href="#__codelineno-7-77"> 77</a></span>
<span class="normal"><a href="#__codelineno-7-78"> 78</a></span>
<span class="normal"><a href="#__codelineno-7-79"> 79</a></span>
<span class="normal"><a href="#__codelineno-7-80"> 80</a></span>
<span class="normal"><a href="#__codelineno-7-81"> 81</a></span>
<span class="normal"><a href="#__codelineno-7-82"> 82</a></span>
<span class="normal"><a href="#__codelineno-7-83"> 83</a></span>
<span class="normal"><a href="#__codelineno-7-84"> 84</a></span>
<span class="normal"><a href="#__codelineno-7-85"> 85</a></span>
<span class="normal"><a href="#__codelineno-7-86"> 86</a></span>
<span class="normal"><a href="#__codelineno-7-87"> 87</a></span>
<span class="normal"><a href="#__codelineno-7-88"> 88</a></span>
<span class="normal"><a href="#__codelineno-7-89"> 89</a></span>
<span class="normal"><a href="#__codelineno-7-90"> 90</a></span>
<span class="normal"><a href="#__codelineno-7-91"> 91</a></span>
<span class="normal"><a href="#__codelineno-7-92"> 92</a></span>
<span class="normal"><a href="#__codelineno-7-93"> 93</a></span>
<span class="normal"><a href="#__codelineno-7-94"> 94</a></span>
<span class="normal"><a href="#__codelineno-7-95"> 95</a></span>
<span class="normal"><a href="#__codelineno-7-96"> 96</a></span>
<span class="normal"><a href="#__codelineno-7-97"> 97</a></span>
<span class="normal"><a href="#__codelineno-7-98"> 98</a></span>
<span class="normal"><a href="#__codelineno-7-99"> 99</a></span>
<span class="normal"><a href="#__codelineno-7-100">100</a></span>
<span class="normal"><a href="#__codelineno-7-101">101</a></span>
<span class="normal"><a href="#__codelineno-7-102">102</a></span>
<span class="normal"><a href="#__codelineno-7-103">103</a></span>
<span class="normal"><a href="#__codelineno-7-104">104</a></span>
<span class="normal"><a href="#__codelineno-7-105">105</a></span>
<span class="normal"><a href="#__codelineno-7-106">106</a></span>
<span class="normal"><a href="#__codelineno-7-107">107</a></span>
<span class="normal"><a href="#__codelineno-7-108">108</a></span>
<span class="normal"><a href="#__codelineno-7-109">109</a></span>
<span class="normal"><a href="#__codelineno-7-110">110</a></span>
<span class="normal"><a href="#__codelineno-7-111">111</a></span>
<span class="normal"><a href="#__codelineno-7-112">112</a></span>
<span class="normal"><a href="#__codelineno-7-113">113</a></span>
<span class="normal"><a href="#__codelineno-7-114">114</a></span>
<span class="normal"><a href="#__codelineno-7-115">115</a></span>
<span class="normal"><a href="#__codelineno-7-116">116</a></span>
<span class="normal"><a href="#__codelineno-7-117">117</a></span>
<span class="normal"><a href="#__codelineno-7-118">118</a></span>
<span class="normal"><a href="#__codelineno-7-119">119</a></span>
<span class="normal"><a href="#__codelineno-7-120">120</a></span>
<span class="normal"><a href="#__codelineno-7-121">121</a></span>
<span class="normal"><a href="#__codelineno-7-122">122</a></span>
<span class="normal"><a href="#__codelineno-7-123">123</a></span>
<span class="normal"><a href="#__codelineno-7-124">124</a></span>
<span class="normal"><a href="#__codelineno-7-125">125</a></span>
<span class="normal"><a href="#__codelineno-7-126">126</a></span>
<span class="normal"><a href="#__codelineno-7-127">127</a></span>
<span class="normal"><a href="#__codelineno-7-128">128</a></span>
<span class="normal"><a href="#__codelineno-7-129">129</a></span>
<span class="normal"><a href="#__codelineno-7-130">130</a></span>
<span class="normal"><a href="#__codelineno-7-131">131</a></span>
<span class="normal"><a href="#__codelineno-7-132">132</a></span>
<span class="normal"><a href="#__codelineno-7-133">133</a></span>
<span class="normal"><a href="#__codelineno-7-134">134</a></span>
<span class="normal"><a href="#__codelineno-7-135">135</a></span>
<span class="normal"><a href="#__codelineno-7-136">136</a></span>
<span class="normal"><a href="#__codelineno-7-137">137</a></span>
<span class="normal"><a href="#__codelineno-7-138">138</a></span>
<span class="normal"><a href="#__codelineno-7-139">139</a></span>
<span class="normal"><a href="#__codelineno-7-140">140</a></span>
<span class="normal"><a href="#__codelineno-7-141">141</a></span>
<span class="normal"><a href="#__codelineno-7-142">142</a></span>
<span class="normal"><a href="#__codelineno-7-143">143</a></span>
<span class="normal"><a href="#__codelineno-7-144">144</a></span>
<span class="normal"><a href="#__codelineno-7-145">145</a></span>
<span class="normal"><a href="#__codelineno-7-146">146</a></span>
<span class="normal"><a href="#__codelineno-7-147">147</a></span>
<span class="normal"><a href="#__codelineno-7-148">148</a></span>
<span class="normal"><a href="#__codelineno-7-149">149</a></span>
<span class="normal"><a href="#__codelineno-7-150">150</a></span>
<span class="normal"><a href="#__codelineno-7-151">151</a></span>
<span class="normal"><a href="#__codelineno-7-152">152</a></span>
<span class="normal"><a href="#__codelineno-7-153">153</a></span>
<span class="normal"><a href="#__codelineno-7-154">154</a></span>
<span class="normal"><a href="#__codelineno-7-155">155</a></span>
<span class="normal"><a href="#__codelineno-7-156">156</a></span>
<span class="normal"><a href="#__codelineno-7-157">157</a></span>
<span class="normal"><a href="#__codelineno-7-158">158</a></span>
<span class="normal"><a href="#__codelineno-7-159">159</a></span>
<span class="normal"><a href="#__codelineno-7-160">160</a></span>
<span class="normal"><a href="#__codelineno-7-161">161</a></span>
<span class="normal"><a href="#__codelineno-7-162">162</a></span>
<span class="normal"><a href="#__codelineno-7-163">163</a></span>
<span class="normal"><a href="#__codelineno-7-164">164</a></span>
<span class="normal"><a href="#__codelineno-7-165">165</a></span>
<span class="normal"><a href="#__codelineno-7-166">166</a></span>
<span class="normal"><a href="#__codelineno-7-167">167</a></span>
<span class="normal"><a href="#__codelineno-7-168">168</a></span>
<span class="normal"><a href="#__codelineno-7-169">169</a></span>
<span class="normal"><a href="#__codelineno-7-170">170</a></span>
<span class="normal"><a href="#__codelineno-7-171">171</a></span>
<span class="normal"><a href="#__codelineno-7-172">172</a></span>
<span class="normal"><a href="#__codelineno-7-173">173</a></span>
<span class="normal"><a href="#__codelineno-7-174">174</a></span>
<span class="normal"><a href="#__codelineno-7-175">175</a></span>
<span class="normal"><a href="#__codelineno-7-176">176</a></span>
<span class="normal"><a href="#__codelineno-7-177">177</a></span>
<span class="normal"><a href="#__codelineno-7-178">178</a></span>
<span class="normal"><a href="#__codelineno-7-179">179</a></span>
<span class="normal"><a href="#__codelineno-7-180">180</a></span>
<span class="normal"><a href="#__codelineno-7-181">181</a></span>
<span class="normal"><a href="#__codelineno-7-182">182</a></span>
<span class="normal"><a href="#__codelineno-7-183">183</a></span>
<span class="normal"><a href="#__codelineno-7-184">184</a></span>
<span class="normal"><a href="#__codelineno-7-185">185</a></span>
<span class="normal"><a href="#__codelineno-7-186">186</a></span>
<span class="normal"><a href="#__codelineno-7-187">187</a></span>
<span class="normal"><a href="#__codelineno-7-188">188</a></span>
<span class="normal"><a href="#__codelineno-7-189">189</a></span>
<span class="normal"><a href="#__codelineno-7-190">190</a></span>
<span class="normal"><a href="#__codelineno-7-191">191</a></span>
<span class="normal"><a href="#__codelineno-7-192">192</a></span>
<span class="normal"><a href="#__codelineno-7-193">193</a></span>
<span class="normal"><a href="#__codelineno-7-194">194</a></span>
<span class="normal"><a href="#__codelineno-7-195">195</a></span>
<span class="normal"><a href="#__codelineno-7-196">196</a></span>
<span class="normal"><a href="#__codelineno-7-197">197</a></span>
<span class="normal"><a href="#__codelineno-7-198">198</a></span>
<span class="normal"><a href="#__codelineno-7-199">199</a></span>
<span class="normal"><a href="#__codelineno-7-200">200</a></span>
<span class="normal"><a href="#__codelineno-7-201">201</a></span>
<span class="normal"><a href="#__codelineno-7-202">202</a></span>
<span class="normal"><a href="#__codelineno-7-203">203</a></span>
<span class="normal"><a href="#__codelineno-7-204">204</a></span>
<span class="normal"><a href="#__codelineno-7-205">205</a></span>
<span class="normal"><a href="#__codelineno-7-206">206</a></span>
<span class="normal"><a href="#__codelineno-7-207">207</a></span>
<span class="normal"><a href="#__codelineno-7-208">208</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<a id="__codelineno-7-2" name="__codelineno-7-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">torch.optim.optimizer</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optimizer</span>
<a id="__codelineno-7-3" name="__codelineno-7-3"></a>
<a id="__codelineno-7-4" name="__codelineno-7-4"></a><span class="c1"># 导入类型提示，兼容不同版本的 PyTorch 和 torch_optimizer</span>
<a id="__codelineno-7-5" name="__codelineno-7-5"></a><span class="k">try</span><span class="p">:</span>
<a id="__codelineno-7-6" name="__codelineno-7-6"></a>    <span class="kn">from</span><span class="w"> </span><span class="nn">torch.optim.optimizer</span><span class="w"> </span><span class="kn">import</span> <span class="n">Params</span><span class="p">,</span> <span class="n">OptLossClosure</span><span class="p">,</span> <span class="n">OptFloat</span>
<a id="__codelineno-7-7" name="__codelineno-7-7"></a><span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
<a id="__codelineno-7-8" name="__codelineno-7-8"></a>    <span class="kn">from</span><span class="w"> </span><span class="nn">torch_optimizer.types</span><span class="w"> </span><span class="kn">import</span> <span class="n">Params</span><span class="p">,</span> <span class="n">OptLossClosure</span><span class="p">,</span> <span class="n">OptFloat</span>
<a id="__codelineno-7-9" name="__codelineno-7-9"></a>
<a id="__codelineno-7-10" name="__codelineno-7-10"></a><span class="c1"># --- 迭代求逆根算法的预计算系数 ---</span>
<a id="__codelineno-7-11" name="__codelineno-7-11"></a><span class="c1"># 这是一个硬编码的系数表，用于不同阶数 (r) 的矩阵求逆根的多项式近似。</span>
<a id="__codelineno-7-12" name="__codelineno-7-12"></a><span class="c1"># 每一行对应一个 r 值 (r=1, 2, 3, ...)。</span>
<a id="__codelineno-7-13" name="__codelineno-7-13"></a><span class="c1"># 每个元组 (a, b, c) 是多项式 p(x) = a + bx + cx^2 的系数。</span>
<a id="__codelineno-7-14" name="__codelineno-7-14"></a><span class="c1"># 这是算法的核心“黑魔法”，是预先通过数值优化得到的。</span>
<a id="__codelineno-7-15" name="__codelineno-7-15"></a><span class="n">coefs</span> <span class="o">=</span> <span class="p">[</span>
<a id="__codelineno-7-16" name="__codelineno-7-16"></a>    <span class="kc">None</span><span class="p">,</span>  <span class="c1"># r=0 无意义</span>
<a id="__codelineno-7-17" name="__codelineno-7-17"></a>    <span class="c1"># r=1</span>
<a id="__codelineno-7-18" name="__codelineno-7-18"></a>    <span class="p">[</span>
<a id="__codelineno-7-19" name="__codelineno-7-19"></a>        <span class="p">(</span><span class="mf">14.2975</span><span class="p">,</span> <span class="o">-</span><span class="mf">31.2203</span><span class="p">,</span> <span class="mf">18.9214</span><span class="p">),</span> <span class="p">(</span><span class="mf">7.12258</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.78207</span><span class="p">,</span> <span class="mf">2.35989</span><span class="p">),</span>
<a id="__codelineno-7-20" name="__codelineno-7-20"></a>        <span class="p">(</span><span class="mf">6.9396</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.61544</span><span class="p">,</span> <span class="mf">2.3195</span><span class="p">),</span> <span class="p">(</span><span class="mf">5.98456</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.77016</span><span class="p">,</span> <span class="mf">2.12571</span><span class="p">),</span>
<a id="__codelineno-7-21" name="__codelineno-7-21"></a>        <span class="p">(</span><span class="mf">3.79109</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.18664</span><span class="p">,</span> <span class="mf">1.39555</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<a id="__codelineno-7-22" name="__codelineno-7-22"></a>    <span class="p">],</span>
<a id="__codelineno-7-23" name="__codelineno-7-23"></a>    <span class="c1"># r=2 (用于四阶张量，如卷积核)</span>
<a id="__codelineno-7-24" name="__codelineno-7-24"></a>    <span class="p">[</span>
<a id="__codelineno-7-25" name="__codelineno-7-25"></a>        <span class="p">(</span><span class="mf">7.42487</span><span class="p">,</span> <span class="o">-</span><span class="mf">18.3958</span><span class="p">,</span> <span class="mf">12.8967</span><span class="p">),</span> <span class="p">(</span><span class="mf">3.48773</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.33004</span><span class="p">,</span> <span class="mf">0.440469</span><span class="p">),</span>
<a id="__codelineno-7-26" name="__codelineno-7-26"></a>        <span class="p">(</span><span class="mf">2.77661</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.07064</span><span class="p">,</span> <span class="mf">0.463023</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.99131</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.37394</span><span class="p">,</span> <span class="mf">0.387593</span><span class="p">),</span>
<a id="__codelineno-7-27" name="__codelineno-7-27"></a>        <span class="p">(</span><span class="mi">15</span> <span class="o">/</span> <span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span> <span class="o">/</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">8</span><span class="p">),</span> <span class="c1"># 理论最优系数</span>
<a id="__codelineno-7-28" name="__codelineno-7-28"></a>    <span class="p">],</span>
<a id="__codelineno-7-29" name="__codelineno-7-29"></a>    <span class="c1"># r=3</span>
<a id="__codelineno-7-30" name="__codelineno-7-30"></a>    <span class="p">[</span>
<a id="__codelineno-7-31" name="__codelineno-7-31"></a>        <span class="p">(</span><span class="mf">5.05052</span><span class="p">,</span> <span class="o">-</span><span class="mf">13.5427</span><span class="p">,</span> <span class="mf">10.2579</span><span class="p">),</span> <span class="p">(</span><span class="mf">2.31728</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.06581</span><span class="p">,</span> <span class="mf">0.144441</span><span class="p">),</span>
<a id="__codelineno-7-32" name="__codelineno-7-32"></a>        <span class="p">(</span><span class="mf">1.79293</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.913562</span><span class="p">,</span> <span class="mf">0.186699</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.56683</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.786609</span><span class="p">,</span> <span class="mf">0.220008</span><span class="p">),</span>
<a id="__codelineno-7-33" name="__codelineno-7-33"></a>        <span class="p">(</span><span class="mi">14</span> <span class="o">/</span> <span class="mi">9</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span> <span class="o">/</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">9</span><span class="p">),</span>
<a id="__codelineno-7-34" name="__codelineno-7-34"></a>    <span class="p">],</span>
<a id="__codelineno-7-35" name="__codelineno-7-35"></a>    <span class="c1"># r=4</span>
<a id="__codelineno-7-36" name="__codelineno-7-36"></a>    <span class="p">[</span>
<a id="__codelineno-7-37" name="__codelineno-7-37"></a>        <span class="p">(</span><span class="mf">3.85003</span><span class="p">,</span> <span class="o">-</span><span class="mf">10.8539</span><span class="p">,</span> <span class="mf">8.61893</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.80992</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.587778</span><span class="p">,</span> <span class="mf">0.0647852</span><span class="p">),</span>
<a id="__codelineno-7-38" name="__codelineno-7-38"></a>        <span class="p">(</span><span class="mf">1.50394</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.594516</span><span class="p">,</span> <span class="mf">0.121161</span><span class="p">),</span> <span class="p">(</span><span class="mi">45</span> <span class="o">/</span> <span class="mi">32</span><span class="p">,</span> <span class="o">-</span><span class="mi">9</span> <span class="o">/</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span> <span class="o">/</span> <span class="mi">32</span><span class="p">),</span>
<a id="__codelineno-7-39" name="__codelineno-7-39"></a>    <span class="p">],</span>
<a id="__codelineno-7-40" name="__codelineno-7-40"></a>    <span class="c1"># r=5</span>
<a id="__codelineno-7-41" name="__codelineno-7-41"></a>    <span class="p">[</span>
<a id="__codelineno-7-42" name="__codelineno-7-42"></a>        <span class="p">(</span><span class="mf">3.11194</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.28217</span><span class="p">,</span> <span class="mf">6.67716</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.5752</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.393327</span><span class="p">,</span> <span class="mf">0.0380364</span><span class="p">),</span>
<a id="__codelineno-7-43" name="__codelineno-7-43"></a>        <span class="p">(</span><span class="mf">1.3736</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.44661</span><span class="p">,</span> <span class="mf">0.0911259</span><span class="p">),</span> <span class="p">(</span><span class="mi">33</span> <span class="o">/</span> <span class="mi">25</span><span class="p">,</span> <span class="o">-</span><span class="mi">11</span> <span class="o">/</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">25</span><span class="p">),</span>
<a id="__codelineno-7-44" name="__codelineno-7-44"></a>    <span class="p">],</span>
<a id="__codelineno-7-45" name="__codelineno-7-45"></a><span class="p">]</span>
<a id="__codelineno-7-46" name="__codelineno-7-46"></a>
<a id="__codelineno-7-47" name="__codelineno-7-47"></a><span class="k">def</span><span class="w"> </span><span class="nf">abc</span><span class="p">(</span><span class="n">r</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<a id="__codelineno-7-48" name="__codelineno-7-48"></a><span class="w">    </span><span class="sd">"""一个生成器，用于按需提供上述系数。"""</span>
<a id="__codelineno-7-49" name="__codelineno-7-49"></a>    <span class="n">w</span><span class="p">,</span> <span class="n">steps</span> <span class="o">=</span> <span class="n">coefs</span><span class="p">[</span><span class="n">r</span><span class="p">],</span> <span class="n">steps</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">coefs</span><span class="p">[</span><span class="n">r</span><span class="p">])</span>
<a id="__codelineno-7-50" name="__codelineno-7-50"></a>    <span class="c1"># 按照迭代步数提供系数，如果步数超过系数表长度，则重复使用最后一个系数</span>
<a id="__codelineno-7-51" name="__codelineno-7-51"></a>    <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">w</span><span class="p">[:</span><span class="n">steps</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="n">steps</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="mi">0</span><span class="p">):</span>
<a id="__codelineno-7-52" name="__codelineno-7-52"></a>        <span class="c1"># 根据缩放因子 scale 对系数进行调整</span>
<a id="__codelineno-7-53" name="__codelineno-7-53"></a>        <span class="k">yield</span> <span class="n">a</span> <span class="o">/</span> <span class="n">scale</span><span class="p">,</span> <span class="n">b</span> <span class="o">/</span> <span class="n">scale</span><span class="o">**</span><span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">c</span> <span class="o">/</span> <span class="n">scale</span><span class="o">**</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">r</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-7-54" name="__codelineno-7-54"></a>
<a id="__codelineno-7-55" name="__codelineno-7-55"></a><span class="k">def</span><span class="w"> </span><span class="nf">_compute_inv_root_iterative</span><span class="p">(</span><span class="n">P</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-7-56" name="__codelineno-7-56"></a><span class="w">    </span><span class="sd">"""使用迭代矩阵乘法方法，高效计算 P^(-1/r)。"""</span>
<a id="__codelineno-7-57" name="__codelineno-7-57"></a>    <span class="n">s</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># 论文中定义的常量</span>
<a id="__codelineno-7-58" name="__codelineno-7-58"></a>    <span class="n">I</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">P</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">P</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="c1"># 创建单位矩阵</span>
<a id="__codelineno-7-59" name="__codelineno-7-59"></a>
<a id="__codelineno-7-60" name="__codelineno-7-60"></a>    <span class="c1"># 为了数值稳定性，对输入矩阵 P 进行归一化</span>
<a id="__codelineno-7-61" name="__codelineno-7-61"></a>    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">P</span> <span class="o">*</span> <span class="n">P</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span> <span class="c1"># 计算 P 的 Frobenius 范数</span>
<a id="__codelineno-7-62" name="__codelineno-7-62"></a>    <span class="n">P_norm</span> <span class="o">=</span> <span class="n">P</span> <span class="o">/</span> <span class="n">t</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">I</span> <span class="c1"># 归一化并加上一个小的扰动项</span>
<a id="__codelineno-7-63" name="__codelineno-7-63"></a>
<a id="__codelineno-7-64" name="__codelineno-7-64"></a>    <span class="n">G</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="c1"># G 将会是最终的 P^(-1/r)</span>
<a id="__codelineno-7-65" name="__codelineno-7-65"></a>
<a id="__codelineno-7-66" name="__codelineno-7-66"></a>    <span class="c1"># steps=None 会让 abc 使用该阶数下所有可用的系数</span>
<a id="__codelineno-7-67" name="__codelineno-7-67"></a>    <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">abc</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.001</span><span class="p">):</span> <span class="c1"># scale &gt; 1.0 保证收敛</span>
<a id="__codelineno-7-68" name="__codelineno-7-68"></a>        <span class="c1"># 计算多项式 W = a*I + b*P_norm + c*P_norm^2</span>
<a id="__codelineno-7-69" name="__codelineno-7-69"></a>        <span class="n">W</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">I</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">P_norm</span> <span class="o">+</span> <span class="n">c</span> <span class="o">*</span> <span class="p">(</span><span class="n">P_norm</span> <span class="o">@</span> <span class="n">P_norm</span><span class="p">)</span>
<a id="__codelineno-7-70" name="__codelineno-7-70"></a>        <span class="c1"># 这是一个耦合迭代过程，同时更新 G 和 P_norm</span>
<a id="__codelineno-7-71" name="__codelineno-7-71"></a>        <span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_power</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
<a id="__codelineno-7-72" name="__codelineno-7-72"></a>        <span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_power</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
<a id="__codelineno-7-73" name="__codelineno-7-73"></a>        <span class="n">G</span> <span class="o">=</span> <span class="n">G</span> <span class="o">@</span> <span class="n">W1</span>
<a id="__codelineno-7-74" name="__codelineno-7-74"></a>        <span class="n">P_norm</span> <span class="o">=</span> <span class="n">P_norm</span> <span class="o">@</span> <span class="n">W2</span>
<a id="__codelineno-7-75" name="__codelineno-7-75"></a>
<a id="__codelineno-7-76" name="__codelineno-7-76"></a>    <span class="c1"># 最后，对结果进行反归一化，得到最终的逆根矩阵</span>
<a id="__codelineno-7-77" name="__codelineno-7-77"></a>    <span class="k">return</span> <span class="n">G</span> <span class="o">*</span> <span class="p">(</span><span class="n">t</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="n">s</span> <span class="o">/</span> <span class="n">r</span><span class="p">))</span>
<a id="__codelineno-7-78" name="__codelineno-7-78"></a>
<a id="__codelineno-7-79" name="__codelineno-7-79"></a>
<a id="__codelineno-7-80" name="__codelineno-7-80"></a><span class="k">class</span><span class="w"> </span><span class="nc">Shampoo</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
<a id="__codelineno-7-81" name="__codelineno-7-81"></a><span class="w">    </span><span class="sa">r</span><span class="sd">"""实现了 Shampoo 优化器算法。</span>
<a id="__codelineno-7-82" name="__codelineno-7-82"></a>
<a id="__codelineno-7-83" name="__codelineno-7-83"></a><span class="sd">    **注意**: 这个版本被修改为使用一种快速的、基于迭代矩阵乘法的方法来计算矩阵的逆根，</span>
<a id="__codelineno-7-84" name="__codelineno-7-84"></a><span class="sd">    而不是原始的基于 SVD 的方法。</span>
<a id="__codelineno-7-85" name="__codelineno-7-85"></a><span class="sd">    """</span>
<a id="__codelineno-7-86" name="__codelineno-7-86"></a>
<a id="__codelineno-7-87" name="__codelineno-7-87"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-7-88" name="__codelineno-7-88"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-7-89" name="__codelineno-7-89"></a>        <span class="n">params</span><span class="p">:</span> <span class="n">Params</span><span class="p">,</span>
<a id="__codelineno-7-90" name="__codelineno-7-90"></a>        <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-1</span><span class="p">,</span>
<a id="__codelineno-7-91" name="__codelineno-7-91"></a>        <span class="n">momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
<a id="__codelineno-7-92" name="__codelineno-7-92"></a>        <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
<a id="__codelineno-7-93" name="__codelineno-7-93"></a>        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span>     <span class="c1"># 用于稳定预处理器 (preconditioner)</span>
<a id="__codelineno-7-94" name="__codelineno-7-94"></a>        <span class="n">update_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>      <span class="c1"># 每隔多少步更新一次逆根矩阵</span>
<a id="__codelineno-7-95" name="__codelineno-7-95"></a>        <span class="n">iter_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>    <span class="c1"># 迭代求逆根算法中的扰动项</span>
<a id="__codelineno-7-96" name="__codelineno-7-96"></a>    <span class="p">):</span>
<a id="__codelineno-7-97" name="__codelineno-7-97"></a>        <span class="c1"># --- 输入参数合法性检查 ---</span>
<a id="__codelineno-7-98" name="__codelineno-7-98"></a>        <span class="k">if</span> <span class="n">lr</span> <span class="o">&lt;=</span> <span class="mf">0.0</span><span class="p">:</span>
<a id="__codelineno-7-99" name="__codelineno-7-99"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'无效的学习率: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
<a id="__codelineno-7-100" name="__codelineno-7-100"></a>        <span class="k">if</span> <span class="n">momentum</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">:</span>
<a id="__codelineno-7-101" name="__codelineno-7-101"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'无效的动量值: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">momentum</span><span class="p">))</span>
<a id="__codelineno-7-102" name="__codelineno-7-102"></a>        <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">:</span>
<a id="__codelineno-7-103" name="__codelineno-7-103"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<a id="__codelineno-7-104" name="__codelineno-7-104"></a>                <span class="s1">'无效的 weight_decay 值: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">weight_decay</span><span class="p">)</span>
<a id="__codelineno-7-105" name="__codelineno-7-105"></a>            <span class="p">)</span>
<a id="__codelineno-7-106" name="__codelineno-7-106"></a>        <span class="k">if</span> <span class="n">epsilon</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">:</span>
<a id="__codelineno-7-107" name="__codelineno-7-107"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'无效的 epsilon 值: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epsilon</span><span class="p">))</span>
<a id="__codelineno-7-108" name="__codelineno-7-108"></a>        <span class="k">if</span> <span class="n">update_freq</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-7-109" name="__codelineno-7-109"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'无效的 update_freq 值: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">update_freq</span><span class="p">))</span>
<a id="__codelineno-7-110" name="__codelineno-7-110"></a>        <span class="k">if</span> <span class="n">iter_eps</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">:</span>
<a id="__codelineno-7-111" name="__codelineno-7-111"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'无效的 iter_eps 值: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">iter_eps</span><span class="p">))</span>
<a id="__codelineno-7-112" name="__codelineno-7-112"></a>
<a id="__codelineno-7-113" name="__codelineno-7-113"></a>        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
<a id="__codelineno-7-114" name="__codelineno-7-114"></a>            <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
<a id="__codelineno-7-115" name="__codelineno-7-115"></a>            <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span>
<a id="__codelineno-7-116" name="__codelineno-7-116"></a>            <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
<a id="__codelineno-7-117" name="__codelineno-7-117"></a>            <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
<a id="__codelineno-7-118" name="__codelineno-7-118"></a>            <span class="n">update_freq</span><span class="o">=</span><span class="n">update_freq</span><span class="p">,</span>
<a id="__codelineno-7-119" name="__codelineno-7-119"></a>            <span class="n">iter_eps</span><span class="o">=</span><span class="n">iter_eps</span><span class="p">,</span> 
<a id="__codelineno-7-120" name="__codelineno-7-120"></a>        <span class="p">)</span>
<a id="__codelineno-7-121" name="__codelineno-7-121"></a>        <span class="nb">super</span><span class="p">(</span><span class="n">Shampoo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>
<a id="__codelineno-7-122" name="__codelineno-7-122"></a>
<a id="__codelineno-7-123" name="__codelineno-7-123"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="p">:</span> <span class="n">OptLossClosure</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">OptFloat</span><span class="p">:</span>
<a id="__codelineno-7-124" name="__codelineno-7-124"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
<a id="__codelineno-7-125" name="__codelineno-7-125"></a>        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-7-126" name="__codelineno-7-126"></a>            <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>
<a id="__codelineno-7-127" name="__codelineno-7-127"></a>
<a id="__codelineno-7-128" name="__codelineno-7-128"></a>        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
<a id="__codelineno-7-129" name="__codelineno-7-129"></a>            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">'params'</span><span class="p">]:</span>
<a id="__codelineno-7-130" name="__codelineno-7-130"></a>                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-7-131" name="__codelineno-7-131"></a>                    <span class="k">continue</span>
<a id="__codelineno-7-132" name="__codelineno-7-132"></a>                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
<a id="__codelineno-7-133" name="__codelineno-7-133"></a>                <span class="n">order</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">ndimension</span><span class="p">()</span> <span class="c1"># 获取梯度张量的阶数 (维度数量)</span>
<a id="__codelineno-7-134" name="__codelineno-7-134"></a>                <span class="n">original_size</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<a id="__codelineno-7-135" name="__codelineno-7-135"></a>                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
<a id="__codelineno-7-136" name="__codelineno-7-136"></a>                <span class="n">momentum</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">'momentum'</span><span class="p">]</span>
<a id="__codelineno-7-137" name="__codelineno-7-137"></a>
<a id="__codelineno-7-138" name="__codelineno-7-138"></a>                <span class="c1"># 确定使用的求逆根的阶数 r，最大不超过系数表中定义的最大阶数</span>
<a id="__codelineno-7-139" name="__codelineno-7-139"></a>                <span class="n">inv_root_order</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">order</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">coefs</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-7-140" name="__codelineno-7-140"></a>
<a id="__codelineno-7-141" name="__codelineno-7-141"></a>                <span class="c1"># --- 状态初始化 ---</span>
<a id="__codelineno-7-142" name="__codelineno-7-142"></a>                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-7-143" name="__codelineno-7-143"></a>                    <span class="n">state</span><span class="p">[</span><span class="s1">'step'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-7-144" name="__codelineno-7-144"></a>                    <span class="k">if</span> <span class="n">momentum</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-7-145" name="__codelineno-7-145"></a>                        <span class="n">state</span><span class="p">[</span><span class="s1">'momentum_buffer'</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<a id="__codelineno-7-146" name="__codelineno-7-146"></a>                    <span class="c1"># 为张量的每个维度初始化一个预处理器 (preconditioner) 矩阵</span>
<a id="__codelineno-7-147" name="__codelineno-7-147"></a>                    <span class="k">for</span> <span class="n">dim_id</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span>
<a id="__codelineno-7-148" name="__codelineno-7-148"></a>                        <span class="c1"># precond_i 是第 i 维的协方差矩阵</span>
<a id="__codelineno-7-149" name="__codelineno-7-149"></a>                        <span class="n">state</span><span class="p">[</span><span class="sa">f</span><span class="s1">'precond_</span><span class="si">{</span><span class="n">dim_id</span><span class="si">}</span><span class="s1">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span>
<a id="__codelineno-7-150" name="__codelineno-7-150"></a>                            <span class="s1">'epsilon'</span>
<a id="__codelineno-7-151" name="__codelineno-7-151"></a>                        <span class="p">]</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">grad</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">grad</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<a id="__codelineno-7-152" name="__codelineno-7-152"></a>                        <span class="c1"># inv_precond_i 是其对应的逆根矩阵</span>
<a id="__codelineno-7-153" name="__codelineno-7-153"></a>                        <span class="n">state</span><span class="p">[</span>
<a id="__codelineno-7-154" name="__codelineno-7-154"></a>                            <span class="sa">f</span><span class="s1">'inv_precond_</span><span class="si">{</span><span class="n">dim_id</span><span class="si">}</span><span class="s1">'</span>
<a id="__codelineno-7-155" name="__codelineno-7-155"></a>                        <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="sa">f</span><span class="s1">'precond_</span><span class="si">{</span><span class="n">dim_id</span><span class="si">}</span><span class="s1">'</span><span class="p">])</span>
<a id="__codelineno-7-156" name="__codelineno-7-156"></a>
<a id="__codelineno-7-157" name="__codelineno-7-157"></a>                <span class="c1"># 1. 应用动量 (Momentum)</span>
<a id="__codelineno-7-158" name="__codelineno-7-158"></a>                <span class="k">if</span> <span class="n">momentum</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-7-159" name="__codelineno-7-159"></a>                    <span class="k">if</span> <span class="s1">'momentum_buffer'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
<a id="__codelineno-7-160" name="__codelineno-7-160"></a>                         <span class="n">buf</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">'momentum_buffer'</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<a id="__codelineno-7-161" name="__codelineno-7-161"></a>                    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-7-162" name="__codelineno-7-162"></a>                         <span class="n">buf</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">'momentum_buffer'</span><span class="p">]</span>
<a id="__codelineno-7-163" name="__codelineno-7-163"></a>                         <span class="n">buf</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">momentum</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">momentum</span><span class="p">)</span>
<a id="__codelineno-7-164" name="__codelineno-7-164"></a>                    <span class="n">grad</span> <span class="o">=</span> <span class="n">buf</span> <span class="c1"># 后续操作在动量缓冲上进行</span>
<a id="__codelineno-7-165" name="__codelineno-7-165"></a>
<a id="__codelineno-7-166" name="__codelineno-7-166"></a>                <span class="c1"># 2. 应用权重衰减 (Weight Decay)</span>
<a id="__codelineno-7-167" name="__codelineno-7-167"></a>                <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">'weight_decay'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-7-168" name="__codelineno-7-168"></a>                    <span class="n">grad</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s1">'weight_decay'</span><span class="p">])</span>
<a id="__codelineno-7-169" name="__codelineno-7-169"></a>
<a id="__codelineno-7-170" name="__codelineno-7-170"></a>                <span class="c1"># --- 核心：逐维度进行预处理 ---</span>
<a id="__codelineno-7-171" name="__codelineno-7-171"></a>                <span class="c1"># 遍历张量的每一个维度</span>
<a id="__codelineno-7-172" name="__codelineno-7-172"></a>                <span class="k">for</span> <span class="n">dim_id</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span>
<a id="__codelineno-7-173" name="__codelineno-7-173"></a>                    <span class="n">precond</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="sa">f</span><span class="s1">'precond_</span><span class="si">{</span><span class="n">dim_id</span><span class="si">}</span><span class="s1">'</span><span class="p">]</span>
<a id="__codelineno-7-174" name="__codelineno-7-174"></a>                    <span class="n">inv_precond</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="sa">f</span><span class="s1">'inv_precond_</span><span class="si">{</span><span class="n">dim_id</span><span class="si">}</span><span class="s1">'</span><span class="p">]</span>
<a id="__codelineno-7-175" name="__codelineno-7-175"></a>
<a id="__codelineno-7-176" name="__codelineno-7-176"></a>                    <span class="c1"># a. 将当前维度换到第0维，并将其他维度展平</span>
<a id="__codelineno-7-177" name="__codelineno-7-177"></a>                    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">transpose_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim_id</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
<a id="__codelineno-7-178" name="__codelineno-7-178"></a>                    <span class="n">transposed_size</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<a id="__codelineno-7-179" name="__codelineno-7-179"></a>                    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-7-180" name="__codelineno-7-180"></a>
<a id="__codelineno-7-181" name="__codelineno-7-181"></a>                    <span class="c1"># b. 更新预处理器 (协方差矩阵)</span>
<a id="__codelineno-7-182" name="__codelineno-7-182"></a>                    <span class="c1"># 这是在累积二阶矩信息</span>
<a id="__codelineno-7-183" name="__codelineno-7-183"></a>                    <span class="n">grad_t</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
<a id="__codelineno-7-184" name="__codelineno-7-184"></a>                    <span class="n">precond</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span> <span class="o">@</span> <span class="n">grad_t</span><span class="p">)</span>
<a id="__codelineno-7-185" name="__codelineno-7-185"></a>
<a id="__codelineno-7-186" name="__codelineno-7-186"></a>                    <span class="c1"># c. 定期更新逆根矩阵 (这是计算成本最高的部分)</span>
<a id="__codelineno-7-187" name="__codelineno-7-187"></a>                    <span class="k">if</span> <span class="n">state</span><span class="p">[</span><span class="s1">'step'</span><span class="p">]</span> <span class="o">%</span> <span class="n">group</span><span class="p">[</span><span class="s1">'update_freq'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-7-188" name="__codelineno-7-188"></a>                        <span class="n">inv_precond</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">_compute_inv_root_iterative</span><span class="p">(</span>
<a id="__codelineno-7-189" name="__codelineno-7-189"></a>                            <span class="n">precond</span><span class="p">,</span>
<a id="__codelineno-7-190" name="__codelineno-7-190"></a>                            <span class="n">r</span><span class="o">=</span><span class="n">inv_root_order</span><span class="p">,</span> <span class="c1"># 使用与张量阶数匹配的根</span>
<a id="__codelineno-7-191" name="__codelineno-7-191"></a>                            <span class="n">eps</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s1">'iter_eps'</span><span class="p">]</span>
<a id="__codelineno-7-192" name="__codelineno-7-192"></a>                        <span class="p">))</span>
<a id="__codelineno-7-193" name="__codelineno-7-193"></a>
<a id="__codelineno-7-194" name="__codelineno-7-194"></a>                    <span class="c1"># d. 使用逆根矩阵对梯度进行预处理</span>
<a id="__codelineno-7-195" name="__codelineno-7-195"></a>                    <span class="k">if</span> <span class="n">dim_id</span> <span class="o">==</span> <span class="n">order</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-7-196" name="__codelineno-7-196"></a>                        <span class="c1"># 对于最后一个维度，需要右乘</span>
<a id="__codelineno-7-197" name="__codelineno-7-197"></a>                        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad_t</span> <span class="o">@</span> <span class="n">inv_precond</span>
<a id="__codelineno-7-198" name="__codelineno-7-198"></a>                        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_size</span><span class="p">)</span> <span class="c1"># 恢复原始形状</span>
<a id="__codelineno-7-199" name="__codelineno-7-199"></a>                    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-7-200" name="__codelineno-7-200"></a>                        <span class="c1"># 对于其他维度，左乘</span>
<a id="__codelineno-7-201" name="__codelineno-7-201"></a>                        <span class="n">grad</span> <span class="o">=</span> <span class="n">inv_precond</span> <span class="o">@</span> <span class="n">grad</span>
<a id="__codelineno-7-202" name="__codelineno-7-202"></a>                        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">transposed_size</span><span class="p">)</span> <span class="c1"># 恢复转置后的形状</span>
<a id="__codelineno-7-203" name="__codelineno-7-203"></a>
<a id="__codelineno-7-204" name="__codelineno-7-204"></a>                <span class="n">state</span><span class="p">[</span><span class="s1">'step'</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<a id="__codelineno-7-205" name="__codelineno-7-205"></a>                <span class="c1"># 3. 最后一步：用最终处理过的梯度更新参数</span>
<a id="__codelineno-7-206" name="__codelineno-7-206"></a>                <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">group</span><span class="p">[</span><span class="s1">'lr'</span><span class="p">])</span>
<a id="__codelineno-7-207" name="__codelineno-7-207"></a>
<a id="__codelineno-7-208" name="__codelineno-7-208"></a>        <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="admonition info">
<p class="admonition-title">📝 如果您需要引用本文</p>
<p>Yan Li. (Sep. 1, 2025). 自适应学习率改进策略 [Blog post]. Retrieved from <a href="https://dicaeopolis.github.io/DNN/optimizer/Adaptive">https://dicaeopolis.github.io/DNN/optimizer/Adaptive</a></p>
<p>在 BibTeX 格式中：
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-8-1">1</a></span>
<span class="normal"><a href="#__codelineno-8-2">2</a></span>
<span class="normal"><a href="#__codelineno-8-3">3</a></span>
<span class="normal"><a href="#__codelineno-8-4">4</a></span>
<span class="normal"><a href="#__codelineno-8-5">5</a></span>
<span class="normal"><a href="#__codelineno-8-6">6</a></span>
<span class="normal"><a href="#__codelineno-8-7">7</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1"></a>@online{Adaptive,
<a id="__codelineno-8-2" name="__codelineno-8-2"></a>    title={自适应学习率改进策略},
<a id="__codelineno-8-3" name="__codelineno-8-3"></a>    author={Yan Li},
<a id="__codelineno-8-4" name="__codelineno-8-4"></a>    year={2025},
<a id="__codelineno-8-5" name="__codelineno-8-5"></a>    month={Sep},
<a id="__codelineno-8-6" name="__codelineno-8-6"></a>    url={\url{https://dicaeopolis.github.io/DNN/optimizer/Adaptive}},
<a id="__codelineno-8-7" name="__codelineno-8-7"></a>}
</code></pre></div></td></tr></table></div></p>
</div>
<form class="md-feedback" hidden="" name="feedback">
<fieldset>
<legend class="md-feedback__title">
        Was this page helpful?
      </legend>
<div class="md-feedback__inner">
<div class="md-feedback__list">
<button class="md-feedback__icon md-icon" data-md-value="1" title="This page was helpful" type="submit">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m7 0c0 .8-.7 1.5-1.5 1.5S14 10.3 14 9.5 14.7 8 15.5 8s1.5.7 1.5 1.5m-5 7.73c-1.75 0-3.29-.73-4.19-1.81L9.23 14c.45.72 1.52 1.23 2.77 1.23s2.32-.51 2.77-1.23l1.42 1.42c-.9 1.08-2.44 1.81-4.19 1.81"></path></svg>
</button>
<button class="md-feedback__icon md-icon" data-md-value="0" title="This page could be improved" type="submit">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10m-6.5-4c.8 0 1.5.7 1.5 1.5s-.7 1.5-1.5 1.5-1.5-.7-1.5-1.5.7-1.5 1.5-1.5M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m2 4.5c1.75 0 3.29.72 4.19 1.81l-1.42 1.42C14.32 16.5 13.25 16 12 16s-2.32.5-2.77 1.23l-1.42-1.42C8.71 14.72 10.25 14 12 14"></path></svg>
</button>
</div>
<div class="md-feedback__note">
<div data-md-value="1" hidden="">
              
              
                
              
              
              
                
                
              
              Thanks for your feedback!
            </div>
<div data-md-value="0" hidden="">
              
              
                
              
              
              
                
                
              
              Thanks for your feedback! Help us improve this page by using our <a href="..." rel="noopener" target="_blank">feedback form</a>.
            </div>
</div>
</div>
</fieldset>
</form>
<h2 id="__comments">评论</h2>
<!-- Giscus 脚本粘贴在这里 -->
<script async="" crossorigin="anonymous" data-category="Comments" data-category-id="DIC_kwDOOfbpCM4CtuiH" data-emit-metadata="0" data-input-position="bottom" data-lang="zh-CN" data-mapping="pathname" data-reactions-enabled="1" data-repo="dicaeopolis/dicaeopolis.github.io" data-repo-id="R_kgDOOfbpCA" data-strict="0" data-theme="preferred_color_scheme" src="https://giscus.app/client.js">
</script>
<!-- 这段脚本用于同步 Giscus 和 mkdocs-material 的主题 -->
<script>
    var giscus = document.querySelector("script[src*=giscus]");
    document.addEventListener("DOMContentLoaded", function () {
      var palette = __md_get("__palette");
      if (palette && typeof palette.color === "object") {
        var theme = palette.color.scheme === "slate" ? "transparent_dark" : "light";
        giscus.setAttribute("data-theme", theme);
      }
      var ref = document.querySelector("[data-md-component=palette]");
      ref.addEventListener("change", function () {
        var palette = __md_get("__palette");
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate" ? "transparent_dark" : "light";
          var message = { setConfig: { theme: theme } };
          var frame = document.querySelector(".giscus-frame");
          frame.contentWindow.postMessage({ giscus: message }, "https://giscus.app");
        }
      });
    });
  </script>
</article>
</div>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
</main>
<footer class="md-footer">
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../../..", "features": ["content.code.copy", "content.code.select", "navigation.titles", "navigation.tabs", "navigation.indexes"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
<script src="../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
<script src="../../../assets/js/custom.js"></script>
<script src="../../../themes/js/custom.js"></script>
<script src="../../../themes/js/simpleLightbox.min.js"></script>
<script src="../../../themes/js/optionalConfig.js"></script>
<script src="../../../themes/js/mermaidloader.js"></script>
<script src="../../../themes/js/umlconvert.js"></script>
<script src="../../../themes/js/mathjax.js"></script>
<script src="../../../themes/js/katex.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.17.1/flowchart.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.3.0/raphael.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.13.6/underscore-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mermaid-js/mermaid-mindmap@9.3.0/dist/diagram-definition.0faef4c2.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/markdown-it-plantuml@1.4.1/index.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.28/webfontloader.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml-full.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-svg-full.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
</body>
</html>